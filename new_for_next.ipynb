{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00b1b047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: biopython in c:\\users\\cs21b\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (1.85)\n",
      "Requirement already satisfied: numpy in c:\\users\\cs21b\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from biopython) (2.2.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "[' final_test_dataset.csv', ' final_train_dataset.csv', 'Dataset', 'Dataset_output', 'help.ipynb', 'human_IDPPI.fasta', 'human_ParkMarcotte.fasta', 'new_for_next.ipynb', 'ngram_feature_vector.csv', 'ngram_feature_vector_new.csv', 'pair_formatted_dataset.tsv', 'Project_all_part.ipynb', 'TestSet1.csv', 'TestSet2.csv', 'TestSet3.csv', 'test_dataset11.tsv', 'TrainSet1.csv', 'TrainSet2.csv', 'TrainSet3.csv', 'train_dataset11.tsv']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: C:\\Users\\cs21b\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install biopython\n",
    "import Bio\n",
    "\n",
    "import os\n",
    "\n",
    "# List files in the current directory\n",
    "print(os.listdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a21116da",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\human_IDPPI.fasta\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93f92c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total protein entries: 20117\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "P00441 : MATKAVCVLKGDGPVQGIINFEQKESNGPVKVWGSIKGLTEGLHGFHVHE... (length=154)\n",
      "Q13046 : MGPLSAPPCTQHITWKGLLLTASLLNFWNPPTTAQVTIEAQPPKVSEGKD... (length=419)\n",
      "P0C868 : MVVDLKNLLFNPSKPVSRGSQPADVDLMIDCLVSCLRVSPHNNQQFKIFL... (length=119)\n"
     ]
    }
   ],
   "source": [
    "file_path = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\human_ParkMarcotte.fasta\"\n",
    "\n",
    "protein_data = {}  # Dictionary to store ID: sequence\n",
    "\n",
    "with open(file_path, \"r\") as f:\n",
    "    current_id = None\n",
    "    current_seq = []\n",
    "\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if line.startswith(\">\"):\n",
    "            if current_id:  # Save the previous entry\n",
    "                protein_data[current_id] = \"\".join(current_seq)\n",
    "            # Extract protein ID (e.g., P27540)\n",
    "            parts = line.split(\"|\")\n",
    "            if len(parts) > 1:\n",
    "                current_id = parts[1]\n",
    "            else:\n",
    "                current_id = line[1:].split()[0]  # Fallback if '|' not found\n",
    "            current_seq = []\n",
    "        else:\n",
    "            current_seq.append(line)\n",
    "\n",
    "    # Save last sequence\n",
    "    if current_id:\n",
    "        protein_data[current_id] = \"\".join(current_seq)\n",
    "\n",
    "print(\"Total protein entries:\", len(protein_data))\n",
    "print(\"\\n\\n\\n\")\n",
    "\n",
    "\n",
    "# Optional: Check first few entries\n",
    "for pid, seq in list(protein_data.items())[:3]:\n",
    "    print(f\"{pid} : {seq[:50]}... (length={len(seq)})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2976fa2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame shape: (20117, 401)\n",
      "  Sequence_ID        AA        AC        AD        AE        AF        AG  \\\n",
      "0      P00441  0.000000  0.006536  0.019608  0.000000  0.000000  0.019608   \n",
      "1      Q13046  0.000000  0.002392  0.002392  0.000000  0.000000  0.002392   \n",
      "2      P0C868  0.000000  0.000000  0.008475  0.000000  0.000000  0.000000   \n",
      "3      Q96G97  0.002519  0.000000  0.002519  0.002519  0.002519  0.005038   \n",
      "4      Q9UBN7  0.014003  0.004119  0.003295  0.007414  0.000824  0.008237   \n",
      "\n",
      "         AH        AI        AK  ...        YM        YN        YP        YQ  \\\n",
      "0  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
      "1  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.002392  0.002392   \n",
      "2  0.000000  0.000000  0.008475  ...  0.000000  0.000000  0.000000  0.000000   \n",
      "3  0.005038  0.002519  0.000000  ...  0.005038  0.002519  0.000000  0.000000   \n",
      "4  0.001647  0.004119  0.000824  ...  0.000824  0.001647  0.000000  0.000824   \n",
      "\n",
      "         YR        YS        YT        YV        YW        YY  \n",
      "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "1  0.000000  0.007177  0.007177  0.002392  0.000000  0.002392  \n",
      "2  0.000000  0.000000  0.000000  0.008475  0.000000  0.000000  \n",
      "3  0.010076  0.002519  0.002519  0.002519  0.000000  0.005038  \n",
      "4  0.000000  0.000824  0.000824  0.002471  0.000824  0.000824  \n",
      "\n",
      "[5 rows x 401 columns]\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# from itertools import product\n",
    "\n",
    "# # Step 1: Define all possible 2-grams\n",
    "# amino_acids = 'ACDEFGHIKLMNPQRSTVWY'\n",
    "# two_grams = [''.join(p) for p in product(amino_acids, repeat=2)]\n",
    "# gram_index = {gram: idx for idx, gram in enumerate(two_grams)}  # Map: dipeptide -> index\n",
    "\n",
    "# # Step 2: Convert each sequence into 400-dim vector\n",
    "# def sequence_to_2gram_vector(sequence):\n",
    "#     vector = np.zeros(400, dtype=np.float32)\n",
    "#     for i in range(len(sequence) - 1):\n",
    "#         dipeptide = sequence[i:i+2]\n",
    "#         if dipeptide in gram_index:\n",
    "#             vector[gram_index[dipeptide]] += 1\n",
    "#     return vector\n",
    "\n",
    "# # Step 3: Apply to all sequences\n",
    "# protein_vectors = {}\n",
    "# for pid, seq in protein_data.items():\n",
    "#     protein_vectors[pid] = sequence_to_2gram_vector(seq)\n",
    "\n",
    "# # Optional: check shape and sample\n",
    "# sample_id = list(protein_vectors.keys())[0]\n",
    "# print(\"Sample ID:\", sample_id)\n",
    "# print(\"Vector shape:\", protein_vectors[sample_id].shape)\n",
    "# print(\"Vector (first 10):\", protein_vectors[sample_id][:10])\n",
    "\n",
    "# === Step 2: 2-gram model setup ===\n",
    "# amino_acids = 'ACDEFGHIKLMNPQRSTVWY'\n",
    "# two_grams = [''.join(p) for p in product(amino_acids, repeat=2)]\n",
    "# gram_index = {gram: idx for idx, gram in enumerate(two_grams)}\n",
    "\n",
    "# def sequence_to_2gram_vector(sequence):\n",
    "#     vector = np.zeros(400, dtype=np.float32)\n",
    "#     for i in range(len(sequence) - 1):\n",
    "#         dipeptide = sequence[i:i+2]\n",
    "#         if dipeptide in gram_index:\n",
    "#             vector[gram_index[dipeptide]] += 1\n",
    "#     return vector\n",
    "\n",
    "# # === Step 3: Generate vectors for all protein sequences ===\n",
    "# protein_vectors = {}\n",
    "\n",
    "# for pid, seq in protein_data.items():\n",
    "#     protein_vectors[pid] = sequence_to_2gram_vector(seq)\n",
    "\n",
    "# # Optional check\n",
    "# # sample_id = list(protein_vectors.keys())[0]\n",
    "# # print(\"Sample ID:\", sample_id)\n",
    "# # print(\"Vector shape:\", protein_vectors[sample_id].shape)\n",
    "# # print(\"Vector (first 10 values):\", protein_vectors[sample_id][:10])\n",
    "\n",
    "# # Print the full vector for a specific protein ID\n",
    "# sample_id = list(protein_vectors.keys())[0]  # First protein ID in the dictionary\n",
    "# print(f\"Full vector for Protein ID {sample_id}:\")\n",
    "# print(protein_vectors[sample_id])\n",
    "# # import numpy as np\n",
    "\n",
    "# # Convert all vectors to a single NumPy array\n",
    "# all_vectors = np.array(list(protein_vectors.values()))\n",
    "\n",
    "# print(\"Shape of feature matrix:\", all_vectors.shape)  # (total_sequences, 400)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from Bio import SeqIO\n",
    "\n",
    "# Amino acids (standard 20)\n",
    "amino_acids = 'ACDEFGHIKLMNPQRSTVWY'\n",
    "aa_index = {aa: i for i, aa in enumerate(amino_acids)}\n",
    "\n",
    "# Function to create 20x20 bigram frequency matrix\n",
    "def create_bigram_matrix(sequence):\n",
    "    matrix = np.zeros((20, 20), dtype=float)\n",
    "    total_bigrams = 0\n",
    "\n",
    "    for i in range(len(sequence) - 1):\n",
    "        a1, a2 = sequence[i], sequence[i + 1]\n",
    "        if a1 in aa_index and a2 in aa_index:\n",
    "            matrix[aa_index[a1], aa_index[a2]] += 1\n",
    "            total_bigrams += 1\n",
    "\n",
    "    if total_bigrams > 0:\n",
    "        matrix /= total_bigrams  # Normalize\n",
    "\n",
    "    return matrix.flatten()  # Convert to 400-dim feature vector\n",
    "\n",
    "# Path to the FASTA file\n",
    "file_path = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\human_ParkMarcotte.fasta\"\n",
    "\n",
    "# Process all sequences and generate feature vectors\n",
    "feature_vectors = []\n",
    "seq_ids = []\n",
    "\n",
    "for record in SeqIO.parse(file_path, \"fasta\"):\n",
    "    seq = str(record.seq)\n",
    "    vec = create_bigram_matrix(seq)\n",
    "    feature_vectors.append(vec)\n",
    "    seq_ids.append(record.id)\n",
    "\n",
    "# Create DataFrame with 400-dimensional feature vectors\n",
    "columns = [f'{a1}{a2}' for a1 in amino_acids for a2 in amino_acids]\n",
    "df = pd.DataFrame(feature_vectors, columns=columns)\n",
    "df.insert(0, \"Sequence_ID\", seq_ids)\n",
    "\n",
    "# Save to CSV\n",
    "output_path = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\ngram_feature_vector_new.csv\"\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "# Optional: Check DataFrame shape and first few rows\n",
    "print(\"DataFrame shape:\", df.shape)\n",
    "print(df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cb33a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from itertools import combinations\n",
    "\n",
    "# # Read the CSV\n",
    "# csv_path = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\ngram_feature_vector_new.csv\"\n",
    "# df = pd.read_csv(csv_path)\n",
    "\n",
    "# # Separate IDs and vectors\n",
    "# ids = df['Sequence_ID'].values  # Array of protein IDs\n",
    "# X = df.drop(columns=['Sequence_ID']).values  # Feature vectors\n",
    "\n",
    "# # Prepare lines\n",
    "# lines = []\n",
    "# pairs = list(combinations(range(len(ids)), 2))  # All unique pairs\n",
    "\n",
    "# for i, j in pairs:\n",
    "#     id1 = ids[i]\n",
    "#     id2 = ids[j]\n",
    "#     pair_id = f\"{id1}+{id2}\"\n",
    "#     vec_i = X[i]\n",
    "#     vec_j = X[j]\n",
    "#     combined_features = np.concatenate([vec_i, vec_j])\n",
    "#     features_str = ' '.join(map(str, combined_features))  # space-separated features\n",
    "#     label = ''  # Placeholder for label\n",
    "#     line = f\"{pair_id}\\t{features_str}\\t{label}\"  # tab-separated line\n",
    "#     lines.append(line)\n",
    "\n",
    "# # Save to TSV\n",
    "# output_path = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\pair_formatted_dataset_new.tsv\"\n",
    "# with open(output_path, 'w') as f:\n",
    "#     for line in lines:\n",
    "#         f.write(line + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1368d74e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " For DataSet inside C1 :- \n",
      "TrainSet1.csv → Valid pairs: 45380, Missing: 124\n",
      "TestSet1.csv → Valid pairs: 5049, Missing: 7\n",
      "TrainSet2.csv → Valid pairs: 45383, Missing: 121\n",
      "TestSet2.csv → Valid pairs: 5046, Missing: 10\n",
      "TrainSet3.csv → Valid pairs: 45380, Missing: 124\n",
      "TestSet3.csv → Valid pairs: 5049, Missing: 7\n",
      "TrainSet4.csv → Valid pairs: 45386, Missing: 118\n",
      "TestSet4.csv → Valid pairs: 5043, Missing: 13\n",
      "TrainSet5.csv → Valid pairs: 45384, Missing: 120\n",
      "TestSet5.csv → Valid pairs: 5045, Missing: 11\n",
      "\n",
      " For DataSet inside C2 :- \n",
      "TrainSet1.csv → Valid pairs: 18940, Missing: 122\n",
      "TestSet1.csv → Valid pairs: 1496, Missing: 4\n",
      "TrainSet2.csv → Valid pairs: 18356, Missing: 92\n",
      "TestSet2.csv → Valid pairs: 1497, Missing: 3\n",
      "TrainSet3.csv → Valid pairs: 18982, Missing: 57\n",
      "TestSet3.csv → Valid pairs: 1488, Missing: 12\n",
      "TrainSet4.csv → Valid pairs: 19237, Missing: 86\n",
      "TestSet4.csv → Valid pairs: 1491, Missing: 9\n",
      "TrainSet5.csv → Valid pairs: 19176, Missing: 92\n",
      "TestSet5.csv → Valid pairs: 1499, Missing: 1\n",
      "\n",
      " For DataSet inside C3 :- \n",
      "TrainSet1.csv → Valid pairs: 21512, Missing: 52\n",
      "TestSet1.csv → Valid pairs: 5923, Missing: 3\n",
      "TrainSet2.csv → Valid pairs: 21964, Missing: 54\n",
      "TestSet2.csv → Valid pairs: 5777, Missing: 5\n",
      "TrainSet3.csv → Valid pairs: 20898, Missing: 50\n",
      "TestSet3.csv → Valid pairs: 6264, Missing: 2\n",
      "TrainSet4.csv → Valid pairs: 20980, Missing: 46\n",
      "TestSet4.csv → Valid pairs: 6363, Missing: 5\n",
      "TrainSet5.csv → Valid pairs: 21472, Missing: 54\n",
      "TestSet5.csv → Valid pairs: 6153, Missing: 3\n"
     ]
    }
   ],
   "source": [
    "# # # # # import pandas as pd\n",
    "# # # # # import numpy as np\n",
    "\n",
    "# # # # # # Load precomputed 400-dim vectors from CSV\n",
    "# # # # # vector_csv_path = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\ngram_feature_vector_new.csv\"\n",
    "# # # # # vector_df = pd.read_csv(vector_csv_path)\n",
    "\n",
    "# # # # # # Make a dictionary: ID → vector\n",
    "# # # # # vector_dict = {}\n",
    "# # # # # for _, row in vector_df.iterrows():\n",
    "# # # # #     vector_dict[row['Sequence_ID']] = row.drop('Sequence_ID').values.astype(float)\n",
    "\n",
    "# # # # # # Function to generate paired dataset\n",
    "# # # # # def generate_pair_dataset(pair_csv_path, output_path):\n",
    "# # # # #     pair_df = pd.read_csv(pair_csv_path)\n",
    "# # # # #     lines = []\n",
    "\n",
    "# # # # #     for _, row in pair_df.iterrows():\n",
    "# # # # #         id1, id2, label = row['IntA'], row['IntB'], row['Cls']\n",
    "\n",
    "# # # # #         if id1 in vector_dict and id2 in vector_dict:\n",
    "# # # # #             vec1 = vector_dict[id1]\n",
    "# # # # #             vec2 = vector_dict[id2]\n",
    "# # # # #             combined = np.concatenate([vec1, vec2])\n",
    "# # # # #             combined_str = ' '.join(map(str, combined))\n",
    "# # # # #             line = f\"{id1}+{id2}\\t{combined_str}\\t{label}\"\n",
    "# # # # #             lines.append(line)\n",
    "# # # # #         else:\n",
    "# # # # #             print(f\"Missing vector for: {id1} or {id2}\")\n",
    "\n",
    "# # # # #     # Save to file\n",
    "# # # # #     with open(output_path, 'w') as f:\n",
    "# # # # #         for line in lines:\n",
    "# # # # #             f.write(line + '\\n')\n",
    "\n",
    "# # # # # # Paths\n",
    "# # # # # train_csv = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\TrainSet1.csv\"\n",
    "# # # # # test_csv = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\TestSet1.csv\"\n",
    "# # # # # train_output = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\TrainSet111.csv\"\n",
    "# # # # # test_output = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\TestSet111.csv\"\n",
    "\n",
    "# # # # # # Generate datasets\n",
    "# # # # # generate_pair_dataset(train_csv, train_output)\n",
    "# # # # # generate_pair_dataset(test_csv, test_output)\n",
    "\n",
    "\n",
    "# # # # # import pandas as pd\n",
    "# # # # # import numpy as np\n",
    "\n",
    "# # # # # # Load 400-dim vectors\n",
    "# # # # # vector_df = pd.read_csv(r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\ngram_feature_vector_new.csv\")\n",
    "# # # # # vector_dict = {\n",
    "# # # # #     row['Sequence_ID']: row.drop('Sequence_ID').values.astype(float)\n",
    "# # # # #     for _, row in vector_df.iterrows()\n",
    "# # # # # }\n",
    "\n",
    "# # # # # # Function to generate dataset\n",
    "# # # # # def create_dataset(pair_file, output_file):\n",
    "# # # # #     pairs_df = pd.read_csv(pair_file)\n",
    "# # # # #     lines = []\n",
    "\n",
    "# # # # #     for _, row in pairs_df.iterrows():\n",
    "# # # # #         id1, id2, label = row['IntA'], row['IntB'], row['Cls']\n",
    "\n",
    "# # # # #         if id1 in vector_dict and id2 in vector_dict:\n",
    "# # # # #             vec1 = vector_dict[id1]\n",
    "# # # # #             vec2 = vector_dict[id2]\n",
    "\n",
    "# # # # #             vec1_str = ' '.join(map(str, vec1))\n",
    "# # # # #             vec2_str = ' '.join(map(str, vec2))\n",
    "\n",
    "# # # # #             line = f\"{id1}+{id2}\\t{vec1_str}\\t{vec2_str}\\t{label}\"\n",
    "# # # # #             lines.append(line)\n",
    "# # # # #         else:\n",
    "# # # # #             print(f\" Missing vector for: {id1} or {id2}\")\n",
    "\n",
    "# # # # #     # Write to output\n",
    "# # # # #     with open(output_file, 'w') as f:\n",
    "# # # # #         for line in lines:\n",
    "# # # # #             f.write(line + '\\n')\n",
    "\n",
    "# # # # # # Paths\n",
    "# # # # # train_input = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\TrainSet1.csv\"\n",
    "# # # # # test_input = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\TestSet1.csv\"\n",
    "# # # # # train_output = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Train_Output1.tsv\"\n",
    "# # # # # test_output = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Test_Output1.tsv\"\n",
    "\n",
    "# # # # # # Generate both datasets\n",
    "# # # # # create_dataset(train_input, train_output)\n",
    "# # # # # create_dataset(test_input, test_output)\n",
    "\n",
    "\n",
    "# # # # import pandas as pd\n",
    "# # # # import numpy as np\n",
    "\n",
    "# # # # # --- Load feature vectors from CSV ---\n",
    "# # # # feature_vector_path = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\ngram_feature_vector_new.csv\"\n",
    "# # # # fv_df = pd.read_csv(feature_vector_path)\n",
    "\n",
    "# # # # # Convert to dict: ID -> 400-dim vector\n",
    "# # # # feature_vectors = {}\n",
    "# # # # for _, row in fv_df.iterrows():\n",
    "# # # #     pid = row['Sequence_ID']\n",
    "# # # #     vector = row.drop('Sequence_ID').values.astype(float)\n",
    "# # # #     feature_vectors[pid] = vector\n",
    "\n",
    "# # # # # --- ID Cleaning Function ---\n",
    "# # # # def clean_id(protein_id):\n",
    "# # # #     # Extract ID like P27540 from sp|P27540|ARNT_HUMAN\n",
    "# # # #     if \"|\" in protein_id:\n",
    "# # # #         parts = protein_id.split(\"|\")\n",
    "# # # #         if len(parts) > 1:\n",
    "# # # #             return parts[1]\n",
    "# # # #     return protein_id\n",
    "\n",
    "# # # # # --- Function to create final dataset ---\n",
    "# # # # def create_pair_dataset(input_csv_path, output_tsv_path):\n",
    "# # # #     df = pd.read_csv(input_csv_path)\n",
    "\n",
    "# # # #     lines = []\n",
    "# # # #     missing_count = 0\n",
    "\n",
    "# # # #     for _, row in df.iterrows():\n",
    "# # # #         id1 = clean_id(row['IntA'])\n",
    "# # # #         id2 = clean_id(row['IntB'])\n",
    "# # # #         label = row['Cls']\n",
    "\n",
    "# # # #         if id1 in feature_vectors and id2 in feature_vectors:\n",
    "# # # #             vec1 = feature_vectors[id1]\n",
    "# # # #             vec2 = feature_vectors[id2]\n",
    "# # # #             pair_id = f\"{id1}+{id2}\"\n",
    "# # # #             combined_vec = np.concatenate([vec1, vec2])\n",
    "# # # #             combined_str = \" \".join(map(str, combined_vec))\n",
    "# # # #             line = f\"{pair_id}\\t{combined_str}\\t{label}\"\n",
    "# # # #             lines.append(line)\n",
    "# # # #         else:\n",
    "# # # #             missing_count += 1\n",
    "\n",
    "# # # #     print(f\"Total valid pairs: {len(lines)}\")\n",
    "# # # #     print(f\"Missing pairs (not found in feature vectors): {missing_count}\")\n",
    "\n",
    "# # # #     # Write to TSV\n",
    "# # # #     with open(output_tsv_path, 'w') as f:\n",
    "# # # #         for line in lines:\n",
    "# # # #             f.write(line + '\\n')\n",
    "\n",
    "# # # # # --- File paths ---\n",
    "# # # # train_input = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\TrainSet1.csv\"\n",
    "# # # # train_output = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\train_dataset11.tsv\"\n",
    "\n",
    "# # # # test_input = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\TestSet1.csv\"\n",
    "# # # # test_output = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\test_dataset11.tsv\"\n",
    "\n",
    "# # # # # --- Run ---\n",
    "# # # # create_pair_dataset(train_input, train_output)\n",
    "# # # # create_pair_dataset(test_input, test_output)\n",
    "\n",
    "\n",
    "# # # import pandas as pd\n",
    "# # # import numpy as np\n",
    "# # # import os\n",
    "\n",
    "# # # # --- Load feature vectors ---\n",
    "# # # feature_vector_path = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\ngram_feature_vector_new.csv\"\n",
    "# # # fv_df = pd.read_csv(feature_vector_path)\n",
    "# # # feature_vectors = {\n",
    "# # #     row['Sequence_ID']: row.drop('Sequence_ID').values.astype(float)\n",
    "# # #     for _, row in fv_df.iterrows()\n",
    "# # # }\n",
    "\n",
    "# # # # --- ID Cleaning Function ---\n",
    "# # # def clean_id(protein_id):\n",
    "# # #     if \"|\" in protein_id:\n",
    "# # #         parts = protein_id.split(\"|\")\n",
    "# # #         if len(parts) > 1:\n",
    "# # #             return parts[1]\n",
    "# # #     return protein_id\n",
    "\n",
    "# # # # --- Function to create dataset ---\n",
    "# # # def create_pair_dataset(input_csv_path, output_tsv_path):\n",
    "# # #     df = pd.read_csv(input_csv_path)\n",
    "\n",
    "# # #     lines = []\n",
    "# # #     missing_count = 0\n",
    "\n",
    "# # #     for _, row in df.iterrows():\n",
    "# # #         id1 = clean_id(row['IntA'])\n",
    "# # #         id2 = clean_id(row['IntB'])\n",
    "# # #         label = row['Cls']\n",
    "\n",
    "# # #         if id1 in feature_vectors and id2 in feature_vectors:\n",
    "# # #             vec1 = feature_vectors[id1]\n",
    "# # #             vec2 = feature_vectors[id2]\n",
    "# # #             pair_id = f\"{id1}+{id2}\"\n",
    "# # #             combined_vec = np.concatenate([vec1, vec2])\n",
    "# # #             combined_str = \" \".join(map(str, combined_vec))\n",
    "# # #             line = f\"{pair_id}\\t{combined_str}\\t{label}\"\n",
    "# # #             lines.append(line)\n",
    "# # #         else:\n",
    "# # #             missing_count += 1\n",
    "\n",
    "# # #     print(f\"{os.path.basename(input_csv_path)} -> Valid: {len(lines)}, Missing: {missing_count}\")\n",
    "\n",
    "# # #     # Write output\n",
    "# # #     os.makedirs(os.path.dirname(output_tsv_path), exist_ok=True)\n",
    "# # #     with open(output_tsv_path, 'w') as f:\n",
    "# # #         for line in lines:\n",
    "# # #             f.write(line + '\\n')\n",
    "\n",
    "# # # # --- Dataset paths ---\n",
    "# # # datasets = [\n",
    "# # #   {  train_input11 = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Dataset\\Dataset_C1\\TrainSet1.csv\"\n",
    "# # #     train_output11 = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Dataset_output\\For_C1\\For_Train\\train_dataset11.tsv\"\n",
    "# # #     test_input11 = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Dataset\\Dataset_C1\\TestSet1.csv\"\n",
    "# # #     test_output11 = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Dataset_output\\For_C1\\For_Test\\test_dataset11.tsv\"\n",
    "# # # },\n",
    "# # #   {  train_input12 = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Dataset\\Dataset_C1\\TrainSet2.csv\"\n",
    "# # #     train_output12 = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Dataset_output\\For_C1\\For_Train\\train_dataset12.tsv\"\n",
    "# # #     test_input12 = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Dataset\\Dataset_C1\\TestSet2.csv\"\n",
    "# # #     test_output12 = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Dataset_output\\For_C1\\For_Test\\test_dataset12.tsv\"\n",
    "# # # },\n",
    "# # #   {  train_input13 = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Dataset\\Dataset_C1\\TrainSet3.csv\"\n",
    "# # #     train_output13 = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Dataset_output\\For_C1\\For_Train\\train_dataset13.tsv\"\n",
    "# # #     test_input13 = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Dataset\\Dataset_C1\\TestSet3.csv\"\n",
    "# # #     test_output13 = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Dataset_output\\For_C1\\For_Test\\test_dataset13.tsv\"\n",
    "# # # },\n",
    "# # #   {  train_input14 = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Dataset\\Dataset_C1\\TrainSet4.csv\"\n",
    "# # #     train_output14 = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Dataset_output\\For_C1\\For_Train\\train_dataset14.tsv\"\n",
    "# # #     test_input14 = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Dataset\\Dataset_C1\\TestSet4.csv\"\n",
    "# # #     test_output14 = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Dataset_output\\For_C1\\For_Test\\test_dataset14.tsv\"\n",
    "# # # },\n",
    "# # #   {  train_input15 = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Dataset\\Dataset_C1\\TrainSet5.csv\"\n",
    "# # #     train_output15 = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Dataset_output\\For_C1\\For_Train\\train_dataset15.tsv\"\n",
    "# # #     test_input15 = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Dataset\\Dataset_C1\\TestSet5.csv\"\n",
    "# # #     test_output15 = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Dataset_output\\For_C1\\For_Test\\test_dataset15.tsv\"\n",
    "# # # },\n",
    "# # #   {  train_input21 = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Dataset\\Dataset_C2\\TrainSet1.csv\"\n",
    "# # #     train_output21 = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Dataset_output\\For_C2\\For_Train\\train_dataset21.tsv\"\n",
    "# # #     test_input21 = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Dataset\\Dataset_C2\\TestSet1.csv\"\n",
    "# # #     test_output21 = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Dataset_output\\For_C2\\For_Test\\test_dataset21.tsv\"\n",
    "# # # },\n",
    "# # #   {  train_input22 = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Dataset\\Dataset_C2\\TrainSet2.csv\"\n",
    "# # #     train_output22 = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Dataset_output\\For_C2\\For_Train\\train_dataset22.tsv\"\n",
    "# # #     test_input22 = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Dataset\\Dataset_C2\\TestSet2.csv\"\n",
    "# # #     test_output22 = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Dataset_output\\For_C2\\For_Test\\test_dataset22.tsv\"\n",
    "# # # },\n",
    "# # #   {  train_input23 = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Dataset\\Dataset_C2\\TrainSet3.csv\"\n",
    "# # #     train_output23 = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Dataset_output\\For_C2\\For_Train\\train_dataset23.tsv\"\n",
    "# # #     test_input23 = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Dataset\\Dataset_C2\\TestSet3.csv\"\n",
    "# # #     test_output23 = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Dataset_output\\For_C2\\For_Test\\test_dataset23.tsv\"\n",
    "# # # },\n",
    "# # #   {  train_input24 = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Dataset\\Dataset_C2\\TrainSet4.csv\"\n",
    "# # #     train_output24 = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Dataset_output\\For_C2\\For_Train\\train_dataset24.tsv\"\n",
    "# # #     test_input24 = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Dataset\\Dataset_C2\\TestSet4.csv\"\n",
    "# # #     test_output24 = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Dataset_output\\For_C2\\For_Test\\test_dataset24.tsv\"\n",
    "# # # },\n",
    "# # #   {  train_input25 = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Dataset\\Dataset_C2\\TrainSet5.csv\"\n",
    "# # #     train_output25 = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Dataset_output\\For_C2\\For_Train\\train_dataset25.tsv\"\n",
    "# # #     test_input25 = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Dataset\\Dataset_C2\\TestSet5.csv\"\n",
    "# # #     test_output25 = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Dataset_output\\For_C2\\For_Test\\test_dataset25.tsv\"\n",
    "# # # },\n",
    "# # #   {  train_input31 = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Dataset\\Dataset_C3\\TrainSet1.csv\"\n",
    "# # #     train_output31 = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Dataset_output\\For_C3\\For_Train\\train_dataset31.tsv\"\n",
    "# # #     test_input31 = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Dataset\\Dataset_C3\\TestSet1.csv\"\n",
    "# # #     test_output31 = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Dataset_output\\For_C3\\For_Test\\test_dataset31.tsv\"\n",
    "# # # },\n",
    "# # #   {  train_input32 = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Dataset\\Dataset_C3\\TrainSet2.csv\"\n",
    "# # #     train_output32 = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Dataset_output\\For_C3\\For_Train\\train_dataset32.tsv\"\n",
    "# # #     test_input32 = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Dataset\\Dataset_C3\\TestSet2.csv\"\n",
    "# # #     test_output32 = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Dataset_output\\For_C3\\For_Test\\test_dataset32.tsv\"\n",
    "# # # },\n",
    "# # #   {  train_input33 = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Dataset\\Dataset_C3\\TrainSet3.csv\"\n",
    "# # #     train_output33 = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Dataset_output\\For_C3\\For_Train\\train_dataset33.tsv\"\n",
    "# # #     test_input33 = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Dataset\\Dataset_C3\\TestSet3.csv\"\n",
    "# # #     test_output33 = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Dataset_output\\For_C3\\For_Test\\test_dataset33.tsv\"\n",
    "# # # },\n",
    "# # #  {   train_input34 = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Dataset\\Dataset_C3\\TrainSet4.csv\"\n",
    "# # #     train_output34 = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Dataset_output\\For_C3\\For_Train\\train_dataset34.tsv\"\n",
    "# # #     test_input34 = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Dataset\\Dataset_C3\\TestSet4.csv\"\n",
    "# # #     test_output34 = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Dataset_output\\For_C3\\For_Test\\test_dataset34.tsv\"\n",
    "# # # },\n",
    "# # #     {train_input35 = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Dataset\\Dataset_C3\\TrainSet5.csv\"\n",
    "# # #     train_output35 = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Dataset_output\\For_C3\\For_Train\\train_dataset35.tsv\"\n",
    "# # #     test_input35 = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Dataset\\Dataset_C3\\TestSet5.csv\"\n",
    "# # #     test_output35 = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Dataset_output\\For_C3\\For_Test\\test_dataset35.tsv\"\n",
    "# # # },\n",
    "\n",
    "# # # ]\n",
    "\n",
    "# # # # --- Run for all datasets ---\n",
    "# # # for dataset in datasets:\n",
    "# # #     create_pair_dataset(dataset[\"train_input\"], dataset[\"train_output\"])\n",
    "# # #     create_pair_dataset(dataset[\"test_input\"], dataset[\"test_output\"])\n",
    "\n",
    "\n",
    "\n",
    "# # import pandas as pd\n",
    "# # import numpy as np\n",
    "\n",
    "# # # --- Load feature vectors from CSV ---\n",
    "# # feature_vector_path = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\ngram_feature_vector_new.csv\"\n",
    "# # fv_df = pd.read_csv(feature_vector_path)\n",
    "\n",
    "# # # Convert to dict: ID -> 400-dim vector\n",
    "# # feature_vectors = {\n",
    "# #     row['Sequence_ID']: row.drop('Sequence_ID').values.astype(float)\n",
    "# #     for _, row in fv_df.iterrows()\n",
    "# # }\n",
    "\n",
    "# # # --- ID Cleaning Function ---\n",
    "# # def clean_id(protein_id):\n",
    "# #     if \"|\" in protein_id:\n",
    "# #         parts = protein_id.split(\"|\")\n",
    "# #         if len(parts) > 1:\n",
    "# #             return parts[1]\n",
    "# #     return protein_id\n",
    "\n",
    "# # # --- Function to create final dataset ---\n",
    "# # def create_pair_dataset(input_csv_path, output_tsv_path):\n",
    "# #     df = pd.read_csv(input_csv_path)\n",
    "# #     lines = []\n",
    "# #     missing_count = 0\n",
    "\n",
    "# #     for _, row in df.iterrows():\n",
    "# #         id1 = clean_id(row['IntA'])\n",
    "# #         id2 = clean_id(row['IntB'])\n",
    "# #         label = row['Cls']\n",
    "\n",
    "# #         if id1 in feature_vectors and id2 in feature_vectors:\n",
    "# #             vec1 = feature_vectors[id1]\n",
    "# #             vec2 = feature_vectors[id2]\n",
    "# #             pair_id = f\"{id1}+{id2}\"\n",
    "# #             combined_vec = np.concatenate([vec1, vec2])\n",
    "# #             combined_str = \" \".join(map(str, combined_vec))\n",
    "# #             line = f\"{pair_id}\\t{combined_str}\\t{label}\"\n",
    "# #             lines.append(line)\n",
    "# #         else:\n",
    "# #             missing_count += 1\n",
    "\n",
    "# #     # print(f\" {input_csv_path} → {output_tsv_path}\")\n",
    "# #     print(f\"   Total valid pairs: {len(lines)}\")\n",
    "# #     print(f\"   Missing pairs: {missing_count}\\n\\n\")\n",
    "\n",
    "# #     with open(output_tsv_path, 'w') as f:\n",
    "# #         for line in lines:\n",
    "# #             f.write(line + '\\n')\n",
    "\n",
    "# # # --- List of input-output file path pairs ---\n",
    "# # file_paths = [\n",
    "# #     (\n",
    "# #         r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Dataset\\Dataset_C1\\TrainSet1.csv\",\n",
    "# #         r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Dataset_output\\For_C1\\For_Train\\train_dataset11.tsv\",\n",
    "# #         r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Dataset\\Dataset_C1\\TestSet1.csv\",\n",
    "# #         r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Dataset_output\\For_C1\\For_Test\\test_dataset11.tsv\",\n",
    "# #     ),\n",
    "# #     (\n",
    "# #         r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Dataset\\Dataset_C1\\TrainSet2.csv\",\n",
    "# #         r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Dataset_output\\For_C1\\For_Train\\train_dataset12.tsv\",\n",
    "# #         r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Dataset\\Dataset_C1\\TestSet2.csv\",\n",
    "# #         r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Dataset_output\\For_C1\\For_Test\\test_dataset12.tsv\",\n",
    "# #     ),\n",
    "# #     (\n",
    "# #         r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Dataset\\Dataset_C1\\TrainSet3.csv\",\n",
    "# #         r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Dataset_output\\For_C1\\For_Train\\train_dataset13.tsv\",\n",
    "# #         r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Dataset\\Dataset_C1\\TestSet3.csv\",\n",
    "# #         r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Dataset_output\\For_C1\\For_Test\\test_dataset13.tsv\",\n",
    "# #     ),\n",
    "# #     (\n",
    "# #         r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Dataset\\Dataset_C1\\TrainSet4.csv\",\n",
    "# #         r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Dataset_output\\For_C1\\For_Train\\train_dataset14.tsv\",\n",
    "# #         r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Dataset\\Dataset_C1\\TestSet4.csv\",\n",
    "# #         r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Dataset_output\\For_C1\\For_Test\\test_dataset14.tsv\",\n",
    "# #     ),\n",
    "# # ]\n",
    "\n",
    "# # # --- Run loop for each pair ---\n",
    "# # for train_in, train_out, test_in, test_out in file_paths:\n",
    "# #     create_pair_dataset(train_in, train_out)\n",
    "# #     create_pair_dataset(test_in, test_out)\n",
    "\n",
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import os\n",
    "\n",
    "# # --- Load feature vectors ---\n",
    "# feature_vector_path = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\ngram_feature_vector_new.csv\"\n",
    "# fv_df = pd.read_csv(feature_vector_path)\n",
    "# feature_vectors = {\n",
    "#     row['Sequence_ID']: row.drop('Sequence_ID').values.astype(float)\n",
    "#     for _, row in fv_df.iterrows()\n",
    "# }\n",
    "\n",
    "# # --- ID Cleaning Function ---\n",
    "# def clean_id(protein_id):\n",
    "#     if \"|\" in protein_id:\n",
    "#         parts = protein_id.split(\"|\")\n",
    "#         if len(parts) > 1:\n",
    "#             return parts[1]\n",
    "#     return protein_id\n",
    "\n",
    "# # --- Function to create dataset ---\n",
    "# def create_pair_dataset(input_csv_path, output_tsv_path):\n",
    "#     df = pd.read_csv(input_csv_path)\n",
    "\n",
    "#     lines = []\n",
    "#     missing_count = 0\n",
    "\n",
    "#     for _, row in df.iterrows():\n",
    "#         id1 = clean_id(row['IntA'])\n",
    "#         id2 = clean_id(row['IntB'])\n",
    "#         label = row['Cls']\n",
    "\n",
    "#         if id1 in feature_vectors and id2 in feature_vectors:\n",
    "#             vec1 = feature_vectors[id1]\n",
    "#             vec2 = feature_vectors[id2]\n",
    "#             pair_id = f\"{id1}+{id2}\"\n",
    "#             combined_vec = np.concatenate([vec1, vec2])\n",
    "#             combined_str = \" \".join(map(str, combined_vec))\n",
    "#             line = f\"{pair_id}\\t{combined_str}\\t{label}\"\n",
    "#             lines.append(line)\n",
    "#         else:\n",
    "#             missing_count += 1\n",
    "\n",
    "#     print(f\"{os.path.basename(input_csv_path)} -> Valid: {len(lines)}, Missing: {missing_count}\")\n",
    "\n",
    "#     # Write output\n",
    "#     os.makedirs(os.path.dirname(output_tsv_path), exist_ok=True)\n",
    "#     with open(output_tsv_path, 'w') as f:\n",
    "#         for line in lines:\n",
    "#             f.write(line + '\\n')\n",
    "\n",
    "# # --- Dataset paths ---\n",
    "# datasets = [\n",
    "#     {\n",
    "#         \"train_input\": r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Dataset\\Dataset_C1\\TrainSet1.csv\",\n",
    "#         \"train_output\": r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Dataset_output\\For_C1\\For_Train\\train_dataset11.tsv\",\n",
    "#         \"test_input\": r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Dataset\\Dataset_C1\\TestSet1.csv\",\n",
    "#         \"test_output\": r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Dataset_output\\For_C1\\For_Test\\test_dataset11.tsv\",\n",
    "#     },\n",
    "#     {\n",
    "#         \"train_input\": r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Dataset\\Dataset_C1\\TrainSet2.csv\",\n",
    "#         \"train_output\": r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Dataset_output\\For_C1\\For_Train\\train_dataset12.tsv\",\n",
    "#         \"test_input\": r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Dataset\\Dataset_C1\\TestSet2.csv\",\n",
    "#         \"test_output\": r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Dataset_output\\For_C1\\For_Test\\test_dataset12.tsv\",\n",
    "#     },\n",
    "#     {\n",
    "#         \"train_input\": r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Dataset\\Dataset_C1\\TrainSet3.csv\",\n",
    "#         \"train_output\": r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Dataset_output\\For_C1\\For_Train\\train_dataset13.tsv\",\n",
    "#         \"test_input\": r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Dataset\\Dataset_C1\\TestSet3.csv\",\n",
    "#         \"test_output\": r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Dataset_output\\For_C1\\For_Test\\test_dataset13.tsv\",\n",
    "#     },\n",
    "#     {\n",
    "#         \"train_input\": r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Dataset\\Dataset_C1\\TrainSet4.csv\",\n",
    "#         \"train_output\": r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Dataset_output\\For_C1\\For_Train\\train_dataset14.tsv\",\n",
    "#         \"test_input\": r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Dataset\\Dataset_C1\\TestSet4.csv\",\n",
    "#         \"test_output\": r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Dataset_output\\For_C1\\For_Test\\test_dataset14.tsv\",\n",
    "#     },\n",
    "# ]\n",
    "\n",
    "# # --- Run for all datasets ---\n",
    "# for dataset in datasets:\n",
    "#     create_pair_dataset(dataset[\"train_input\"], dataset[\"train_output\"])\n",
    "#     create_pair_dataset(dataset[\"test_input\"], dataset[\"test_output\"])\n",
    "\n",
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import os\n",
    "\n",
    "# # --- Load feature vectors from CSV ---\n",
    "# feature_vector_path = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\ngram_feature_vector_new.csv\"\n",
    "# fv_df = pd.read_csv(feature_vector_path)\n",
    "# feature_vectors = {\n",
    "#     row['Sequence_ID']: row.drop('Sequence_ID').values.astype(float)\n",
    "#     for _, row in fv_df.iterrows()\n",
    "# }\n",
    "\n",
    "# # --- Clean protein ID ---\n",
    "# def clean_id(protein_id):\n",
    "#     if \"|\" in protein_id:\n",
    "#         parts = protein_id.split(\"|\")\n",
    "#         if len(parts) > 1:\n",
    "#             return parts[1]\n",
    "#     return protein_id\n",
    "\n",
    "# # --- Function to create final dataset ---\n",
    "# def create_pair_dataset(input_csv_path, output_tsv_path):\n",
    "#     df = pd.read_csv(input_csv_path)\n",
    "#     lines = []\n",
    "#     missing_count = 0\n",
    "\n",
    "#     for _, row in df.iterrows():\n",
    "#         id1 = clean_id(row['IntA'])\n",
    "#         id2 = clean_id(row['IntB'])\n",
    "#         label = row['Cls']\n",
    "\n",
    "#         if id1 in feature_vectors and id2 in feature_vectors:\n",
    "#             vec1 = feature_vectors[id1]\n",
    "#             vec2 = feature_vectors[id2]\n",
    "#             pair_id = f\"{id1}+{id2}\"\n",
    "#             combined_vec = np.concatenate([vec1, vec2])\n",
    "#             combined_str = \" \".join(map(str, combined_vec))\n",
    "#             line = f\"{pair_id}\\t{combined_str}\\t{label}\"\n",
    "#             lines.append(line)\n",
    "#         else:\n",
    "#             missing_count += 1\n",
    "\n",
    "#     print(f\"{os.path.basename(input_csv_path)} → Valid pairs: {len(lines)}, Missing: {missing_count}\")\n",
    "\n",
    "#     with open(output_tsv_path, 'w') as f:\n",
    "#         for line in lines:\n",
    "#             f.write(line + '\\n')\n",
    "\n",
    "# # --- All input/output pairs ---\n",
    "# dataset_paths = [\n",
    "#     # C1\n",
    "#     (11, \"C1\"), (12, \"C1\"), (13, \"C1\"), (14, \"C1\"), (15, \"C1\"),\n",
    "#     # C2\n",
    "#     (21, \"C2\"), (22, \"C2\"), (23, \"C2\"), (24, \"C2\"), (25, \"C2\"),\n",
    "#     # C3\n",
    "#     (31, \"C3\"), (32, \"C3\"), (33, \"C3\"), (34, \"C3\"), (35, \"C3\"),\n",
    "# ]\n",
    "\n",
    "# # --- Run in loop ---\n",
    "# base_input = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Dataset\"\n",
    "# base_output = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Dataset_output\"\n",
    "\n",
    "# for num, cat in dataset_paths:\n",
    "#     train_input = fr\"{base_input}\\Dataset_{cat}\\TrainSet{str(num)[-1]}.csv\"\n",
    "#     train_output = fr\"{base_output}\\For_{cat}\\For_Train\\train_dataset{num}.tsv\"\n",
    "#     test_input = fr\"{base_input}\\Dataset_{cat}\\TestSet{str(num)[-1]}.csv\"\n",
    "#     test_output = fr\"{base_output}\\For_{cat}\\For_Test\\test_dataset{num}.tsv\"\n",
    "\n",
    "#     create_pair_dataset(train_input, train_output)\n",
    "#     create_pair_dataset(test_input, test_output)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# --- Load feature vectors from CSV ---\n",
    "feature_vector_path = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\ngram_feature_vector_new.csv\"\n",
    "fv_df = pd.read_csv(feature_vector_path)\n",
    "feature_vectors = {\n",
    "    row['Sequence_ID']: row.drop('Sequence_ID').values.astype(float)\n",
    "    for _, row in fv_df.iterrows()\n",
    "}\n",
    "\n",
    "# --- Clean protein ID ---\n",
    "def clean_id(protein_id):\n",
    "    if \"|\" in protein_id:\n",
    "        parts = protein_id.split(\"|\")\n",
    "        if len(parts) > 1:\n",
    "            return parts[1]\n",
    "    return protein_id\n",
    "\n",
    "# --- Function to create final dataset ---\n",
    "def create_pair_dataset(input_csv_path, output_tsv_path):\n",
    "    df = pd.read_csv(input_csv_path)\n",
    "\n",
    "    # Check for expected columns\n",
    "    required_cols = {'IntA', 'IntB', 'Cls'}\n",
    "    if not required_cols.issubset(df.columns): \n",
    "        # print(f\"[!] Missing columns in {os.path.basename(input_csv_path)} → Found: {df.columns.tolist()}\")\n",
    "        required_cols = {'IntA', 'IntB', 'Class'}\n",
    "        if not required_cols.issubset(df.columns): \n",
    "           print(f\"[!] Missing columns in {os.path.basename(input_csv_path)} → Found: 'IntA', 'IntB', 'Cls' or 'Class'\")\n",
    "           return\n",
    "\n",
    "    lines = []\n",
    "    missing_count = 0\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        id1 = clean_id(row['IntA'])\n",
    "        id2 = clean_id(row['IntB'])\n",
    "        # label = row['Cls']\n",
    "        \n",
    "        if 'Cls' in row:\n",
    "            label = row['Cls']\n",
    "        elif 'Class' in row:\n",
    "            label = row['Class']\n",
    "        else:\n",
    "            # print(\" Label column not found in row\")\n",
    "            continue\n",
    "\n",
    "\n",
    "        if id1 in feature_vectors and id2 in feature_vectors:\n",
    "            vec1 = feature_vectors[id1]\n",
    "            vec2 = feature_vectors[id2]\n",
    "            pair_id = f\"{id1}+{id2}\"\n",
    "            combined_vec = np.concatenate([vec1, vec2])\n",
    "            combined_str = \" \".join(map(str, combined_vec))\n",
    "            line = f\"{pair_id}\\t{combined_str}\\t{label}\"\n",
    "            lines.append(line)\n",
    "        else:\n",
    "            missing_count += 1\n",
    "\n",
    "    print(f\"{os.path.basename(input_csv_path)} → Valid pairs: {len(lines)}, Missing: {missing_count}\")\n",
    "\n",
    "    os.makedirs(os.path.dirname(output_tsv_path), exist_ok=True)\n",
    "    with open(output_tsv_path, 'w') as f:\n",
    "        for line in lines:\n",
    "            f.write(line + '\\n')\n",
    "\n",
    "# --- All input/output pairs ---\n",
    "dataset_paths = {\n",
    "    \"C1\": [11, 12, 13, 14, 15],\n",
    "    \"C2\": [21, 22, 23, 24, 25],\n",
    "    \"C3\": [31, 32, 33, 34, 35],\n",
    "}\n",
    "\n",
    "# --- Run in loop ---\n",
    "base_input = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Dataset\"\n",
    "base_output = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Dataset_output\"\n",
    "\n",
    "for category, nums in dataset_paths.items():\n",
    "    print(f\"\\n For DataSet inside {category} :- \")\n",
    "    for num in nums:\n",
    "        idx = str(num)[-1]  # Get 1-5 for C1, etc.\n",
    "\n",
    "        train_input = fr\"{base_input}\\Dataset_{category}\\TrainSet{idx}.csv\"\n",
    "        train_output = fr\"{base_output}\\For_{category}\\For_Train\\train_dataset{num}.tsv\"\n",
    "        test_input = fr\"{base_input}\\Dataset_{category}\\TestSet{idx}.csv\"\n",
    "        test_output = fr\"{base_output}\\For_{category}\\For_Test\\test_dataset{num}.tsv\"\n",
    "\n",
    "        create_pair_dataset(train_input, train_output)\n",
    "        create_pair_dataset(test_input, test_output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a70d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# # === Step 1: Load feature vectors ===\n",
    "# features_path = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\ngram_feature_vector_new.csv\"  # Dummy path\n",
    "\n",
    "# # Load and clean IDs like sp|P27540|ARNT_HUMAN --> P27540\n",
    "# features_df = pd.read_csv(features_path, header=None)\n",
    "# features_df.rename(columns={0: \"Raw_ID\"}, inplace=True)\n",
    "# features_df[\"Sequence_ID\"] = features_df[\"Raw_ID\"].apply(lambda x: x.split('|')[1] if '|' in x else x)\n",
    "# features_df.drop(columns=[\"Raw_ID\"], inplace=True)\n",
    "\n",
    "# # Create feature dictionary\n",
    "# feature_dict = {\n",
    "#     row[\"Sequence_ID\"]: row[1:].values.astype(float)\n",
    "#     for _, row in features_df.iterrows()\n",
    "# }\n",
    "\n",
    "# # === Step 2: Load interaction pairs ===\n",
    "# train_csv_path = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\TrainSet1.csv\"  # Dummy path\n",
    "# test_csv_path = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\TestSet1.csv\"    # Dummy path\n",
    "\n",
    "# train_df = pd.read_csv(train_csv_path)\n",
    "# test_df = pd.read_csv(test_csv_path)\n",
    "\n",
    "# # === Step 3: Function to generate formatted data ===\n",
    "# def generate_formatted_lines(pairs_df, feature_dict):\n",
    "#     lines = []\n",
    "    \n",
    "#     for _, row in pairs_df.iterrows():\n",
    "#         id1 = str(row[\"IntA\"])\n",
    "#         id2 = str(row[\"IntB\"])\n",
    "#         label = str(row[\"Cls\"])\n",
    "\n",
    "#         if id1 in feature_dict and id2 in feature_dict:\n",
    "#             vec1 = feature_dict[id1]\n",
    "#             vec2 = feature_dict[id2]\n",
    "#             combined = np.concatenate([vec1, vec2])\n",
    "#             vec_str = ' '.join(map(str, combined))\n",
    "#             line = f\"{id1}+{id2}\\t{vec_str}\\t{label}\"\n",
    "#             lines.append(line)\n",
    "#         else:\n",
    "#             print(f\"Skipping pair (missing vector): {id1}, {id2}\")\n",
    "    \n",
    "#     return lines\n",
    "\n",
    "# # === Step 4: Generate and write outputs ===\n",
    "# train_lines = generate_formatted_lines(train_df, feature_dict)\n",
    "# test_lines = generate_formatted_lines(test_df, feature_dict)\n",
    "\n",
    "# train_output_path = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\train_dataset11.tsv\"  # Dummy path\n",
    "# test_output_path = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\test_dataset11.tsv\"    # Dummy path\n",
    "\n",
    "# with open(train_output_path, 'w') as f:\n",
    "#     for line in train_lines:\n",
    "#         f.write(line + '\\n')\n",
    "\n",
    "# with open(test_output_path, 'w') as f:\n",
    "#     for line in test_lines:\n",
    "#         f.write(line + '\\n')\n",
    "\n",
    "# print(\" Train and test datasets saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e88dab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing Category C1 ---\n",
      "Skipping pair (missing vector): P0DI81, Q7Z392\n",
      "Skipping pair (missing vector): P0DI81, Q8IUR0\n",
      "Skipping pair (missing vector): O75376, Q9NNW7\n",
      "Skipping pair (missing vector): P26678, O00631\n",
      "Skipping pair (missing vector): O75865, P0DI81\n",
      "Skipping pair (missing vector): Q92813, P62253\n",
      "Skipping pair (missing vector): P55072, Q9BQE4\n",
      "Skipping pair (missing vector): Q92813, O60337\n",
      "Skipping pair (missing vector): Q96G74, P36969\n",
      "Skipping pair (missing vector): Q9Y296, P0DI81\n",
      "Skipping pair (missing vector): P63261, P62328\n",
      "Skipping pair (missing vector): Q9BQE4, Q86TM6\n",
      "Skipping pair (missing vector): P03372, Q16881\n",
      "Skipping pair (missing vector): P67870, P62945\n",
      "Skipping pair (missing vector): Q92813, Q9Y6I7\n",
      "Skipping pair (missing vector): P06733, P0DI81\n",
      "Skipping pair (missing vector): P68133, P62328\n",
      "Skipping pair (missing vector): O43617, P0DI81\n",
      "Skipping pair (missing vector): Q86SZ2, P0DI81\n",
      "Skipping pair (missing vector): P40692, P62328\n",
      "Skipping pair (missing vector): P17980, Q9NNW7\n",
      "Skipping pair (missing vector): Q8TEY7, Q92813\n",
      "Skipping pair (missing vector): Q8ND90, P0CW24\n",
      "Skipping pair (missing vector): P00519, P07203\n",
      "Skipping pair (missing vector): Q8IZJ3, P0DJD3\n",
      "Skipping pair (missing vector): P47900, P0DJD3\n",
      "Skipping pair (missing vector): Q9NRD8, P86790\n",
      "Skipping pair (missing vector): Q8N7J2, P86790\n",
      "Skipping pair (missing vector): O43653, P86790\n",
      "Skipping pair (missing vector): Q8N3J6, Q9NNW7\n",
      "Skipping pair (missing vector): Q9P0L9, Q9NNW7\n",
      "Skipping pair (missing vector): Q9Y5S8, Q9BVL4\n",
      "Skipping pair (missing vector): P28221, P0DJD3\n",
      "Skipping pair (missing vector): Q1EHB4, Q9NNW7\n",
      "Skipping pair (missing vector): Q9NZC2, Q9NNW7\n",
      "Skipping pair (missing vector): P41247, O60613\n",
      "Skipping pair (missing vector): P51684, O60613\n",
      "Skipping pair (missing vector): Q9BQ51, P86790\n",
      "Skipping pair (missing vector): Q7L5Y1, O60613\n",
      "Skipping pair (missing vector): O75064, Q9NNW7\n",
      "Skipping pair (missing vector): Q9Y5R4, O60613\n",
      "Skipping pair (missing vector): Q9Y225, P0DJD3\n",
      "Skipping pair (missing vector): Q9Y225, Q9BVL4\n",
      "Skipping pair (missing vector): Q9Y225, Q9NNW7\n",
      "Skipping pair (missing vector): Q9BQ51, P0DJD3\n",
      "Skipping pair (missing vector): Q3MUY2, P0DJD3\n",
      "Skipping pair (missing vector): Q9NXG6, Q9BVL4\n",
      "Skipping pair (missing vector): Q9NXG6, Q9NNW7\n",
      "Skipping pair (missing vector): Q6P179, P55073\n",
      "Skipping pair (missing vector): Q9BVX2, Q9BVL4\n",
      "Skipping pair (missing vector): Q96E14, O60613\n",
      "Skipping pair (missing vector): Q8WWZ8, Q9BVL4\n",
      "Skipping pair (missing vector): Q02161, Q9BVL4\n",
      "Skipping pair (missing vector): Q8WV37, O60613\n",
      "Skipping pair (missing vector): Q9H9Q4, P86790\n",
      "Skipping pair (missing vector): Q96KN3, O60613\n",
      "Skipping pair (missing vector): Q9HBX8, O60613\n",
      "Skipping pair (missing vector): Q9H633, Q9NNW7\n",
      "Skipping pair (missing vector): A6BM72, Q9BVL4\n",
      "Skipping pair (missing vector): Q9NY46, Q9BVL4\n",
      "Skipping pair (missing vector): P36021, Q9BVL4\n",
      "Skipping pair (missing vector): O14493, Q9BVL4\n",
      "Skipping pair (missing vector): Q96LA6, O60613\n",
      "Skipping pair (missing vector): Q14832, P86790\n",
      "Skipping pair (missing vector): Q9BYE9, Q9NNW7\n",
      "Skipping pair (missing vector): Q8NHJ6, Q9BVL4\n",
      "Skipping pair (missing vector): O95832, Q9NNW7\n",
      "Skipping pair (missing vector): Q6UX01, P86790\n",
      "Skipping pair (missing vector): A5PKW4, Q9BVL4\n",
      "Skipping pair (missing vector): Q9BY07, P86790\n",
      "Skipping pair (missing vector): Q9NY25, P86790\n",
      "Skipping pair (missing vector): Q14916, Q9NNW7\n",
      "Skipping pair (missing vector): P23352, P0DJD3\n",
      "Skipping pair (missing vector): Q7Z403, Q9NNW7\n",
      "Skipping pair (missing vector): Q92521, P55073\n",
      "Skipping pair (missing vector): Q969S6, P0DJD3\n",
      "Skipping pair (missing vector): Q2TAA5, P0DJD3\n",
      "Skipping pair (missing vector): Q9NPG8, Q9BVL4\n",
      "Skipping pair (missing vector): Q9NQQ7, P86790\n",
      "Skipping pair (missing vector): Q7Z7K0, P0DJD3\n",
      "Skipping pair (missing vector): Q14435, P0DJD3\n",
      "Skipping pair (missing vector): Q9H5J4, P86790\n",
      "Skipping pair (missing vector): Q8NCN2, P86790\n",
      "Skipping pair (missing vector): Q6UVM3, Q9NNW7\n",
      "Skipping pair (missing vector): Q8NE79, Q9NNW7\n",
      "Skipping pair (missing vector): Q7L5A8, Q9NNW7\n",
      "Skipping pair (missing vector): Q8IYJ3, Q9BVL4\n",
      "Skipping pair (missing vector): O15120, P55073\n",
      "Skipping pair (missing vector): Q92536, O60613\n",
      "Skipping pair (missing vector): P23141, P55073\n",
      "Skipping pair (missing vector): Q6UXZ4, Q9NNW7\n",
      "Skipping pair (missing vector): Q9H3R2, Q9BVL4\n",
      "Skipping pair (missing vector): P28566, Q9BVL4\n",
      "Skipping pair (missing vector): Q14957, Q9BVL4\n",
      "Skipping pair (missing vector): P55073, Q8NDC4\n",
      "Skipping pair (missing vector): P55073, Q5BJD5\n",
      "Skipping pair (missing vector): P55061, P86790\n",
      "Skipping pair (missing vector): O60760, P86790\n",
      "Skipping pair (missing vector): Q9Y3S2, Q9NNW7\n",
      "Skipping pair (missing vector): O95180, Q9BVL4\n",
      "Skipping pair (missing vector): Q9BVL4, Q765P7\n",
      "Skipping pair (missing vector): Q9BVL4, Q8WXC3\n",
      "Skipping pair (missing vector): Q9BVL4, Q96M11\n",
      "Skipping pair (missing vector): Q9BVL4, Q9ULE3\n",
      "Skipping pair (missing vector): Q9BVL4, Q9NR23\n",
      "Skipping pair (missing vector): Q9BVL4, Q86WI3\n",
      "Skipping pair (missing vector): Q9BVL4, P22732\n",
      "Skipping pair (missing vector): Q9BVL4, P48764\n",
      "Skipping pair (missing vector): Q9BVL4, Q9HBX9\n",
      "Skipping pair (missing vector): P0DJD3, Q2M329\n",
      "Skipping pair (missing vector): P0DJD3, Q8IZU9\n",
      "Skipping pair (missing vector): A9QM74, P55073\n",
      "Skipping pair (missing vector): Q5SQ64, Q9BVL4\n",
      "Skipping pair (missing vector): P86790, Q86WI3\n",
      "Skipping pair (missing vector): P86790, Q86YB8\n",
      "Skipping pair (missing vector): Q9H400, Q9BVL4\n",
      "Skipping pair (missing vector): Q9NNW7, Q8IYA6\n",
      "Skipping pair (missing vector): Q9NNW7, Q96QP1\n",
      "Skipping pair (missing vector): Q9NNW7, Q9Y6Q2\n",
      "Skipping pair (missing vector): Q9NNW7, Q8N8W4\n",
      "Skipping pair (missing vector): Q9NNW7, Q9NQ75\n",
      "Skipping pair (missing vector): Q9NNW7, Q9HBX9\n",
      "Skipping pair (missing vector): Q9NNW7, O15321\n",
      "Skipping pair (missing vector): Q5H9S7, Q9NNW7\n",
      "Skipping pair (missing vector): P0DI81, Q96Q05\n",
      "Skipping pair (missing vector): Q14686, Q9NNW7\n",
      "Skipping pair (missing vector): O00631, O14983\n",
      "Skipping pair (missing vector): Q3I5F7, Q9NNW7\n",
      "Skipping pair (missing vector): Q60I27, P55073\n",
      "Skipping pair (missing vector): O14960, Q9BVL4\n",
      "Skipping pair (missing vector): Q2PPJ7, P55073\n",
      "→ Saved: train_dataset11.tsv & test_dataset11.tsv\n",
      "Skipping pair (missing vector): P0DI81, Q96Q05\n",
      "Skipping pair (missing vector): Q14686, Q9NNW7\n",
      "Skipping pair (missing vector): O00631, O14983\n",
      "Skipping pair (missing vector): Q92813, P62253\n",
      "Skipping pair (missing vector): P55072, Q9BQE4\n",
      "Skipping pair (missing vector): Q92813, O60337\n",
      "Skipping pair (missing vector): Q96G74, P36969\n",
      "Skipping pair (missing vector): Q9Y296, P0DI81\n",
      "Skipping pair (missing vector): P63261, P62328\n",
      "Skipping pair (missing vector): Q9BQE4, Q86TM6\n",
      "Skipping pair (missing vector): P03372, Q16881\n",
      "Skipping pair (missing vector): P67870, P62945\n",
      "Skipping pair (missing vector): Q92813, Q9Y6I7\n",
      "Skipping pair (missing vector): P06733, P0DI81\n",
      "Skipping pair (missing vector): P68133, P62328\n",
      "Skipping pair (missing vector): O43617, P0DI81\n",
      "Skipping pair (missing vector): Q86SZ2, P0DI81\n",
      "Skipping pair (missing vector): P40692, P62328\n",
      "Skipping pair (missing vector): P17980, Q9NNW7\n",
      "Skipping pair (missing vector): Q8TEY7, Q92813\n",
      "Skipping pair (missing vector): Q8ND90, P0CW24\n",
      "Skipping pair (missing vector): P00519, P07203\n",
      "Skipping pair (missing vector): Q3I5F7, Q9NNW7\n",
      "Skipping pair (missing vector): Q60I27, P55073\n",
      "Skipping pair (missing vector): O14960, Q9BVL4\n",
      "Skipping pair (missing vector): Q2PPJ7, P55073\n",
      "Skipping pair (missing vector): Q8N3J6, Q9NNW7\n",
      "Skipping pair (missing vector): Q9P0L9, Q9NNW7\n",
      "Skipping pair (missing vector): Q9Y5S8, Q9BVL4\n",
      "Skipping pair (missing vector): P28221, P0DJD3\n",
      "Skipping pair (missing vector): Q1EHB4, Q9NNW7\n",
      "Skipping pair (missing vector): Q9NZC2, Q9NNW7\n",
      "Skipping pair (missing vector): P41247, O60613\n",
      "Skipping pair (missing vector): P51684, O60613\n",
      "Skipping pair (missing vector): Q9BQ51, P86790\n",
      "Skipping pair (missing vector): Q7L5Y1, O60613\n",
      "Skipping pair (missing vector): O75064, Q9NNW7\n",
      "Skipping pair (missing vector): Q9Y5R4, O60613\n",
      "Skipping pair (missing vector): Q9Y225, P0DJD3\n",
      "Skipping pair (missing vector): Q9Y225, Q9BVL4\n",
      "Skipping pair (missing vector): Q9Y225, Q9NNW7\n",
      "Skipping pair (missing vector): Q9BQ51, P0DJD3\n",
      "Skipping pair (missing vector): Q3MUY2, P0DJD3\n",
      "Skipping pair (missing vector): Q9NXG6, Q9BVL4\n",
      "Skipping pair (missing vector): Q9NXG6, Q9NNW7\n",
      "Skipping pair (missing vector): Q6P179, P55073\n",
      "Skipping pair (missing vector): Q9BVX2, Q9BVL4\n",
      "Skipping pair (missing vector): Q96E14, O60613\n",
      "Skipping pair (missing vector): Q8WWZ8, Q9BVL4\n",
      "Skipping pair (missing vector): Q02161, Q9BVL4\n",
      "Skipping pair (missing vector): Q8WV37, O60613\n",
      "Skipping pair (missing vector): Q9H9Q4, P86790\n",
      "Skipping pair (missing vector): Q96KN3, O60613\n",
      "Skipping pair (missing vector): Q9HBX8, O60613\n",
      "Skipping pair (missing vector): Q9H633, Q9NNW7\n",
      "Skipping pair (missing vector): A6BM72, Q9BVL4\n",
      "Skipping pair (missing vector): Q9NY46, Q9BVL4\n",
      "Skipping pair (missing vector): P36021, Q9BVL4\n",
      "Skipping pair (missing vector): O14493, Q9BVL4\n",
      "Skipping pair (missing vector): Q96LA6, O60613\n",
      "Skipping pair (missing vector): Q14832, P86790\n",
      "Skipping pair (missing vector): Q9BYE9, Q9NNW7\n",
      "Skipping pair (missing vector): Q8NHJ6, Q9BVL4\n",
      "Skipping pair (missing vector): O95832, Q9NNW7\n",
      "Skipping pair (missing vector): Q6UX01, P86790\n",
      "Skipping pair (missing vector): A5PKW4, Q9BVL4\n",
      "Skipping pair (missing vector): Q9BY07, P86790\n",
      "Skipping pair (missing vector): Q9NY25, P86790\n",
      "Skipping pair (missing vector): Q14916, Q9NNW7\n",
      "Skipping pair (missing vector): P23352, P0DJD3\n",
      "Skipping pair (missing vector): Q7Z403, Q9NNW7\n",
      "Skipping pair (missing vector): Q92521, P55073\n",
      "Skipping pair (missing vector): Q969S6, P0DJD3\n",
      "Skipping pair (missing vector): Q2TAA5, P0DJD3\n",
      "Skipping pair (missing vector): Q9NPG8, Q9BVL4\n",
      "Skipping pair (missing vector): Q9NQQ7, P86790\n",
      "Skipping pair (missing vector): Q7Z7K0, P0DJD3\n",
      "Skipping pair (missing vector): Q14435, P0DJD3\n",
      "Skipping pair (missing vector): Q9H5J4, P86790\n",
      "Skipping pair (missing vector): Q8NCN2, P86790\n",
      "Skipping pair (missing vector): Q6UVM3, Q9NNW7\n",
      "Skipping pair (missing vector): Q8NE79, Q9NNW7\n",
      "Skipping pair (missing vector): Q7L5A8, Q9NNW7\n",
      "Skipping pair (missing vector): Q8IYJ3, Q9BVL4\n",
      "Skipping pair (missing vector): O15120, P55073\n",
      "Skipping pair (missing vector): Q92536, O60613\n",
      "Skipping pair (missing vector): P23141, P55073\n",
      "Skipping pair (missing vector): Q6UXZ4, Q9NNW7\n",
      "Skipping pair (missing vector): Q9H3R2, Q9BVL4\n",
      "Skipping pair (missing vector): P28566, Q9BVL4\n",
      "Skipping pair (missing vector): Q14957, Q9BVL4\n",
      "Skipping pair (missing vector): P55073, Q8NDC4\n",
      "Skipping pair (missing vector): P55073, Q5BJD5\n",
      "Skipping pair (missing vector): P55061, P86790\n",
      "Skipping pair (missing vector): O60760, P86790\n",
      "Skipping pair (missing vector): Q9Y3S2, Q9NNW7\n",
      "Skipping pair (missing vector): O95180, Q9BVL4\n",
      "Skipping pair (missing vector): Q9BVL4, Q765P7\n",
      "Skipping pair (missing vector): Q9BVL4, Q8WXC3\n",
      "Skipping pair (missing vector): Q9BVL4, Q96M11\n",
      "Skipping pair (missing vector): Q9BVL4, Q9ULE3\n",
      "Skipping pair (missing vector): Q9BVL4, Q9NR23\n",
      "Skipping pair (missing vector): Q9BVL4, Q86WI3\n",
      "Skipping pair (missing vector): Q9BVL4, P22732\n",
      "Skipping pair (missing vector): Q9BVL4, P48764\n",
      "Skipping pair (missing vector): Q9BVL4, Q9HBX9\n",
      "Skipping pair (missing vector): P0DJD3, Q2M329\n",
      "Skipping pair (missing vector): P0DJD3, Q8IZU9\n",
      "Skipping pair (missing vector): A9QM74, P55073\n",
      "Skipping pair (missing vector): Q5SQ64, Q9BVL4\n",
      "Skipping pair (missing vector): P86790, Q86WI3\n",
      "Skipping pair (missing vector): P86790, Q86YB8\n",
      "Skipping pair (missing vector): Q9H400, Q9BVL4\n",
      "Skipping pair (missing vector): Q9NNW7, Q8IYA6\n",
      "Skipping pair (missing vector): Q9NNW7, Q96QP1\n",
      "Skipping pair (missing vector): Q9NNW7, Q9Y6Q2\n",
      "Skipping pair (missing vector): Q9NNW7, Q8N8W4\n",
      "Skipping pair (missing vector): Q9NNW7, Q9NQ75\n",
      "Skipping pair (missing vector): Q9NNW7, Q9HBX9\n",
      "Skipping pair (missing vector): Q9NNW7, O15321\n",
      "Skipping pair (missing vector): Q5H9S7, Q9NNW7\n",
      "Skipping pair (missing vector): P0DI81, Q7Z392\n",
      "Skipping pair (missing vector): P0DI81, Q8IUR0\n",
      "Skipping pair (missing vector): O75376, Q9NNW7\n",
      "Skipping pair (missing vector): P26678, O00631\n",
      "Skipping pair (missing vector): O75865, P0DI81\n",
      "Skipping pair (missing vector): Q8IZJ3, P0DJD3\n",
      "Skipping pair (missing vector): P47900, P0DJD3\n",
      "Skipping pair (missing vector): Q9NRD8, P86790\n",
      "Skipping pair (missing vector): Q8N7J2, P86790\n",
      "Skipping pair (missing vector): O43653, P86790\n",
      "→ Saved: train_dataset12.tsv & test_dataset12.tsv\n",
      "Skipping pair (missing vector): P0DI81, Q96Q05\n",
      "Skipping pair (missing vector): Q14686, Q9NNW7\n",
      "Skipping pair (missing vector): O00631, O14983\n",
      "Skipping pair (missing vector): P0DI81, Q7Z392\n",
      "Skipping pair (missing vector): P0DI81, Q8IUR0\n",
      "Skipping pair (missing vector): O75376, Q9NNW7\n",
      "Skipping pair (missing vector): P26678, O00631\n",
      "Skipping pair (missing vector): O75865, P0DI81\n",
      "Skipping pair (missing vector): P55072, Q9BQE4\n",
      "Skipping pair (missing vector): Q92813, O60337\n",
      "Skipping pair (missing vector): Q96G74, P36969\n",
      "Skipping pair (missing vector): Q9Y296, P0DI81\n",
      "Skipping pair (missing vector): P63261, P62328\n",
      "Skipping pair (missing vector): Q9BQE4, Q86TM6\n",
      "Skipping pair (missing vector): P03372, Q16881\n",
      "Skipping pair (missing vector): P67870, P62945\n",
      "Skipping pair (missing vector): Q92813, Q9Y6I7\n",
      "Skipping pair (missing vector): P06733, P0DI81\n",
      "Skipping pair (missing vector): P68133, P62328\n",
      "Skipping pair (missing vector): O43617, P0DI81\n",
      "Skipping pair (missing vector): Q86SZ2, P0DI81\n",
      "Skipping pair (missing vector): P40692, P62328\n",
      "Skipping pair (missing vector): P17980, Q9NNW7\n",
      "Skipping pair (missing vector): Q8TEY7, Q92813\n",
      "Skipping pair (missing vector): Q8ND90, P0CW24\n",
      "Skipping pair (missing vector): P00519, P07203\n",
      "Skipping pair (missing vector): Q3I5F7, Q9NNW7\n",
      "Skipping pair (missing vector): Q60I27, P55073\n",
      "Skipping pair (missing vector): O14960, Q9BVL4\n",
      "Skipping pair (missing vector): Q2PPJ7, P55073\n",
      "Skipping pair (missing vector): Q8IZJ3, P0DJD3\n",
      "Skipping pair (missing vector): P47900, P0DJD3\n",
      "Skipping pair (missing vector): Q9NRD8, P86790\n",
      "Skipping pair (missing vector): Q8N7J2, P86790\n",
      "Skipping pair (missing vector): O43653, P86790\n",
      "Skipping pair (missing vector): P41247, O60613\n",
      "Skipping pair (missing vector): P51684, O60613\n",
      "Skipping pair (missing vector): Q9BQ51, P86790\n",
      "Skipping pair (missing vector): Q7L5Y1, O60613\n",
      "Skipping pair (missing vector): O75064, Q9NNW7\n",
      "Skipping pair (missing vector): Q9Y5R4, O60613\n",
      "Skipping pair (missing vector): Q9Y225, P0DJD3\n",
      "Skipping pair (missing vector): Q9Y225, Q9BVL4\n",
      "Skipping pair (missing vector): Q9Y225, Q9NNW7\n",
      "Skipping pair (missing vector): Q9BQ51, P0DJD3\n",
      "Skipping pair (missing vector): Q3MUY2, P0DJD3\n",
      "Skipping pair (missing vector): Q9NXG6, Q9BVL4\n",
      "Skipping pair (missing vector): Q9NXG6, Q9NNW7\n",
      "Skipping pair (missing vector): Q6P179, P55073\n",
      "Skipping pair (missing vector): Q9BVX2, Q9BVL4\n",
      "Skipping pair (missing vector): Q96E14, O60613\n",
      "Skipping pair (missing vector): Q8WWZ8, Q9BVL4\n",
      "Skipping pair (missing vector): Q02161, Q9BVL4\n",
      "Skipping pair (missing vector): Q8WV37, O60613\n",
      "Skipping pair (missing vector): Q9H9Q4, P86790\n",
      "Skipping pair (missing vector): Q96KN3, O60613\n",
      "Skipping pair (missing vector): Q9HBX8, O60613\n",
      "Skipping pair (missing vector): Q9H633, Q9NNW7\n",
      "Skipping pair (missing vector): A6BM72, Q9BVL4\n",
      "Skipping pair (missing vector): Q9NY46, Q9BVL4\n",
      "Skipping pair (missing vector): P36021, Q9BVL4\n",
      "Skipping pair (missing vector): O14493, Q9BVL4\n",
      "Skipping pair (missing vector): Q96LA6, O60613\n",
      "Skipping pair (missing vector): Q14832, P86790\n",
      "Skipping pair (missing vector): Q9BYE9, Q9NNW7\n",
      "Skipping pair (missing vector): Q8NHJ6, Q9BVL4\n",
      "Skipping pair (missing vector): O95832, Q9NNW7\n",
      "Skipping pair (missing vector): Q6UX01, P86790\n",
      "Skipping pair (missing vector): A5PKW4, Q9BVL4\n",
      "Skipping pair (missing vector): Q9BY07, P86790\n",
      "Skipping pair (missing vector): Q9NY25, P86790\n",
      "Skipping pair (missing vector): Q14916, Q9NNW7\n",
      "Skipping pair (missing vector): P23352, P0DJD3\n",
      "Skipping pair (missing vector): Q7Z403, Q9NNW7\n",
      "Skipping pair (missing vector): Q92521, P55073\n",
      "Skipping pair (missing vector): Q969S6, P0DJD3\n",
      "Skipping pair (missing vector): Q2TAA5, P0DJD3\n",
      "Skipping pair (missing vector): Q9NPG8, Q9BVL4\n",
      "Skipping pair (missing vector): Q9NQQ7, P86790\n",
      "Skipping pair (missing vector): Q7Z7K0, P0DJD3\n",
      "Skipping pair (missing vector): Q14435, P0DJD3\n",
      "Skipping pair (missing vector): Q9H5J4, P86790\n",
      "Skipping pair (missing vector): Q8NCN2, P86790\n",
      "Skipping pair (missing vector): Q6UVM3, Q9NNW7\n",
      "Skipping pair (missing vector): Q8NE79, Q9NNW7\n",
      "Skipping pair (missing vector): Q7L5A8, Q9NNW7\n",
      "Skipping pair (missing vector): Q8IYJ3, Q9BVL4\n",
      "Skipping pair (missing vector): O15120, P55073\n",
      "Skipping pair (missing vector): Q92536, O60613\n",
      "Skipping pair (missing vector): P23141, P55073\n",
      "Skipping pair (missing vector): Q6UXZ4, Q9NNW7\n",
      "Skipping pair (missing vector): Q9H3R2, Q9BVL4\n",
      "Skipping pair (missing vector): P28566, Q9BVL4\n",
      "Skipping pair (missing vector): Q14957, Q9BVL4\n",
      "Skipping pair (missing vector): P55073, Q8NDC4\n",
      "Skipping pair (missing vector): P55073, Q5BJD5\n",
      "Skipping pair (missing vector): P55061, P86790\n",
      "Skipping pair (missing vector): O60760, P86790\n",
      "Skipping pair (missing vector): Q9Y3S2, Q9NNW7\n",
      "Skipping pair (missing vector): O95180, Q9BVL4\n",
      "Skipping pair (missing vector): Q9BVL4, Q765P7\n",
      "Skipping pair (missing vector): Q9BVL4, Q8WXC3\n",
      "Skipping pair (missing vector): Q9BVL4, Q96M11\n",
      "Skipping pair (missing vector): Q9BVL4, Q9ULE3\n",
      "Skipping pair (missing vector): Q9BVL4, Q9NR23\n",
      "Skipping pair (missing vector): Q9BVL4, Q86WI3\n",
      "Skipping pair (missing vector): Q9BVL4, P22732\n",
      "Skipping pair (missing vector): Q9BVL4, P48764\n",
      "Skipping pair (missing vector): Q9BVL4, Q9HBX9\n",
      "Skipping pair (missing vector): P0DJD3, Q2M329\n",
      "Skipping pair (missing vector): P0DJD3, Q8IZU9\n",
      "Skipping pair (missing vector): A9QM74, P55073\n",
      "Skipping pair (missing vector): Q5SQ64, Q9BVL4\n",
      "Skipping pair (missing vector): P86790, Q86WI3\n",
      "Skipping pair (missing vector): P86790, Q86YB8\n",
      "Skipping pair (missing vector): Q9H400, Q9BVL4\n",
      "Skipping pair (missing vector): Q9NNW7, Q8IYA6\n",
      "Skipping pair (missing vector): Q9NNW7, Q96QP1\n",
      "Skipping pair (missing vector): Q9NNW7, Q9Y6Q2\n",
      "Skipping pair (missing vector): Q9NNW7, Q8N8W4\n",
      "Skipping pair (missing vector): Q9NNW7, Q9NQ75\n",
      "Skipping pair (missing vector): Q9NNW7, Q9HBX9\n",
      "Skipping pair (missing vector): Q9NNW7, O15321\n",
      "Skipping pair (missing vector): Q5H9S7, Q9NNW7\n",
      "Skipping pair (missing vector): Q92813, P62253\n",
      "Skipping pair (missing vector): Q8N3J6, Q9NNW7\n",
      "Skipping pair (missing vector): Q9P0L9, Q9NNW7\n",
      "Skipping pair (missing vector): Q9Y5S8, Q9BVL4\n",
      "Skipping pair (missing vector): P28221, P0DJD3\n",
      "Skipping pair (missing vector): Q1EHB4, Q9NNW7\n",
      "Skipping pair (missing vector): Q9NZC2, Q9NNW7\n",
      "→ Saved: train_dataset13.tsv & test_dataset13.tsv\n",
      "Skipping pair (missing vector): P0DI81, Q96Q05\n",
      "Skipping pair (missing vector): Q14686, Q9NNW7\n",
      "Skipping pair (missing vector): O00631, O14983\n",
      "Skipping pair (missing vector): P0DI81, Q7Z392\n",
      "Skipping pair (missing vector): P0DI81, Q8IUR0\n",
      "Skipping pair (missing vector): O75376, Q9NNW7\n",
      "Skipping pair (missing vector): P26678, O00631\n",
      "Skipping pair (missing vector): O75865, P0DI81\n",
      "Skipping pair (missing vector): Q92813, P62253\n",
      "Skipping pair (missing vector): Q9Y296, P0DI81\n",
      "Skipping pair (missing vector): P63261, P62328\n",
      "Skipping pair (missing vector): Q9BQE4, Q86TM6\n",
      "Skipping pair (missing vector): P03372, Q16881\n",
      "Skipping pair (missing vector): P67870, P62945\n",
      "Skipping pair (missing vector): Q92813, Q9Y6I7\n",
      "Skipping pair (missing vector): P06733, P0DI81\n",
      "Skipping pair (missing vector): P68133, P62328\n",
      "Skipping pair (missing vector): O43617, P0DI81\n",
      "Skipping pair (missing vector): Q86SZ2, P0DI81\n",
      "Skipping pair (missing vector): P40692, P62328\n",
      "Skipping pair (missing vector): P17980, Q9NNW7\n",
      "Skipping pair (missing vector): Q8TEY7, Q92813\n",
      "Skipping pair (missing vector): Q8ND90, P0CW24\n",
      "Skipping pair (missing vector): P00519, P07203\n",
      "Skipping pair (missing vector): Q3I5F7, Q9NNW7\n",
      "Skipping pair (missing vector): Q60I27, P55073\n",
      "Skipping pair (missing vector): O14960, Q9BVL4\n",
      "Skipping pair (missing vector): Q2PPJ7, P55073\n",
      "Skipping pair (missing vector): Q8IZJ3, P0DJD3\n",
      "Skipping pair (missing vector): P47900, P0DJD3\n",
      "Skipping pair (missing vector): Q9NRD8, P86790\n",
      "Skipping pair (missing vector): Q8N7J2, P86790\n",
      "Skipping pair (missing vector): O43653, P86790\n",
      "Skipping pair (missing vector): Q8N3J6, Q9NNW7\n",
      "Skipping pair (missing vector): Q9P0L9, Q9NNW7\n",
      "Skipping pair (missing vector): Q9Y5S8, Q9BVL4\n",
      "Skipping pair (missing vector): P28221, P0DJD3\n",
      "Skipping pair (missing vector): Q1EHB4, Q9NNW7\n",
      "Skipping pair (missing vector): Q9NZC2, Q9NNW7\n",
      "Skipping pair (missing vector): Q3MUY2, P0DJD3\n",
      "Skipping pair (missing vector): Q9NXG6, Q9BVL4\n",
      "Skipping pair (missing vector): Q9NXG6, Q9NNW7\n",
      "Skipping pair (missing vector): Q6P179, P55073\n",
      "Skipping pair (missing vector): Q9BVX2, Q9BVL4\n",
      "Skipping pair (missing vector): Q96E14, O60613\n",
      "Skipping pair (missing vector): Q8WWZ8, Q9BVL4\n",
      "Skipping pair (missing vector): Q02161, Q9BVL4\n",
      "Skipping pair (missing vector): Q8WV37, O60613\n",
      "Skipping pair (missing vector): Q9H9Q4, P86790\n",
      "Skipping pair (missing vector): Q96KN3, O60613\n",
      "Skipping pair (missing vector): Q9HBX8, O60613\n",
      "Skipping pair (missing vector): Q9H633, Q9NNW7\n",
      "Skipping pair (missing vector): A6BM72, Q9BVL4\n",
      "Skipping pair (missing vector): Q9NY46, Q9BVL4\n",
      "Skipping pair (missing vector): P36021, Q9BVL4\n",
      "Skipping pair (missing vector): O14493, Q9BVL4\n",
      "Skipping pair (missing vector): Q96LA6, O60613\n",
      "Skipping pair (missing vector): Q14832, P86790\n",
      "Skipping pair (missing vector): Q9BYE9, Q9NNW7\n",
      "Skipping pair (missing vector): Q8NHJ6, Q9BVL4\n",
      "Skipping pair (missing vector): O95832, Q9NNW7\n",
      "Skipping pair (missing vector): Q6UX01, P86790\n",
      "Skipping pair (missing vector): A5PKW4, Q9BVL4\n",
      "Skipping pair (missing vector): Q9BY07, P86790\n",
      "Skipping pair (missing vector): Q9NY25, P86790\n",
      "Skipping pair (missing vector): Q14916, Q9NNW7\n",
      "Skipping pair (missing vector): P23352, P0DJD3\n",
      "Skipping pair (missing vector): Q7Z403, Q9NNW7\n",
      "Skipping pair (missing vector): Q92521, P55073\n",
      "Skipping pair (missing vector): Q969S6, P0DJD3\n",
      "Skipping pair (missing vector): Q2TAA5, P0DJD3\n",
      "Skipping pair (missing vector): Q9NPG8, Q9BVL4\n",
      "Skipping pair (missing vector): Q9NQQ7, P86790\n",
      "Skipping pair (missing vector): Q7Z7K0, P0DJD3\n",
      "Skipping pair (missing vector): Q14435, P0DJD3\n",
      "Skipping pair (missing vector): Q9H5J4, P86790\n",
      "Skipping pair (missing vector): Q8NCN2, P86790\n",
      "Skipping pair (missing vector): Q6UVM3, Q9NNW7\n",
      "Skipping pair (missing vector): Q8NE79, Q9NNW7\n",
      "Skipping pair (missing vector): Q7L5A8, Q9NNW7\n",
      "Skipping pair (missing vector): Q8IYJ3, Q9BVL4\n",
      "Skipping pair (missing vector): O15120, P55073\n",
      "Skipping pair (missing vector): Q92536, O60613\n",
      "Skipping pair (missing vector): P23141, P55073\n",
      "Skipping pair (missing vector): Q6UXZ4, Q9NNW7\n",
      "Skipping pair (missing vector): Q9H3R2, Q9BVL4\n",
      "Skipping pair (missing vector): P28566, Q9BVL4\n",
      "Skipping pair (missing vector): Q14957, Q9BVL4\n",
      "Skipping pair (missing vector): P55073, Q8NDC4\n",
      "Skipping pair (missing vector): P55073, Q5BJD5\n",
      "Skipping pair (missing vector): P55061, P86790\n",
      "Skipping pair (missing vector): O60760, P86790\n",
      "Skipping pair (missing vector): Q9Y3S2, Q9NNW7\n",
      "Skipping pair (missing vector): O95180, Q9BVL4\n",
      "Skipping pair (missing vector): Q9BVL4, Q765P7\n",
      "Skipping pair (missing vector): Q9BVL4, Q8WXC3\n",
      "Skipping pair (missing vector): Q9BVL4, Q96M11\n",
      "Skipping pair (missing vector): Q9BVL4, Q9ULE3\n",
      "Skipping pair (missing vector): Q9BVL4, Q9NR23\n",
      "Skipping pair (missing vector): Q9BVL4, Q86WI3\n",
      "Skipping pair (missing vector): Q9BVL4, P22732\n",
      "Skipping pair (missing vector): Q9BVL4, P48764\n",
      "Skipping pair (missing vector): Q9BVL4, Q9HBX9\n",
      "Skipping pair (missing vector): P0DJD3, Q2M329\n",
      "Skipping pair (missing vector): P0DJD3, Q8IZU9\n",
      "Skipping pair (missing vector): A9QM74, P55073\n",
      "Skipping pair (missing vector): Q5SQ64, Q9BVL4\n",
      "Skipping pair (missing vector): P86790, Q86WI3\n",
      "Skipping pair (missing vector): P86790, Q86YB8\n",
      "Skipping pair (missing vector): Q9H400, Q9BVL4\n",
      "Skipping pair (missing vector): Q9NNW7, Q8IYA6\n",
      "Skipping pair (missing vector): Q9NNW7, Q96QP1\n",
      "Skipping pair (missing vector): Q9NNW7, Q9Y6Q2\n",
      "Skipping pair (missing vector): Q9NNW7, Q8N8W4\n",
      "Skipping pair (missing vector): Q9NNW7, Q9NQ75\n",
      "Skipping pair (missing vector): Q9NNW7, Q9HBX9\n",
      "Skipping pair (missing vector): Q9NNW7, O15321\n",
      "Skipping pair (missing vector): Q5H9S7, Q9NNW7\n",
      "Skipping pair (missing vector): P55072, Q9BQE4\n",
      "Skipping pair (missing vector): Q92813, O60337\n",
      "Skipping pair (missing vector): Q96G74, P36969\n",
      "Skipping pair (missing vector): P41247, O60613\n",
      "Skipping pair (missing vector): P51684, O60613\n",
      "Skipping pair (missing vector): Q9BQ51, P86790\n",
      "Skipping pair (missing vector): Q7L5Y1, O60613\n",
      "Skipping pair (missing vector): O75064, Q9NNW7\n",
      "Skipping pair (missing vector): Q9Y5R4, O60613\n",
      "Skipping pair (missing vector): Q9Y225, P0DJD3\n",
      "Skipping pair (missing vector): Q9Y225, Q9BVL4\n",
      "Skipping pair (missing vector): Q9Y225, Q9NNW7\n",
      "Skipping pair (missing vector): Q9BQ51, P0DJD3\n",
      "→ Saved: train_dataset14.tsv & test_dataset14.tsv\n",
      "Skipping pair (missing vector): P0DI81, Q96Q05\n",
      "Skipping pair (missing vector): Q14686, Q9NNW7\n",
      "Skipping pair (missing vector): O00631, O14983\n",
      "Skipping pair (missing vector): P0DI81, Q7Z392\n",
      "Skipping pair (missing vector): P0DI81, Q8IUR0\n",
      "Skipping pair (missing vector): O75376, Q9NNW7\n",
      "Skipping pair (missing vector): P26678, O00631\n",
      "Skipping pair (missing vector): O75865, P0DI81\n",
      "Skipping pair (missing vector): Q92813, P62253\n",
      "Skipping pair (missing vector): P55072, Q9BQE4\n",
      "Skipping pair (missing vector): Q92813, O60337\n",
      "Skipping pair (missing vector): Q96G74, P36969\n",
      "Skipping pair (missing vector): P06733, P0DI81\n",
      "Skipping pair (missing vector): P68133, P62328\n",
      "Skipping pair (missing vector): O43617, P0DI81\n",
      "Skipping pair (missing vector): Q86SZ2, P0DI81\n",
      "Skipping pair (missing vector): P40692, P62328\n",
      "Skipping pair (missing vector): P17980, Q9NNW7\n",
      "Skipping pair (missing vector): Q8TEY7, Q92813\n",
      "Skipping pair (missing vector): Q8ND90, P0CW24\n",
      "Skipping pair (missing vector): P00519, P07203\n",
      "Skipping pair (missing vector): Q3I5F7, Q9NNW7\n",
      "Skipping pair (missing vector): Q60I27, P55073\n",
      "Skipping pair (missing vector): O14960, Q9BVL4\n",
      "Skipping pair (missing vector): Q2PPJ7, P55073\n",
      "Skipping pair (missing vector): Q8IZJ3, P0DJD3\n",
      "Skipping pair (missing vector): P47900, P0DJD3\n",
      "Skipping pair (missing vector): Q9NRD8, P86790\n",
      "Skipping pair (missing vector): Q8N7J2, P86790\n",
      "Skipping pair (missing vector): O43653, P86790\n",
      "Skipping pair (missing vector): Q8N3J6, Q9NNW7\n",
      "Skipping pair (missing vector): Q9P0L9, Q9NNW7\n",
      "Skipping pair (missing vector): Q9Y5S8, Q9BVL4\n",
      "Skipping pair (missing vector): P28221, P0DJD3\n",
      "Skipping pair (missing vector): Q1EHB4, Q9NNW7\n",
      "Skipping pair (missing vector): Q9NZC2, Q9NNW7\n",
      "Skipping pair (missing vector): P41247, O60613\n",
      "Skipping pair (missing vector): P51684, O60613\n",
      "Skipping pair (missing vector): Q9BQ51, P86790\n",
      "Skipping pair (missing vector): Q7L5Y1, O60613\n",
      "Skipping pair (missing vector): O75064, Q9NNW7\n",
      "Skipping pair (missing vector): Q9Y5R4, O60613\n",
      "Skipping pair (missing vector): Q9Y225, P0DJD3\n",
      "Skipping pair (missing vector): Q9Y225, Q9BVL4\n",
      "Skipping pair (missing vector): Q9Y225, Q9NNW7\n",
      "Skipping pair (missing vector): Q9BQ51, P0DJD3\n",
      "Skipping pair (missing vector): Q96E14, O60613\n",
      "Skipping pair (missing vector): Q8WWZ8, Q9BVL4\n",
      "Skipping pair (missing vector): Q02161, Q9BVL4\n",
      "Skipping pair (missing vector): Q8WV37, O60613\n",
      "Skipping pair (missing vector): Q9H9Q4, P86790\n",
      "Skipping pair (missing vector): Q96KN3, O60613\n",
      "Skipping pair (missing vector): Q9HBX8, O60613\n",
      "Skipping pair (missing vector): Q9H633, Q9NNW7\n",
      "Skipping pair (missing vector): A6BM72, Q9BVL4\n",
      "Skipping pair (missing vector): Q9NY46, Q9BVL4\n",
      "Skipping pair (missing vector): P36021, Q9BVL4\n",
      "Skipping pair (missing vector): O14493, Q9BVL4\n",
      "Skipping pair (missing vector): Q96LA6, O60613\n",
      "Skipping pair (missing vector): Q14832, P86790\n",
      "Skipping pair (missing vector): Q9BYE9, Q9NNW7\n",
      "Skipping pair (missing vector): Q8NHJ6, Q9BVL4\n",
      "Skipping pair (missing vector): O95832, Q9NNW7\n",
      "Skipping pair (missing vector): Q6UX01, P86790\n",
      "Skipping pair (missing vector): A5PKW4, Q9BVL4\n",
      "Skipping pair (missing vector): Q9BY07, P86790\n",
      "Skipping pair (missing vector): Q9NY25, P86790\n",
      "Skipping pair (missing vector): Q14916, Q9NNW7\n",
      "Skipping pair (missing vector): P23352, P0DJD3\n",
      "Skipping pair (missing vector): Q7Z403, Q9NNW7\n",
      "Skipping pair (missing vector): Q92521, P55073\n",
      "Skipping pair (missing vector): Q969S6, P0DJD3\n",
      "Skipping pair (missing vector): Q2TAA5, P0DJD3\n",
      "Skipping pair (missing vector): Q9NPG8, Q9BVL4\n",
      "Skipping pair (missing vector): Q9NQQ7, P86790\n",
      "Skipping pair (missing vector): Q7Z7K0, P0DJD3\n",
      "Skipping pair (missing vector): Q14435, P0DJD3\n",
      "Skipping pair (missing vector): Q9H5J4, P86790\n",
      "Skipping pair (missing vector): Q8NCN2, P86790\n",
      "Skipping pair (missing vector): Q6UVM3, Q9NNW7\n",
      "Skipping pair (missing vector): Q8NE79, Q9NNW7\n",
      "Skipping pair (missing vector): Q7L5A8, Q9NNW7\n",
      "Skipping pair (missing vector): Q8IYJ3, Q9BVL4\n",
      "Skipping pair (missing vector): O15120, P55073\n",
      "Skipping pair (missing vector): Q92536, O60613\n",
      "Skipping pair (missing vector): P23141, P55073\n",
      "Skipping pair (missing vector): Q6UXZ4, Q9NNW7\n",
      "Skipping pair (missing vector): Q9H3R2, Q9BVL4\n",
      "Skipping pair (missing vector): P28566, Q9BVL4\n",
      "Skipping pair (missing vector): Q14957, Q9BVL4\n",
      "Skipping pair (missing vector): P55073, Q8NDC4\n",
      "Skipping pair (missing vector): P55073, Q5BJD5\n",
      "Skipping pair (missing vector): P55061, P86790\n",
      "Skipping pair (missing vector): O60760, P86790\n",
      "Skipping pair (missing vector): Q9Y3S2, Q9NNW7\n",
      "Skipping pair (missing vector): O95180, Q9BVL4\n",
      "Skipping pair (missing vector): Q9BVL4, Q765P7\n",
      "Skipping pair (missing vector): Q9BVL4, Q8WXC3\n",
      "Skipping pair (missing vector): Q9BVL4, Q96M11\n",
      "Skipping pair (missing vector): Q9BVL4, Q9ULE3\n",
      "Skipping pair (missing vector): Q9BVL4, Q9NR23\n",
      "Skipping pair (missing vector): Q9BVL4, Q86WI3\n",
      "Skipping pair (missing vector): Q9BVL4, P22732\n",
      "Skipping pair (missing vector): Q9BVL4, P48764\n",
      "Skipping pair (missing vector): Q9BVL4, Q9HBX9\n",
      "Skipping pair (missing vector): P0DJD3, Q2M329\n",
      "Skipping pair (missing vector): P0DJD3, Q8IZU9\n",
      "Skipping pair (missing vector): A9QM74, P55073\n",
      "Skipping pair (missing vector): Q5SQ64, Q9BVL4\n",
      "Skipping pair (missing vector): P86790, Q86WI3\n",
      "Skipping pair (missing vector): P86790, Q86YB8\n",
      "Skipping pair (missing vector): Q9H400, Q9BVL4\n",
      "Skipping pair (missing vector): Q9NNW7, Q8IYA6\n",
      "Skipping pair (missing vector): Q9NNW7, Q96QP1\n",
      "Skipping pair (missing vector): Q9NNW7, Q9Y6Q2\n",
      "Skipping pair (missing vector): Q9NNW7, Q8N8W4\n",
      "Skipping pair (missing vector): Q9NNW7, Q9NQ75\n",
      "Skipping pair (missing vector): Q9NNW7, Q9HBX9\n",
      "Skipping pair (missing vector): Q9NNW7, O15321\n",
      "Skipping pair (missing vector): Q5H9S7, Q9NNW7\n",
      "Skipping pair (missing vector): Q9Y296, P0DI81\n",
      "Skipping pair (missing vector): P63261, P62328\n",
      "Skipping pair (missing vector): Q9BQE4, Q86TM6\n",
      "Skipping pair (missing vector): P03372, Q16881\n",
      "Skipping pair (missing vector): P67870, P62945\n",
      "Skipping pair (missing vector): Q92813, Q9Y6I7\n",
      "Skipping pair (missing vector): Q3MUY2, P0DJD3\n",
      "Skipping pair (missing vector): Q9NXG6, Q9BVL4\n",
      "Skipping pair (missing vector): Q9NXG6, Q9NNW7\n",
      "Skipping pair (missing vector): Q6P179, P55073\n",
      "Skipping pair (missing vector): Q9BVX2, Q9BVL4\n",
      "→ Saved: train_dataset15.tsv & test_dataset15.tsv\n",
      "\n",
      "--- Processing Category C2 ---\n",
      "Skipping pair (missing vector): O95376, Q9NNW7\n",
      "Skipping pair (missing vector): Q96DI7, P63302\n",
      "Skipping pair (missing vector): P49842, P07203\n",
      "Skipping pair (missing vector): P39880, P07203\n",
      "Skipping pair (missing vector): O76031, P0DJD3\n",
      "Skipping pair (missing vector): P07203, Q14938\n",
      "Skipping pair (missing vector): P07203, Q9UL03\n",
      "Skipping pair (missing vector): P07203, O14593\n",
      "Skipping pair (missing vector): P07203, O43684\n",
      "Skipping pair (missing vector): P07203, P28715\n",
      "Skipping pair (missing vector): P07203, Q96JB3\n",
      "Skipping pair (missing vector): P07203, Q01081\n",
      "Skipping pair (missing vector): P07203, Q96JN0\n",
      "Skipping pair (missing vector): P07203, Q16621\n",
      "Skipping pair (missing vector): P07203, P84103\n",
      "Skipping pair (missing vector): P07203, P27707\n",
      "Skipping pair (missing vector): P07203, Q86UP8\n",
      "Skipping pair (missing vector): P07203, Q8WYH8\n",
      "Skipping pair (missing vector): P07203, Q969L4\n",
      "Skipping pair (missing vector): P07203, Q13105\n",
      "Skipping pair (missing vector): P07203, P35548\n",
      "Skipping pair (missing vector): P07203, Q9UGN5\n",
      "Skipping pair (missing vector): P07203, P42684\n",
      "Skipping pair (missing vector): P82663, P0DJD3\n",
      "Skipping pair (missing vector): Q9H211, Q9NNW7\n",
      "Skipping pair (missing vector): P84103, P0DJD3\n",
      "Skipping pair (missing vector): Q9Y2T1, P0DJD3\n",
      "Skipping pair (missing vector): Q5QNW6, P63302\n",
      "Skipping pair (missing vector): Q9Y5Q9, P63302\n",
      "Skipping pair (missing vector): Q9H4Z3, P63302\n",
      "Skipping pair (missing vector): Q96A08, P63302\n",
      "Skipping pair (missing vector): P42679, P0DJD3\n",
      "Skipping pair (missing vector): Q05923, Q9NNW7\n",
      "Skipping pair (missing vector): Q06416, Q9NNW7\n",
      "Skipping pair (missing vector): Q9Y5P8, P63302\n",
      "Skipping pair (missing vector): O76039, Q9NNW7\n",
      "Skipping pair (missing vector): P20264, Q9NNW7\n",
      "Skipping pair (missing vector): P49917, P63302\n",
      "Skipping pair (missing vector): P18858, P63302\n",
      "Skipping pair (missing vector): O95948, Q9NNW7\n",
      "Skipping pair (missing vector): P10253, P63302\n",
      "Skipping pair (missing vector): Q9GZP8, P63302\n",
      "Skipping pair (missing vector): Q9UDW3, P63302\n",
      "Skipping pair (missing vector): Q8IUM7, P63302\n",
      "Skipping pair (missing vector): Q8IUM7, Q9NNW7\n",
      "Skipping pair (missing vector): O14924, P63302\n",
      "Skipping pair (missing vector): P62995, P63302\n",
      "Skipping pair (missing vector): P62995, P0DJD3\n",
      "Skipping pair (missing vector): Q9H8M7, Q9NNW7\n",
      "Skipping pair (missing vector): P28358, Q9NNW7\n",
      "Skipping pair (missing vector): P09629, Q9NNW7\n",
      "Skipping pair (missing vector): Q16385, Q9NNW7\n",
      "Skipping pair (missing vector): Q8IUE6, Q9NNW7\n",
      "Skipping pair (missing vector): P53539, P63302\n",
      "Skipping pair (missing vector): Q9NSY1, Q9NNW7\n",
      "Skipping pair (missing vector): Q8N1G1, Q9NNW7\n",
      "Skipping pair (missing vector): Q14999, P0DJD3\n",
      "Skipping pair (missing vector): Q5H9F3, Q9NNW7\n",
      "Skipping pair (missing vector): Q12996, Q9NNW7\n",
      "Skipping pair (missing vector): O94805, P63302\n",
      "Skipping pair (missing vector): Q9BYN8, P0DJD3\n",
      "Skipping pair (missing vector): Q02548, P63302\n",
      "Skipping pair (missing vector): Q8N5A5, P63302\n",
      "Skipping pair (missing vector): P23527, Q9NNW7\n",
      "Skipping pair (missing vector): A0AVK6, Q9NNW7\n",
      "Skipping pair (missing vector): P20671, Q9NNW7\n",
      "Skipping pair (missing vector): Q12952, Q9NNW7\n",
      "Skipping pair (missing vector): Q10587, P63302\n",
      "Skipping pair (missing vector): P63302, P37231\n",
      "Skipping pair (missing vector): P63302, Q69YI7\n",
      "Skipping pair (missing vector): P63302, Q9UIF7\n",
      "Skipping pair (missing vector): P63302, Q9UBR2\n",
      "Skipping pair (missing vector): P63302, Q5MJ70\n",
      "Skipping pair (missing vector): P63302, P16401\n",
      "Skipping pair (missing vector): P63302, Q9BQ83\n",
      "Skipping pair (missing vector): P63302, P55318\n",
      "Skipping pair (missing vector): P63302, Q9Y222\n",
      "Skipping pair (missing vector): P63302, P40617\n",
      "Skipping pair (missing vector): P63302, Q02556\n",
      "Skipping pair (missing vector): Q9Y222, Q9NNW7\n",
      "Skipping pair (missing vector): P61218, Q9NNW7\n",
      "Skipping pair (missing vector): P20591, P0DJD3\n",
      "Skipping pair (missing vector): P16401, Q9NNW7\n",
      "Skipping pair (missing vector): P06732, P0DJD3\n",
      "Skipping pair (missing vector): P0DJD3, Q96T66\n",
      "Skipping pair (missing vector): P0DJD3, P29466\n",
      "Skipping pair (missing vector): P0DJD3, Q07973\n",
      "Skipping pair (missing vector): P0DJD3, O75525\n",
      "Skipping pair (missing vector): P0DJD3, Q13242\n",
      "Skipping pair (missing vector): Q9NNW7, P14678\n",
      "Skipping pair (missing vector): Q9NNW7, P53567\n",
      "Skipping pair (missing vector): Q9NNW7, P51946\n",
      "Skipping pair (missing vector): Q9NNW7, Q9H9Q4\n",
      "Skipping pair (missing vector): Q9NNW7, Q9UJA3\n",
      "Skipping pair (missing vector): Q9NNW7, Q06330\n",
      "Skipping pair (missing vector): Q9NNW7, P30876\n",
      "Skipping pair (missing vector): Q9NNW7, Q99990\n",
      "Skipping pair (missing vector): Q9NNW7, P32519\n",
      "Skipping pair (missing vector): Q9NNW7, O75376\n",
      "Skipping pair (missing vector): P68133, P62328\n",
      "Skipping pair (missing vector): P62736, P62328\n",
      "Skipping pair (missing vector): P67870, P62945\n",
      "Skipping pair (missing vector): P08572, P0DJI8\n",
      "Skipping pair (missing vector): P48059, P62328\n",
      "Skipping pair (missing vector): P21980, P62328\n",
      "Skipping pair (missing vector): P62328, P60510\n",
      "Skipping pair (missing vector): Q96I34, Q16881\n",
      "Skipping pair (missing vector): P21462, P0DJI8\n",
      "Skipping pair (missing vector): P25391, P0DJI8\n",
      "Skipping pair (missing vector): Q01955, P0DJI8\n",
      "Skipping pair (missing vector): P26678, O00631\n",
      "Skipping pair (missing vector): O14983, O00631\n",
      "Skipping pair (missing vector): O00299, P0DI82\n",
      "Skipping pair (missing vector): P53420, P0DJI8\n",
      "Skipping pair (missing vector): P11226, P0DI82\n",
      "Skipping pair (missing vector): P0DJI8, Q9BQE4\n",
      "Skipping pair (missing vector): Q8TEY7, Q92813\n",
      "Skipping pair (missing vector): Q16881, Q9NS18\n",
      "Skipping pair (missing vector): P0DI82, O15247\n",
      "Skipping pair (missing vector): P28289, P63313\n",
      "Skipping pair (missing vector): Q9NYU2, O60613\n",
      "Skipping pair (missing vector): P68871, P62341\n",
      "Skipping pair (missing vector): Q0VG06, Q9NNW7\n",
      "Skipping pair (missing vector): P07686, P63302\n",
      "Skipping pair (missing vector): P07203, Q16633\n",
      "Skipping pair (missing vector): O43829, Q9NNW7\n",
      "→ Saved: train_dataset21.tsv & test_dataset21.tsv\n",
      "Skipping pair (missing vector): Q16643, P0DJD3\n",
      "Skipping pair (missing vector): Q9BRA2, Q16881\n",
      "Skipping pair (missing vector): P43699, Q9NNW7\n",
      "Skipping pair (missing vector): P07203, Q14938\n",
      "Skipping pair (missing vector): P07203, Q96JB3\n",
      "Skipping pair (missing vector): P07203, Q01081\n",
      "Skipping pair (missing vector): P07203, Q96JN0\n",
      "Skipping pair (missing vector): P07203, Q16621\n",
      "Skipping pair (missing vector): P07203, Q8TAP9\n",
      "Skipping pair (missing vector): P07203, P84103\n",
      "Skipping pair (missing vector): P07203, Q86UP8\n",
      "Skipping pair (missing vector): P07203, P61964\n",
      "Skipping pair (missing vector): P07203, Q16633\n",
      "Skipping pair (missing vector): P07203, Q14181\n",
      "Skipping pair (missing vector): P07203, Q8WYH8\n",
      "Skipping pair (missing vector): P07203, Q13547\n",
      "Skipping pair (missing vector): P07203, Q13105\n",
      "Skipping pair (missing vector): P07203, Q16594\n",
      "Skipping pair (missing vector): P07203, P42684\n",
      "Skipping pair (missing vector): Q9Y6P5, Q9NNW7\n",
      "Skipping pair (missing vector): P84103, P0DJD3\n",
      "Skipping pair (missing vector): Q9Y2T1, P0DJD3\n",
      "Skipping pair (missing vector): Q12931, P0DJD3\n",
      "Skipping pair (missing vector): Q16633, Q9NNW7\n",
      "Skipping pair (missing vector): P49715, Q9NNW7\n",
      "Skipping pair (missing vector): Q96I25, Q9NNW7\n",
      "Skipping pair (missing vector): Q05923, Q9NNW7\n",
      "Skipping pair (missing vector): Q06416, Q9NNW7\n",
      "Skipping pair (missing vector): Q9UL17, Q9NNW7\n",
      "Skipping pair (missing vector): P15336, Q9NNW7\n",
      "Skipping pair (missing vector): Q8N1G1, Q9NNW7\n",
      "Skipping pair (missing vector): Q07617, P0DJD3\n",
      "Skipping pair (missing vector): O76039, Q9NNW7\n",
      "Skipping pair (missing vector): Q8WXC3, P0DJD3\n",
      "Skipping pair (missing vector): Q9HC29, P0DJD3\n",
      "Skipping pair (missing vector): Q96AA8, P0DJD3\n",
      "Skipping pair (missing vector): O43829, Q9NNW7\n",
      "Skipping pair (missing vector): Q9H8M7, Q9NNW7\n",
      "Skipping pair (missing vector): P28358, Q9NNW7\n",
      "Skipping pair (missing vector): Q15173, P0DJD3\n",
      "Skipping pair (missing vector): Q9NSY1, Q9NNW7\n",
      "Skipping pair (missing vector): Q5H9F3, Q9NNW7\n",
      "Skipping pair (missing vector): Q12996, Q9NNW7\n",
      "Skipping pair (missing vector): Q9BYN8, P0DJD3\n",
      "Skipping pair (missing vector): P51649, P0DJD3\n",
      "Skipping pair (missing vector): O95786, P0DJD3\n",
      "Skipping pair (missing vector): P23527, Q9NNW7\n",
      "Skipping pair (missing vector): A0AVK6, Q9NNW7\n",
      "Skipping pair (missing vector): P20264, Q9NNW7\n",
      "Skipping pair (missing vector): P20671, Q9NNW7\n",
      "Skipping pair (missing vector): P52655, Q9NNW7\n",
      "Skipping pair (missing vector): Q12952, Q9NNW7\n",
      "Skipping pair (missing vector): Q9Y222, Q9NNW7\n",
      "Skipping pair (missing vector): P61218, Q9NNW7\n",
      "Skipping pair (missing vector): Q0VG06, Q9NNW7\n",
      "Skipping pair (missing vector): P16401, Q9NNW7\n",
      "Skipping pair (missing vector): P06732, P0DJD3\n",
      "Skipping pair (missing vector): Q96QT6, Q9NNW7\n",
      "Skipping pair (missing vector): P0DJD3, Q9UPY3\n",
      "Skipping pair (missing vector): P0DJD3, Q07973\n",
      "Skipping pair (missing vector): P0DJD3, Q13242\n",
      "Skipping pair (missing vector): Q9NNW7, Q8IUE6\n",
      "Skipping pair (missing vector): Q9NNW7, P53567\n",
      "Skipping pair (missing vector): Q9NNW7, P51946\n",
      "Skipping pair (missing vector): Q9NNW7, P11161\n",
      "Skipping pair (missing vector): Q9NNW7, Q9UJA3\n",
      "Skipping pair (missing vector): Q9NNW7, Q9NPI8\n",
      "Skipping pair (missing vector): Q9NNW7, P30876\n",
      "Skipping pair (missing vector): Q9NNW7, P32519\n",
      "Skipping pair (missing vector): P62736, P62328\n",
      "Skipping pair (missing vector): P67870, P62945\n",
      "Skipping pair (missing vector): P08572, P0DJI8\n",
      "Skipping pair (missing vector): P48059, P62328\n",
      "Skipping pair (missing vector): P63261, P62328\n",
      "Skipping pair (missing vector): P63261, O14604\n",
      "Skipping pair (missing vector): P21980, P62328\n",
      "Skipping pair (missing vector): P62328, P60510\n",
      "Skipping pair (missing vector): Q96I34, Q16881\n",
      "Skipping pair (missing vector): P21462, P0DJI8\n",
      "Skipping pair (missing vector): P55072, Q9BQE4\n",
      "Skipping pair (missing vector): P26678, O00631\n",
      "Skipping pair (missing vector): O14983, O00631\n",
      "Skipping pair (missing vector): O00299, P0DI82\n",
      "Skipping pair (missing vector): P53420, P0DJI8\n",
      "Skipping pair (missing vector): P11226, P0DI82\n",
      "Skipping pair (missing vector): P0DJI8, Q9BQE4\n",
      "Skipping pair (missing vector): Q8TEY7, Q92813\n",
      "Skipping pair (missing vector): Q16881, Q9NS18\n",
      "Skipping pair (missing vector): P0DI82, O15247\n",
      "Skipping pair (missing vector): P28289, P63313\n",
      "Skipping pair (missing vector): Q9NYU2, O60613\n",
      "Skipping pair (missing vector): P68871, P62341\n",
      "Skipping pair (missing vector): O00268, P63302\n",
      "Skipping pair (missing vector): P07686, P63302\n",
      "Skipping pair (missing vector): P25391, P0DJI8\n",
      "→ Saved: train_dataset22.tsv & test_dataset22.tsv\n",
      "Skipping pair (missing vector): Q9BRA2, Q16881\n",
      "Skipping pair (missing vector): O95376, Q9NNW7\n",
      "Skipping pair (missing vector): P43699, Q9NNW7\n",
      "Skipping pair (missing vector): Q9Y6P5, Q9NNW7\n",
      "Skipping pair (missing vector): Q9H211, Q9NNW7\n",
      "Skipping pair (missing vector): Q96I25, Q9NNW7\n",
      "Skipping pair (missing vector): Q05923, Q9NNW7\n",
      "Skipping pair (missing vector): Q9UL17, Q9NNW7\n",
      "Skipping pair (missing vector): Q8N1G1, Q9NNW7\n",
      "Skipping pair (missing vector): O43829, Q9NNW7\n",
      "Skipping pair (missing vector): Q8IUM7, Q9NNW7\n",
      "Skipping pair (missing vector): P09629, Q9NNW7\n",
      "Skipping pair (missing vector): Q16385, Q9NNW7\n",
      "Skipping pair (missing vector): Q5H9F3, Q9NNW7\n",
      "Skipping pair (missing vector): Q12996, Q9NNW7\n",
      "Skipping pair (missing vector): P23527, Q9NNW7\n",
      "Skipping pair (missing vector): P21675, Q9NNW7\n",
      "Skipping pair (missing vector): P20671, Q9NNW7\n",
      "Skipping pair (missing vector): Q9Y222, Q9NNW7\n",
      "Skipping pair (missing vector): P61218, Q9NNW7\n",
      "Skipping pair (missing vector): Q14209, Q9NNW7\n",
      "Skipping pair (missing vector): Q0VG06, Q9NNW7\n",
      "Skipping pair (missing vector): P16401, Q9NNW7\n",
      "Skipping pair (missing vector): Q96QT6, Q9NNW7\n",
      "Skipping pair (missing vector): Q9NNW7, P51946\n",
      "Skipping pair (missing vector): Q9NNW7, P49639\n",
      "Skipping pair (missing vector): Q9NNW7, P11161\n",
      "Skipping pair (missing vector): Q9NNW7, Q9UJA3\n",
      "Skipping pair (missing vector): Q9NNW7, Q06330\n",
      "Skipping pair (missing vector): Q9NNW7, Q9NPI8\n",
      "Skipping pair (missing vector): Q9NNW7, P30876\n",
      "Skipping pair (missing vector): Q9NNW7, Q99990\n",
      "Skipping pair (missing vector): Q9NNW7, P32519\n",
      "Skipping pair (missing vector): Q9NNW7, O75376\n",
      "Skipping pair (missing vector): P51693, P62328\n",
      "Skipping pair (missing vector): Q13418, P62328\n",
      "Skipping pair (missing vector): P68133, P62328\n",
      "Skipping pair (missing vector): P62736, P62328\n",
      "Skipping pair (missing vector): P67870, P62945\n",
      "Skipping pair (missing vector): P08572, P0DJI8\n",
      "Skipping pair (missing vector): P48059, P62328\n",
      "Skipping pair (missing vector): P21980, P62328\n",
      "Skipping pair (missing vector): P62328, P60510\n",
      "Skipping pair (missing vector): Q96I34, Q16881\n",
      "Skipping pair (missing vector): Q01955, P0DJI8\n",
      "Skipping pair (missing vector): P26678, O00631\n",
      "Skipping pair (missing vector): O14983, O00631\n",
      "Skipping pair (missing vector): O00299, P0DI82\n",
      "Skipping pair (missing vector): P11226, P0DI82\n",
      "Skipping pair (missing vector): P0DJI8, P21462\n",
      "Skipping pair (missing vector): P0DJI8, P25391\n",
      "Skipping pair (missing vector): P0DJI8, Q9BQE4\n",
      "Skipping pair (missing vector): Q8TEY7, Q92813\n",
      "Skipping pair (missing vector): Q16881, Q9NS18\n",
      "Skipping pair (missing vector): P0DI82, O15247\n",
      "Skipping pair (missing vector): Q9NYU2, O60613\n",
      "Skipping pair (missing vector): P68871, P62341\n",
      "Skipping pair (missing vector): Q12931, P0DJD3\n",
      "Skipping pair (missing vector): P28838, P0DJD3\n",
      "Skipping pair (missing vector): Q8IUE6, Q9NNW7\n",
      "Skipping pair (missing vector): P63302, Q5MJ70\n",
      "Skipping pair (missing vector): P63302, P16401\n",
      "Skipping pair (missing vector): P63302, Q9GZP8\n",
      "Skipping pair (missing vector): P07203, Q96JB3\n",
      "Skipping pair (missing vector): P52655, Q9NNW7\n",
      "Skipping pair (missing vector): Q16643, P0DJD3\n",
      "Skipping pair (missing vector): Q8IX03, P0DJD3\n",
      "Skipping pair (missing vector): P29466, P0DJD3\n",
      "Skipping pair (missing vector): P28289, P63313\n",
      "→ Saved: train_dataset23.tsv & test_dataset23.tsv\n",
      "Skipping pair (missing vector): Q9BRA2, Q16881\n",
      "Skipping pair (missing vector): O95376, Q9NNW7\n",
      "Skipping pair (missing vector): P43699, Q9NNW7\n",
      "Skipping pair (missing vector): P49842, P07203\n",
      "Skipping pair (missing vector): P39880, P07203\n",
      "Skipping pair (missing vector): P07203, P07992\n",
      "Skipping pair (missing vector): P07203, O43684\n",
      "Skipping pair (missing vector): P07203, Q5VWG9\n",
      "Skipping pair (missing vector): P07203, Q96JN0\n",
      "Skipping pair (missing vector): P07203, Q16621\n",
      "Skipping pair (missing vector): P07203, Q8TAP9\n",
      "Skipping pair (missing vector): P07203, P84103\n",
      "Skipping pair (missing vector): P07203, P35680\n",
      "Skipping pair (missing vector): P07203, P27707\n",
      "Skipping pair (missing vector): P07203, Q86UP8\n",
      "Skipping pair (missing vector): P07203, P61964\n",
      "Skipping pair (missing vector): P07203, Q8WYH8\n",
      "Skipping pair (missing vector): P07203, Q969L4\n",
      "Skipping pair (missing vector): P07203, P35548\n",
      "Skipping pair (missing vector): P07203, Q9UGN5\n",
      "Skipping pair (missing vector): P07203, P42684\n",
      "Skipping pair (missing vector): P82663, P0DJD3\n",
      "Skipping pair (missing vector): Q9H211, Q9NNW7\n",
      "Skipping pair (missing vector): P84103, P0DJD3\n",
      "Skipping pair (missing vector): Q9Y2T1, P0DJD3\n",
      "Skipping pair (missing vector): Q12931, P0DJD3\n",
      "Skipping pair (missing vector): Q96I25, Q9NNW7\n",
      "Skipping pair (missing vector): Q05923, Q9NNW7\n",
      "Skipping pair (missing vector): Q9UL17, Q9NNW7\n",
      "Skipping pair (missing vector): Q8N1G1, Q9NNW7\n",
      "Skipping pair (missing vector): O43819, P0DJD3\n",
      "Skipping pair (missing vector): P20264, Q9NNW7\n",
      "Skipping pair (missing vector): O95948, Q9NNW7\n",
      "Skipping pair (missing vector): O43829, Q9NNW7\n",
      "Skipping pair (missing vector): P62995, P0DJD3\n",
      "Skipping pair (missing vector): Q9H8M7, Q9NNW7\n",
      "Skipping pair (missing vector): P09629, Q9NNW7\n",
      "Skipping pair (missing vector): Q8IUE6, Q9NNW7\n",
      "Skipping pair (missing vector): Q9NSY1, Q9NNW7\n",
      "Skipping pair (missing vector): Q5H9F3, Q9NNW7\n",
      "Skipping pair (missing vector): Q12996, Q9NNW7\n",
      "Skipping pair (missing vector): Q9BYN8, P0DJD3\n",
      "Skipping pair (missing vector): P51649, P0DJD3\n",
      "Skipping pair (missing vector): P23527, Q9NNW7\n",
      "Skipping pair (missing vector): A0AVK6, Q9NNW7\n",
      "Skipping pair (missing vector): Q16385, Q9NNW7\n",
      "Skipping pair (missing vector): P21675, Q9NNW7\n",
      "Skipping pair (missing vector): Q12952, Q9NNW7\n",
      "Skipping pair (missing vector): Q9Y222, Q9NNW7\n",
      "Skipping pair (missing vector): P61218, Q9NNW7\n",
      "Skipping pair (missing vector): Q14209, Q9NNW7\n",
      "Skipping pair (missing vector): P20591, P0DJD3\n",
      "Skipping pair (missing vector): Q0VG06, Q9NNW7\n",
      "Skipping pair (missing vector): P20671, Q9NNW7\n",
      "Skipping pair (missing vector): P16401, Q9NNW7\n",
      "Skipping pair (missing vector): Q96QT6, Q9NNW7\n",
      "Skipping pair (missing vector): P0DJD3, Q9UPY3\n",
      "Skipping pair (missing vector): P0DJD3, Q07973\n",
      "Skipping pair (missing vector): P0DJD3, O75525\n",
      "Skipping pair (missing vector): P0DJD3, Q13242\n",
      "Skipping pair (missing vector): Q9NNW7, P14678\n",
      "Skipping pair (missing vector): Q9NNW7, P53567\n",
      "Skipping pair (missing vector): Q9NNW7, P51946\n",
      "Skipping pair (missing vector): Q9NNW7, P11161\n",
      "Skipping pair (missing vector): Q9NNW7, Q9UJA3\n",
      "Skipping pair (missing vector): Q9NNW7, Q9NPI8\n",
      "Skipping pair (missing vector): Q9NNW7, Q99990\n",
      "Skipping pair (missing vector): Q9NNW7, P32519\n",
      "Skipping pair (missing vector): P08572, P0DJI8\n",
      "Skipping pair (missing vector): P63261, O14604\n",
      "Skipping pair (missing vector): Q96I34, Q16881\n",
      "Skipping pair (missing vector): P25391, P0DJI8\n",
      "Skipping pair (missing vector): Q01955, P0DJI8\n",
      "Skipping pair (missing vector): P55072, Q9BQE4\n",
      "Skipping pair (missing vector): P26678, O00631\n",
      "Skipping pair (missing vector): O14983, O00631\n",
      "Skipping pair (missing vector): O00299, P0DI82\n",
      "Skipping pair (missing vector): P53420, P0DJI8\n",
      "Skipping pair (missing vector): P11226, P0DI82\n",
      "Skipping pair (missing vector): P0DJI8, Q9BQE4\n",
      "Skipping pair (missing vector): Q8TEY7, Q92813\n",
      "Skipping pair (missing vector): Q16881, Q9NS18\n",
      "Skipping pair (missing vector): P0DI82, O15247\n",
      "Skipping pair (missing vector): P28289, P63313\n",
      "Skipping pair (missing vector): Q9NYU2, O60613\n",
      "Skipping pair (missing vector): P68871, P62341\n",
      "Skipping pair (missing vector): P62807, P63302\n",
      "Skipping pair (missing vector): P07686, P63302\n",
      "Skipping pair (missing vector): P63302, O14924\n",
      "Skipping pair (missing vector): Q16643, P0DJD3\n",
      "Skipping pair (missing vector): Q8IX03, P0DJD3\n",
      "Skipping pair (missing vector): O76039, Q9NNW7\n",
      "Skipping pair (missing vector): P00519, P07203\n",
      "Skipping pair (missing vector): P62328, P62736\n",
      "Skipping pair (missing vector): P62328, P48059\n",
      "→ Saved: train_dataset24.tsv & test_dataset24.tsv\n",
      "Skipping pair (missing vector): Q16643, P0DJD3\n",
      "Skipping pair (missing vector): Q9BRA2, Q16881\n",
      "Skipping pair (missing vector): O95376, Q9NNW7\n",
      "Skipping pair (missing vector): P43699, Q9NNW7\n",
      "Skipping pair (missing vector): P49842, P07203\n",
      "Skipping pair (missing vector): P39880, P07203\n",
      "Skipping pair (missing vector): Q01581, P0DJD3\n",
      "Skipping pair (missing vector): P07203, O43684\n",
      "Skipping pair (missing vector): P07203, P28715\n",
      "Skipping pair (missing vector): P07203, Q96JB3\n",
      "Skipping pair (missing vector): P07203, Q5VWG9\n",
      "Skipping pair (missing vector): P07203, Q96JN0\n",
      "Skipping pair (missing vector): P07203, Q16621\n",
      "Skipping pair (missing vector): P07203, Q8TAP9\n",
      "Skipping pair (missing vector): P07203, P84103\n",
      "Skipping pair (missing vector): P07203, P27707\n",
      "Skipping pair (missing vector): P07203, Q86UP8\n",
      "Skipping pair (missing vector): P07203, P61964\n",
      "Skipping pair (missing vector): P07203, Q16633\n",
      "Skipping pair (missing vector): P07203, Q14181\n",
      "Skipping pair (missing vector): P07203, Q8WYH8\n",
      "Skipping pair (missing vector): P07203, Q969L4\n",
      "Skipping pair (missing vector): P07203, Q13105\n",
      "Skipping pair (missing vector): P07203, O75643\n",
      "Skipping pair (missing vector): P07203, Q16594\n",
      "Skipping pair (missing vector): P07203, Q9UGN5\n",
      "Skipping pair (missing vector): P07203, P42684\n",
      "Skipping pair (missing vector): P82663, P0DJD3\n",
      "Skipping pair (missing vector): Q9H211, Q9NNW7\n",
      "Skipping pair (missing vector): P84103, P0DJD3\n",
      "Skipping pair (missing vector): Q9Y2T1, P0DJD3\n",
      "Skipping pair (missing vector): Q16633, Q9NNW7\n",
      "Skipping pair (missing vector): P49715, Q9NNW7\n",
      "Skipping pair (missing vector): Q05923, Q9NNW7\n",
      "Skipping pair (missing vector): Q06416, Q9NNW7\n",
      "Skipping pair (missing vector): Q9UL17, Q9NNW7\n",
      "Skipping pair (missing vector): P15336, Q9NNW7\n",
      "Skipping pair (missing vector): Q8N1G1, Q9NNW7\n",
      "Skipping pair (missing vector): O43819, P0DJD3\n",
      "Skipping pair (missing vector): Q8WXC3, P0DJD3\n",
      "Skipping pair (missing vector): P20264, Q9NNW7\n",
      "Skipping pair (missing vector): O95948, Q9NNW7\n",
      "Skipping pair (missing vector): Q8IUM7, Q9NNW7\n",
      "Skipping pair (missing vector): P62995, P0DJD3\n",
      "Skipping pair (missing vector): P28358, Q9NNW7\n",
      "Skipping pair (missing vector): P09629, Q9NNW7\n",
      "Skipping pair (missing vector): Q16385, Q9NNW7\n",
      "Skipping pair (missing vector): Q9NSY1, Q9NNW7\n",
      "Skipping pair (missing vector): Q9BV36, P0DJD3\n",
      "Skipping pair (missing vector): Q12996, Q9NNW7\n",
      "Skipping pair (missing vector): Q9BYN8, P0DJD3\n",
      "Skipping pair (missing vector): A0AVK6, Q9NNW7\n",
      "Skipping pair (missing vector): P20671, Q9NNW7\n",
      "Skipping pair (missing vector): P28838, P0DJD3\n",
      "Skipping pair (missing vector): Q9Y222, Q9NNW7\n",
      "Skipping pair (missing vector): P16401, Q9NNW7\n",
      "Skipping pair (missing vector): Q96QT6, Q9NNW7\n",
      "Skipping pair (missing vector): P0DJD3, Q8IX03\n",
      "Skipping pair (missing vector): P0DJD3, Q76N89\n",
      "Skipping pair (missing vector): P0DJD3, Q07973\n",
      "Skipping pair (missing vector): P0DJD3, Q13242\n",
      "Skipping pair (missing vector): Q9NNW7, P53567\n",
      "Skipping pair (missing vector): Q9NNW7, Q9H9Q4\n",
      "Skipping pair (missing vector): Q9NNW7, P49639\n",
      "Skipping pair (missing vector): Q9NNW7, P11161\n",
      "Skipping pair (missing vector): Q9NNW7, Q9UJA3\n",
      "Skipping pair (missing vector): Q9NNW7, Q9NPI8\n",
      "Skipping pair (missing vector): Q9NNW7, P30876\n",
      "Skipping pair (missing vector): Q9NNW7, Q99990\n",
      "Skipping pair (missing vector): Q9NNW7, P32519\n",
      "Skipping pair (missing vector): P51693, P62328\n",
      "Skipping pair (missing vector): Q13418, P62328\n",
      "Skipping pair (missing vector): P62736, P62328\n",
      "Skipping pair (missing vector): P60709, P62328\n",
      "Skipping pair (missing vector): P08572, P0DJI8\n",
      "Skipping pair (missing vector): P48059, P62328\n",
      "Skipping pair (missing vector): P21980, P62328\n",
      "Skipping pair (missing vector): P62328, P60510\n",
      "Skipping pair (missing vector): Q96I34, Q16881\n",
      "Skipping pair (missing vector): P21462, P0DJI8\n",
      "Skipping pair (missing vector): P26678, O00631\n",
      "Skipping pair (missing vector): O14983, O00631\n",
      "Skipping pair (missing vector): O00299, P0DI82\n",
      "Skipping pair (missing vector): P53420, P0DJI8\n",
      "Skipping pair (missing vector): P11226, P0DI82\n",
      "Skipping pair (missing vector): P0DJI8, Q9BQE4\n",
      "Skipping pair (missing vector): Q8TEY7, Q92813\n",
      "Skipping pair (missing vector): Q16881, Q9NS18\n",
      "Skipping pair (missing vector): P0DI82, O15247\n",
      "Skipping pair (missing vector): P28289, P63313\n",
      "Skipping pair (missing vector): Q9NYU2, O60613\n",
      "Skipping pair (missing vector): P68871, P62341\n",
      "Skipping pair (missing vector): P63302, Q9UBR2\n",
      "→ Saved: train_dataset25.tsv & test_dataset25.tsv\n",
      "\n",
      "--- Processing Category C3 ---\n",
      "Skipping pair (missing vector): P03372, Q16881\n",
      "Skipping pair (missing vector): P68133, P62328\n",
      "Skipping pair (missing vector): P17980, Q9NNW7\n",
      "Skipping pair (missing vector): O75376, Q9NNW7\n",
      "Skipping pair (missing vector): Q96G74, P36969\n",
      "Skipping pair (missing vector): P63261, P62328\n",
      "Skipping pair (missing vector): O75865, P0DI81\n",
      "Skipping pair (missing vector): Q9NNW7, O00519\n",
      "Skipping pair (missing vector): Q9NNW7, Q96P44\n",
      "Skipping pair (missing vector): Q9NNW7, P57796\n",
      "Skipping pair (missing vector): Q9NNW7, Q2PPJ7\n",
      "Skipping pair (missing vector): Q9NNW7, Q14CZ8\n",
      "Skipping pair (missing vector): Q9NNW7, Q96A58\n",
      "Skipping pair (missing vector): Q9NNW7, Q86VQ3\n",
      "Skipping pair (missing vector): Q8ND90, P0CW24\n",
      "Skipping pair (missing vector): P0DI81, Q7Z392\n",
      "Skipping pair (missing vector): P57796, P0DJD3\n",
      "Skipping pair (missing vector): P57796, P55073\n",
      "Skipping pair (missing vector): P57796, Q9BVL4\n",
      "Skipping pair (missing vector): P57796, O60613\n",
      "Skipping pair (missing vector): P57796, P86790\n",
      "Skipping pair (missing vector): O00519, P0DJD3\n",
      "Skipping pair (missing vector): O00519, P55073\n",
      "Skipping pair (missing vector): O00519, Q9BVL4\n",
      "Skipping pair (missing vector): O00519, O60613\n",
      "Skipping pair (missing vector): O00519, P86790\n",
      "Skipping pair (missing vector): P0DJD3, Q96P44\n",
      "Skipping pair (missing vector): P0DJD3, Q60I27\n",
      "Skipping pair (missing vector): P0DJD3, Q96JB8\n",
      "Skipping pair (missing vector): P0DJD3, Q2PPJ7\n",
      "Skipping pair (missing vector): P0DJD3, Q14CZ8\n",
      "Skipping pair (missing vector): P0DJD3, Q96A58\n",
      "Skipping pair (missing vector): P0DJD3, Q86VQ3\n",
      "Skipping pair (missing vector): P55073, Q96P44\n",
      "Skipping pair (missing vector): P55073, Q60I27\n",
      "Skipping pair (missing vector): P55073, Q96JB8\n",
      "Skipping pair (missing vector): P55073, Q2PPJ7\n",
      "Skipping pair (missing vector): P55073, Q96A58\n",
      "Skipping pair (missing vector): Q9BVL4, Q14CZ8\n",
      "Skipping pair (missing vector): Q9BVL4, Q96A58\n",
      "Skipping pair (missing vector): Q9BVL4, Q86VQ3\n",
      "Skipping pair (missing vector): O60613, Q60I27\n",
      "Skipping pair (missing vector): O60613, Q96JB8\n",
      "Skipping pair (missing vector): O60613, Q2PPJ7\n",
      "Skipping pair (missing vector): O60613, Q14CZ8\n",
      "Skipping pair (missing vector): O60613, Q96A58\n",
      "Skipping pair (missing vector): O60613, Q86VQ3\n",
      "Skipping pair (missing vector): P86790, Q96P44\n",
      "Skipping pair (missing vector): P86790, Q96JB8\n",
      "Skipping pair (missing vector): P86790, Q2PPJ7\n",
      "Skipping pair (missing vector): P86790, Q14CZ8\n",
      "Skipping pair (missing vector): P86790, Q96A58\n",
      "Skipping pair (missing vector): P00519, P07203\n",
      "Skipping pair (missing vector): P55072, Q9BQE4\n",
      "Skipping pair (missing vector): Q9Y6I7, Q92813\n",
      "→ Saved: train_dataset31.tsv & test_dataset31.tsv\n",
      "Skipping pair (missing vector): P03372, Q16881\n",
      "Skipping pair (missing vector): Q14686, Q9NNW7\n",
      "Skipping pair (missing vector): P68133, P62328\n",
      "Skipping pair (missing vector): P17980, Q9NNW7\n",
      "Skipping pair (missing vector): P67870, P62945\n",
      "Skipping pair (missing vector): Q96G74, P36969\n",
      "Skipping pair (missing vector): P40692, P62328\n",
      "Skipping pair (missing vector): Q9Y6I7, Q92813\n",
      "Skipping pair (missing vector): Q9NNW7, O00519\n",
      "Skipping pair (missing vector): Q9NNW7, Q96P44\n",
      "Skipping pair (missing vector): Q9NNW7, P57796\n",
      "Skipping pair (missing vector): Q9NNW7, Q2PPJ7\n",
      "Skipping pair (missing vector): Q9NNW7, Q14CZ8\n",
      "Skipping pair (missing vector): Q9NNW7, Q96A58\n",
      "Skipping pair (missing vector): Q9NNW7, Q86VQ3\n",
      "Skipping pair (missing vector): Q92813, P62253\n",
      "Skipping pair (missing vector): Q92813, O60337\n",
      "Skipping pair (missing vector): P57796, P0DJD3\n",
      "Skipping pair (missing vector): P57796, P55073\n",
      "Skipping pair (missing vector): P57796, Q9BVL4\n",
      "Skipping pair (missing vector): P57796, O60613\n",
      "Skipping pair (missing vector): P57796, P86790\n",
      "Skipping pair (missing vector): O00519, P0DJD3\n",
      "Skipping pair (missing vector): O00519, P55073\n",
      "Skipping pair (missing vector): O00519, Q9BVL4\n",
      "Skipping pair (missing vector): O00519, O60613\n",
      "Skipping pair (missing vector): O00519, P86790\n",
      "Skipping pair (missing vector): Q96P44, P0DJD3\n",
      "Skipping pair (missing vector): Q96P44, P55073\n",
      "Skipping pair (missing vector): Q96P44, P86790\n",
      "Skipping pair (missing vector): Q60I27, P0DJD3\n",
      "Skipping pair (missing vector): Q60I27, P55073\n",
      "Skipping pair (missing vector): Q60I27, O60613\n",
      "Skipping pair (missing vector): Q96JB8, P0DJD3\n",
      "Skipping pair (missing vector): Q96JB8, P55073\n",
      "Skipping pair (missing vector): Q96JB8, O60613\n",
      "Skipping pair (missing vector): Q96JB8, P86790\n",
      "Skipping pair (missing vector): Q2PPJ7, P0DJD3\n",
      "Skipping pair (missing vector): Q2PPJ7, P55073\n",
      "Skipping pair (missing vector): Q2PPJ7, O60613\n",
      "Skipping pair (missing vector): Q2PPJ7, P86790\n",
      "Skipping pair (missing vector): Q14CZ8, P0DJD3\n",
      "Skipping pair (missing vector): Q14CZ8, Q9BVL4\n",
      "Skipping pair (missing vector): Q14CZ8, O60613\n",
      "Skipping pair (missing vector): Q14CZ8, P86790\n",
      "Skipping pair (missing vector): Q96A58, P0DJD3\n",
      "Skipping pair (missing vector): Q96A58, P55073\n",
      "Skipping pair (missing vector): Q96A58, Q9BVL4\n",
      "Skipping pair (missing vector): Q96A58, O60613\n",
      "Skipping pair (missing vector): Q96A58, P86790\n",
      "Skipping pair (missing vector): Q86VQ3, P0DJD3\n",
      "Skipping pair (missing vector): Q86VQ3, Q9BVL4\n",
      "Skipping pair (missing vector): Q86VQ3, O60613\n",
      "Skipping pair (missing vector): Q86VQ3, P86790\n",
      "Skipping pair (missing vector): P06733, P0DI81\n",
      "Skipping pair (missing vector): P0DI81, O75865\n",
      "Skipping pair (missing vector): P0DI81, O43617\n",
      "Skipping pair (missing vector): P0DI81, Q8IUR0\n",
      "Skipping pair (missing vector): P0DI81, Q96Q05\n",
      "→ Saved: train_dataset32.tsv & test_dataset32.tsv\n",
      "Skipping pair (missing vector): Q96G74, P36969\n",
      "Skipping pair (missing vector): Q14686, Q9NNW7\n",
      "Skipping pair (missing vector): P06733, P0DI81\n",
      "Skipping pair (missing vector): P67870, P62945\n",
      "Skipping pair (missing vector): Q8TEY7, Q92813\n",
      "Skipping pair (missing vector): Q9Y6I7, Q92813\n",
      "Skipping pair (missing vector): Q9NNW7, O00519\n",
      "Skipping pair (missing vector): Q9NNW7, Q96P44\n",
      "Skipping pair (missing vector): Q9NNW7, Q2PPJ7\n",
      "Skipping pair (missing vector): Q9NNW7, Q14CZ8\n",
      "Skipping pair (missing vector): Q9NNW7, Q96A58\n",
      "Skipping pair (missing vector): Q9NNW7, Q86VQ3\n",
      "Skipping pair (missing vector): Q96Q05, P0DI81\n",
      "Skipping pair (missing vector): P0DI81, Q86SZ2\n",
      "Skipping pair (missing vector): P26678, O00631\n",
      "Skipping pair (missing vector): O00519, P0DJD3\n",
      "Skipping pair (missing vector): O00519, P55073\n",
      "Skipping pair (missing vector): O00519, Q9BVL4\n",
      "Skipping pair (missing vector): O00519, O60613\n",
      "Skipping pair (missing vector): O00519, P86790\n",
      "Skipping pair (missing vector): Q96P44, P0DJD3\n",
      "Skipping pair (missing vector): Q96P44, P55073\n",
      "Skipping pair (missing vector): Q96P44, P86790\n",
      "Skipping pair (missing vector): Q60I27, P0DJD3\n",
      "Skipping pair (missing vector): Q60I27, P55073\n",
      "Skipping pair (missing vector): Q60I27, O60613\n",
      "Skipping pair (missing vector): Q96JB8, P0DJD3\n",
      "Skipping pair (missing vector): Q96JB8, P55073\n",
      "Skipping pair (missing vector): Q96JB8, O60613\n",
      "Skipping pair (missing vector): Q96JB8, P86790\n",
      "Skipping pair (missing vector): Q2PPJ7, P0DJD3\n",
      "Skipping pair (missing vector): Q2PPJ7, P55073\n",
      "Skipping pair (missing vector): Q2PPJ7, O60613\n",
      "Skipping pair (missing vector): Q2PPJ7, P86790\n",
      "Skipping pair (missing vector): Q14CZ8, P0DJD3\n",
      "Skipping pair (missing vector): Q14CZ8, Q9BVL4\n",
      "Skipping pair (missing vector): Q14CZ8, O60613\n",
      "Skipping pair (missing vector): Q14CZ8, P86790\n",
      "Skipping pair (missing vector): Q96A58, P0DJD3\n",
      "Skipping pair (missing vector): Q96A58, P55073\n",
      "Skipping pair (missing vector): Q96A58, Q9BVL4\n",
      "Skipping pair (missing vector): Q96A58, O60613\n",
      "Skipping pair (missing vector): Q96A58, P86790\n",
      "Skipping pair (missing vector): Q86VQ3, P0DJD3\n",
      "Skipping pair (missing vector): Q86VQ3, Q9BVL4\n",
      "Skipping pair (missing vector): Q86VQ3, O60613\n",
      "Skipping pair (missing vector): Q86VQ3, P86790\n",
      "Skipping pair (missing vector): O14960, P0DJD3\n",
      "Skipping pair (missing vector): O14960, P55073\n",
      "Skipping pair (missing vector): O14960, Q9BVL4\n",
      "Skipping pair (missing vector): P00519, P07203\n",
      "Skipping pair (missing vector): P63261, P62328\n",
      "→ Saved: train_dataset33.tsv & test_dataset33.tsv\n",
      "Skipping pair (missing vector): P63261, P62328\n",
      "Skipping pair (missing vector): Q96G74, P36969\n",
      "Skipping pair (missing vector): P68133, P62328\n",
      "Skipping pair (missing vector): P67870, P62945\n",
      "Skipping pair (missing vector): P06733, P0DI81\n",
      "Skipping pair (missing vector): P0DI81, O75865\n",
      "Skipping pair (missing vector): P0DI81, O43617\n",
      "Skipping pair (missing vector): P0DI81, Q86SZ2\n",
      "Skipping pair (missing vector): P0DI81, Q9Y296\n",
      "Skipping pair (missing vector): P0DI81, Q8IUR0\n",
      "Skipping pair (missing vector): P0DI81, Q96Q05\n",
      "Skipping pair (missing vector): O00519, P0DJD3\n",
      "Skipping pair (missing vector): O00519, P55073\n",
      "Skipping pair (missing vector): O00519, Q9BVL4\n",
      "Skipping pair (missing vector): O00519, O60613\n",
      "Skipping pair (missing vector): O00519, P86790\n",
      "Skipping pair (missing vector): Q96P44, P0DJD3\n",
      "Skipping pair (missing vector): Q96P44, P55073\n",
      "Skipping pair (missing vector): Q96P44, P86790\n",
      "Skipping pair (missing vector): Q60I27, P0DJD3\n",
      "Skipping pair (missing vector): Q60I27, P55073\n",
      "Skipping pair (missing vector): Q60I27, O60613\n",
      "Skipping pair (missing vector): Q96JB8, P0DJD3\n",
      "Skipping pair (missing vector): Q96JB8, P55073\n",
      "Skipping pair (missing vector): Q96JB8, O60613\n",
      "Skipping pair (missing vector): Q96JB8, P86790\n",
      "Skipping pair (missing vector): P57796, P0DJD3\n",
      "Skipping pair (missing vector): P57796, P55073\n",
      "Skipping pair (missing vector): P57796, Q9BVL4\n",
      "Skipping pair (missing vector): P57796, O60613\n",
      "Skipping pair (missing vector): P57796, P86790\n",
      "Skipping pair (missing vector): Q2PPJ7, P0DJD3\n",
      "Skipping pair (missing vector): Q2PPJ7, P55073\n",
      "Skipping pair (missing vector): Q2PPJ7, O60613\n",
      "Skipping pair (missing vector): Q2PPJ7, P86790\n",
      "Skipping pair (missing vector): Q14CZ8, P0DJD3\n",
      "Skipping pair (missing vector): Q14CZ8, Q9BVL4\n",
      "Skipping pair (missing vector): Q14CZ8, O60613\n",
      "Skipping pair (missing vector): Q14CZ8, P86790\n",
      "Skipping pair (missing vector): Q96A58, P0DJD3\n",
      "Skipping pair (missing vector): Q96A58, P55073\n",
      "Skipping pair (missing vector): Q96A58, Q9BVL4\n",
      "Skipping pair (missing vector): Q96A58, O60613\n",
      "Skipping pair (missing vector): Q96A58, P86790\n",
      "Skipping pair (missing vector): Q86VQ3, P0DJD3\n",
      "Skipping pair (missing vector): Q86VQ3, Q9BVL4\n",
      "Skipping pair (missing vector): P55072, Q9BQE4\n",
      "Skipping pair (missing vector): Q8WTT0, Q9NNW7\n",
      "Skipping pair (missing vector): P28221, Q9NNW7\n",
      "Skipping pair (missing vector): P51684, Q9NNW7\n",
      "Skipping pair (missing vector): Q9NNW7, O60741\n",
      "→ Saved: train_dataset34.tsv & test_dataset34.tsv\n",
      "Skipping pair (missing vector): P06733, P0DI81\n",
      "Skipping pair (missing vector): Q14686, Q9NNW7\n",
      "Skipping pair (missing vector): Q8TEY7, Q92813\n",
      "Skipping pair (missing vector): Q9NNW7, O00519\n",
      "Skipping pair (missing vector): Q9NNW7, Q96P44\n",
      "Skipping pair (missing vector): Q9NNW7, P57796\n",
      "Skipping pair (missing vector): Q9NNW7, Q2PPJ7\n",
      "Skipping pair (missing vector): Q9NNW7, Q14CZ8\n",
      "Skipping pair (missing vector): Q9NNW7, Q96A58\n",
      "Skipping pair (missing vector): Q9NNW7, Q86VQ3\n",
      "Skipping pair (missing vector): Q9Y296, P0DI81\n",
      "Skipping pair (missing vector): Q96Q05, P0DI81\n",
      "Skipping pair (missing vector): O43617, P0DI81\n",
      "Skipping pair (missing vector): Q8ND90, P0CW24\n",
      "Skipping pair (missing vector): P0DI81, Q8IUR0\n",
      "Skipping pair (missing vector): O00631, O14983\n",
      "Skipping pair (missing vector): Q92813, O60337\n",
      "Skipping pair (missing vector): O00519, P0DJD3\n",
      "Skipping pair (missing vector): O00519, P55073\n",
      "Skipping pair (missing vector): O00519, Q9BVL4\n",
      "Skipping pair (missing vector): O00519, O60613\n",
      "Skipping pair (missing vector): O00519, P86790\n",
      "Skipping pair (missing vector): Q96P44, P0DJD3\n",
      "Skipping pair (missing vector): Q96P44, P55073\n",
      "Skipping pair (missing vector): Q96P44, P86790\n",
      "Skipping pair (missing vector): Q60I27, P0DJD3\n",
      "Skipping pair (missing vector): Q60I27, P55073\n",
      "Skipping pair (missing vector): Q60I27, O60613\n",
      "Skipping pair (missing vector): Q96JB8, P0DJD3\n",
      "Skipping pair (missing vector): Q96JB8, P55073\n",
      "Skipping pair (missing vector): Q96JB8, O60613\n",
      "Skipping pair (missing vector): Q96JB8, P86790\n",
      "Skipping pair (missing vector): P57796, P0DJD3\n",
      "Skipping pair (missing vector): P57796, P55073\n",
      "Skipping pair (missing vector): P57796, Q9BVL4\n",
      "Skipping pair (missing vector): P57796, O60613\n",
      "Skipping pair (missing vector): P57796, P86790\n",
      "Skipping pair (missing vector): Q2PPJ7, P0DJD3\n",
      "Skipping pair (missing vector): Q2PPJ7, P55073\n",
      "Skipping pair (missing vector): Q2PPJ7, O60613\n",
      "Skipping pair (missing vector): Q2PPJ7, P86790\n",
      "Skipping pair (missing vector): Q14CZ8, P0DJD3\n",
      "Skipping pair (missing vector): Q14CZ8, Q9BVL4\n",
      "Skipping pair (missing vector): Q14CZ8, O60613\n",
      "Skipping pair (missing vector): Q14CZ8, P86790\n",
      "Skipping pair (missing vector): Q96A58, P0DJD3\n",
      "Skipping pair (missing vector): Q96A58, P55073\n",
      "Skipping pair (missing vector): Q96A58, Q9BVL4\n",
      "Skipping pair (missing vector): Q96A58, O60613\n",
      "Skipping pair (missing vector): Q96A58, P86790\n",
      "Skipping pair (missing vector): Q86VQ3, P0DJD3\n",
      "Skipping pair (missing vector): Q86VQ3, Q9BVL4\n",
      "Skipping pair (missing vector): Q86VQ3, O60613\n",
      "Skipping pair (missing vector): Q86VQ3, P86790\n",
      "Skipping pair (missing vector): P55072, Q9BQE4\n",
      "Skipping pair (missing vector): P63261, P62328\n",
      "Skipping pair (missing vector): P67870, P62945\n",
      "→ Saved: train_dataset35.tsv & test_dataset35.tsv\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "\n",
    "# features_path = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\ngram_feature_vector_new.csv\"\n",
    "\n",
    "# # Load CSV with headers\n",
    "# features_df = pd.read_csv(features_path)\n",
    "\n",
    "\n",
    "# features_df[\"Sequence_ID\"] = features_df[\"Sequence_ID\"].apply(lambda x: x.split('|')[1] if '|' in x else x)\n",
    "\n",
    "# # Create feature dictionary (key: ID, value: 400-d vector)\n",
    "# feature_dict = {\n",
    "#     row[\"Sequence_ID\"]: row.drop(\"Sequence_ID\").values.astype(float)\n",
    "#     for _, row in features_df.iterrows()\n",
    "# }\n",
    "\n",
    "\n",
    "# train_csv_path = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\TrainSet1.csv\"\n",
    "# test_csv_path = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\TestSet1.csv\"\n",
    "\n",
    "# train_df = pd.read_csv(train_csv_path)\n",
    "# test_df = pd.read_csv(test_csv_path)\n",
    "\n",
    "\n",
    "# def generate_formatted_lines(pairs_df, feature_dict):\n",
    "#     lines = []\n",
    "    \n",
    "    \n",
    "#     for _, row in pairs_df.iterrows():\n",
    "#         id1 = str(row[\"IntA\"])\n",
    "#         id2 = str(row[\"IntB\"])\n",
    "#         label = str(row[\"Cls\"])\n",
    "\n",
    "#         if id1 in feature_dict and id2 in feature_dict:\n",
    "#             vec1 = feature_dict[id1]\n",
    "#             vec2 = feature_dict[id2]\n",
    "#             combined = np.concatenate([vec1, vec2])\n",
    "#             vec_str = ' '.join(map(str, combined))\n",
    "#             line = f\"{id1}+{id2}\\t{vec_str}\\t{label}\"\n",
    "#             lines.append(line)\n",
    "#         else:\n",
    "#             print(f\"Skipping pair (missing vector): {id1}, {id2}\")\n",
    "            \n",
    "\n",
    "    \n",
    "#     return lines\n",
    "\n",
    "\n",
    "# train_lines = generate_formatted_lines(train_df, feature_dict)\n",
    "# test_lines = generate_formatted_lines(test_df, feature_dict)\n",
    "\n",
    "# train_output_path = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\train_dataset11.tsv\"\n",
    "# test_output_path = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\test_dataset11.tsv\"\n",
    "\n",
    "# with open(train_output_path, 'w') as f:\n",
    "#     for line in train_lines:\n",
    "#         f.write(line + '\\n')\n",
    "\n",
    "# with open(test_output_path, 'w') as f:\n",
    "#     for line in test_lines:\n",
    "#         f.write(line + '\\n')\n",
    "\n",
    "# # print(\" Train and test datasets saved successfully!\")\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# --- Load feature vectors ---\n",
    "features_path = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\ngram_feature_vector_new.csv\"\n",
    "features_df = pd.read_csv(features_path)\n",
    "features_df[\"Sequence_ID\"] = features_df[\"Sequence_ID\"].apply(lambda x: x.split('|')[1] if '|' in x else x)\n",
    "\n",
    "feature_dict = {\n",
    "    row[\"Sequence_ID\"]: row.drop(\"Sequence_ID\").values.astype(float)\n",
    "    for _, row in features_df.iterrows()\n",
    "}\n",
    "\n",
    "# --- Define dataset path structure ---\n",
    "dataset_map = {\n",
    "    \"C1\": [11, 12, 13, 14, 15],\n",
    "    \"C2\": [21, 22, 23, 24, 25],\n",
    "    \"C3\": [31, 32, 33, 34, 35]\n",
    "}\n",
    "\n",
    "base_input = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Dataset\"\n",
    "base_output = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Dataset_output\"\n",
    "\n",
    "# --- Format generator ---\n",
    "def generate_formatted_lines(pairs_df, feature_dict):\n",
    "    lines = []\n",
    "    for _, row in pairs_df.iterrows():\n",
    "        id1 = str(row[\"IntA\"])\n",
    "        id2 = str(row[\"IntB\"])\n",
    "        # label = str(row[\"Cls\"])\n",
    "        if 'Cls' in row:\n",
    "            label = row['Cls']\n",
    "        elif 'Class' in row:\n",
    "            label = row['Class']\n",
    "        else:\n",
    "            # print(\" Label column not found in row\")\n",
    "            continue\n",
    "\n",
    "        if id1 in feature_dict and id2 in feature_dict:\n",
    "            vec1 = feature_dict[id1]\n",
    "            vec2 = feature_dict[id2]\n",
    "            combined = np.concatenate([vec1, vec2])\n",
    "            vec_str = ' '.join(map(str, combined))\n",
    "            line = f\"{id1}+{id2}\\t{vec_str}\\t{label}\"\n",
    "            lines.append(line)\n",
    "        else:\n",
    "            print(f\"Skipping pair (missing vector): {id1}, {id2}\")\n",
    "    return lines\n",
    "\n",
    "# --- Process all datasets ---\n",
    "for cat, nums in dataset_map.items():\n",
    "    print(f\"\\n--- Processing Category {cat} ---\")\n",
    "    for num in nums:\n",
    "        idx = str(num)[-1]\n",
    "\n",
    "        train_path = fr\"{base_input}\\Dataset_{cat}\\TrainSet{idx}.csv\"\n",
    "        test_path = fr\"{base_input}\\Dataset_{cat}\\TestSet{idx}.csv\"\n",
    "\n",
    "        train_out = fr\"{base_output}\\For_{cat}\\For_Train\\train_dataset{num}.tsv\"\n",
    "        test_out = fr\"{base_output}\\For_{cat}\\For_Test\\test_dataset{num}.tsv\"\n",
    "\n",
    "        train_df = pd.read_csv(train_path)\n",
    "        test_df = pd.read_csv(test_path)\n",
    "\n",
    "        train_lines = generate_formatted_lines(train_df, feature_dict)\n",
    "        test_lines = generate_formatted_lines(test_df, feature_dict)\n",
    "\n",
    "        os.makedirs(os.path.dirname(train_out), exist_ok=True)\n",
    "        os.makedirs(os.path.dirname(test_out), exist_ok=True)\n",
    "\n",
    "        with open(train_out, 'w') as f:\n",
    "            for line in train_lines:\n",
    "                f.write(line + '\\n')\n",
    "\n",
    "        with open(test_out, 'w') as f:\n",
    "            for line in test_lines:\n",
    "                f.write(line + '\\n')\n",
    "\n",
    "        print(f\"→ Saved: train_dataset{num}.tsv & test_dataset{num}.tsv\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97a0d458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"dhdf\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c31951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# # === Load the train .tsv file ===\n",
    "# train_path = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\train_dataset11.tsv\"\n",
    "# train_df = pd.read_csv(train_path, sep=\"\\t\", header=None)\n",
    "\n",
    "# # Column structure: [ID1+ID2, vector_string, label]\n",
    "# # Split the vector string into list of floats\n",
    "# train_df[1] = train_df[1].apply(lambda x: list(map(float, x.strip().split())))\n",
    "\n",
    "# # Extract X and Y\n",
    "# Train_X = np.vstack(train_df[1].values)   # Shape: (num_samples, 800)\n",
    "# Train_Y = train_df[2].astype(int).values  # Shape: (num_samples,)\n",
    "\n",
    "# print(\"Train_X shape:\", Train_X.shape)\n",
    "# print(\"Train_Y shape:\", Train_Y.shape)\n",
    "\n",
    "# # === Same for test file ===\n",
    "# test_path = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\test_dataset11.tsv\"\n",
    "# test_df = pd.read_csv(test_path, sep=\"\\t\", header=None)\n",
    "\n",
    "# test_df[1] = test_df[1].apply(lambda x: list(map(float, x.strip().split())))\n",
    "# Test_X = np.vstack(test_df[1].values)\n",
    "# Test_Y = test_df[2].astype(int).values\n",
    "\n",
    "# print(\"Test_X shape:\", Test_X.shape)\n",
    "# print(\"Test_Y shape:\", Test_Y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384990ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# # === Load the train .tsv file ===\n",
    "# train_path = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\train_dataset11.tsv\"\n",
    "# train_df = pd.read_csv(train_path, sep=\"\\t\", header=None)\n",
    "\n",
    "# # Column structure: [ID1+ID2, vector_string, label]\n",
    "# # Split the vector string into list of floats\n",
    "# train_df[1] = train_df[1].apply(lambda x: list(map(float, x.strip().split())))\n",
    "\n",
    "# # Extract X and Y\n",
    "# Train_X = np.vstack(train_df[1].values)              # Shape: (num_samples, 800)\n",
    "# Train_Y = train_df[2].astype(int).values.reshape(-1, 1)  # Shape: (num_samples, 1)\n",
    "\n",
    "# print(\"Train_X shape:\", Train_X.shape)\n",
    "# print(\"Train_Y shape:\", Train_Y.shape)\n",
    "\n",
    "# # === Same for test file ===\n",
    "# test_path = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\test_dataset11.tsv\"\n",
    "# test_df = pd.read_csv(test_path, sep=\"\\t\", header=None)\n",
    "\n",
    "# test_df[1] = test_df[1].apply(lambda x: list(map(float, x.strip().split())))\n",
    "# Test_X = np.vstack(test_df[1].values)\n",
    "# Test_Y = test_df[2].astype(int).values.reshape(-1, 1)\n",
    "\n",
    "# print(\"Test_X shape:\", Test_X.shape)\n",
    "# print(\"Test_Y shape:\", Test_Y.shape)\n",
    "\n",
    "# # Preview: First 5 rows of Train_X and Train_Y\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a160fba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train_X shape: (45380, 800)\n",
      "✅ Train_Y shape: (45380, 1)\n",
      "✅ Test_X shape: (5049, 800)\n",
      "✅ Test_Y shape: (5049, 1)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "train_path = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\train_dataset11.tsv\"\n",
    "train_df = pd.read_csv(train_path, sep=\"\\t\", header=None)\n",
    "\n",
    "\n",
    "train_df[1] = train_df[1].apply(lambda x: list(map(float, x.strip().split())))\n",
    "\n",
    "\n",
    "Train_X = np.vstack(train_df[1].values)              # Shape: (num_samples, 800)\n",
    "Train_Y = train_df[2].astype(int).values.reshape(-1, 1)  # Shape: (num_samples, 1)\n",
    "\n",
    "print(\" Train_X shape:\", Train_X.shape)\n",
    "print(\" Train_Y shape:\", Train_Y.shape)\n",
    "\n",
    "\n",
    "test_path = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\test_dataset11.tsv\"\n",
    "test_df = pd.read_csv(test_path, sep=\"\\t\", header=None)\n",
    "\n",
    "test_df[1] = test_df[1].apply(lambda x: list(map(float, x.strip().split())))\n",
    "Test_X = np.vstack(test_df[1].values)\n",
    "Test_Y = test_df[2].astype(int).values.reshape(-1, 1)\n",
    "\n",
    "print(\" Test_X shape:\", Test_X.shape)\n",
    "print(\" Test_Y shape:\", Test_Y.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88e8b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📂 Category: C1\n",
      "✅ Dataset 11:\n",
      "   🔹 Train_X shape: (45380, 800)\n",
      "   🔹 Train_Y shape: (45380, 1)\n",
      "   🔹 Test_X shape : (5049, 800)\n",
      "   🔹 Test_Y shape : (5049, 1)\n",
      "✅ Dataset 12:\n",
      "   🔹 Train_X shape: (45383, 800)\n",
      "   🔹 Train_Y shape: (45383, 1)\n",
      "   🔹 Test_X shape : (5046, 800)\n",
      "   🔹 Test_Y shape : (5046, 1)\n",
      "✅ Dataset 13:\n",
      "   🔹 Train_X shape: (45380, 800)\n",
      "   🔹 Train_Y shape: (45380, 1)\n",
      "   🔹 Test_X shape : (5049, 800)\n",
      "   🔹 Test_Y shape : (5049, 1)\n",
      "✅ Dataset 14:\n",
      "   🔹 Train_X shape: (45386, 800)\n",
      "   🔹 Train_Y shape: (45386, 1)\n",
      "   🔹 Test_X shape : (5043, 800)\n",
      "   🔹 Test_Y shape : (5043, 1)\n",
      "✅ Dataset 15:\n",
      "   🔹 Train_X shape: (45384, 800)\n",
      "   🔹 Train_Y shape: (45384, 1)\n",
      "   🔹 Test_X shape : (5045, 800)\n",
      "   🔹 Test_Y shape : (5045, 1)\n",
      "\n",
      "📂 Category: C2\n",
      "✅ Dataset 21:\n",
      "   🔹 Train_X shape: (18940, 800)\n",
      "   🔹 Train_Y shape: (18940, 1)\n",
      "   🔹 Test_X shape : (1496, 800)\n",
      "   🔹 Test_Y shape : (1496, 1)\n",
      "✅ Dataset 22:\n",
      "   🔹 Train_X shape: (18356, 800)\n",
      "   🔹 Train_Y shape: (18356, 1)\n",
      "   🔹 Test_X shape : (1497, 800)\n",
      "   🔹 Test_Y shape : (1497, 1)\n",
      "✅ Dataset 23:\n",
      "   🔹 Train_X shape: (18982, 800)\n",
      "   🔹 Train_Y shape: (18982, 1)\n",
      "   🔹 Test_X shape : (1488, 800)\n",
      "   🔹 Test_Y shape : (1488, 1)\n",
      "✅ Dataset 24:\n",
      "   🔹 Train_X shape: (19237, 800)\n",
      "   🔹 Train_Y shape: (19237, 1)\n",
      "   🔹 Test_X shape : (1491, 800)\n",
      "   🔹 Test_Y shape : (1491, 1)\n",
      "✅ Dataset 25:\n",
      "   🔹 Train_X shape: (19176, 800)\n",
      "   🔹 Train_Y shape: (19176, 1)\n",
      "   🔹 Test_X shape : (1499, 800)\n",
      "   🔹 Test_Y shape : (1499, 1)\n",
      "\n",
      "📂 Category: C3\n",
      "✅ Dataset 31:\n",
      "   🔹 Train_X shape: (21512, 800)\n",
      "   🔹 Train_Y shape: (21512, 1)\n",
      "   🔹 Test_X shape : (5923, 800)\n",
      "   🔹 Test_Y shape : (5923, 1)\n",
      "✅ Dataset 32:\n",
      "   🔹 Train_X shape: (21964, 800)\n",
      "   🔹 Train_Y shape: (21964, 1)\n",
      "   🔹 Test_X shape : (5777, 800)\n",
      "   🔹 Test_Y shape : (5777, 1)\n",
      "✅ Dataset 33:\n",
      "   🔹 Train_X shape: (20898, 800)\n",
      "   🔹 Train_Y shape: (20898, 1)\n",
      "   🔹 Test_X shape : (6264, 800)\n",
      "   🔹 Test_Y shape : (6264, 1)\n",
      "✅ Dataset 34:\n",
      "   🔹 Train_X shape: (20980, 800)\n",
      "   🔹 Train_Y shape: (20980, 1)\n",
      "   🔹 Test_X shape : (6363, 800)\n",
      "   🔹 Test_Y shape : (6363, 1)\n",
      "✅ Dataset 35:\n",
      "   🔹 Train_X shape: (21472, 800)\n",
      "   🔹 Train_Y shape: (21472, 1)\n",
      "   🔹 Test_X shape : (6153, 800)\n",
      "   🔹 Test_Y shape : (6153, 1)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# --- Dataset map same as before ---\n",
    "dataset_map = {\n",
    "    \"C1\": [11, 12, 13, 14, 15],\n",
    "    \"C2\": [21, 22, 23, 24, 25],\n",
    "    \"C3\": [31, 32, 33, 34, 35]\n",
    "}\n",
    "\n",
    "base_output = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Dataset_output\"\n",
    "\n",
    "# --- Function to load tsv and extract features/labels ---\n",
    "def load_dataset(tsv_path):\n",
    "    df = pd.read_csv(tsv_path, sep=\"\\t\", header=None)\n",
    "    df[1] = df[1].apply(lambda x: list(map(float, x.strip().split())))\n",
    "    X = np.vstack(df[1].values)\n",
    "    Y = df[2].astype(int).values.reshape(-1, 1)\n",
    "    return X, Y\n",
    "\n",
    "# --- Process all datasets ---\n",
    "for cat, nums in dataset_map.items():\n",
    "    print(f\"\\n Category: {cat}\")\n",
    "    for num in nums:\n",
    "        train_path = fr\"{base_output}\\For_{cat}\\For_Train\\train_dataset{num}.tsv\"\n",
    "        test_path = fr\"{base_output}\\For_{cat}\\For_Test\\test_dataset{num}.tsv\"\n",
    "\n",
    "        if not os.path.exists(train_path) or not os.path.exists(test_path):\n",
    "            print(f\" Missing files for dataset {num}\")\n",
    "            continue\n",
    "\n",
    "        Train_X, Train_Y = load_dataset(train_path)\n",
    "        Test_X, Test_Y = load_dataset(test_path)\n",
    "\n",
    "        print(f\" Dataset {num}:\")\n",
    "        print(f\"   🔹 Train_X shape: {Train_X.shape}\")\n",
    "        print(f\"   🔹 Train_Y shape: {Train_Y.shape}\")\n",
    "        print(f\"   🔹 Test_X shape : {Test_X.shape}\")\n",
    "        print(f\"   🔹 Test_Y shape : {Test_Y.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e3f871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train_X shape: (45380, 800)\n",
      "✅ Train_Y shape: (1, 45380)\n",
      "✅ Test_X shape: (5049, 800)\n",
      "✅ Test_Y shape: (1, 5049)\n",
      "\n",
      "🔍 Train_Y preview: [[1 1 1 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# === Load the train .tsv file ===\n",
    "train_path = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\train_dataset11.tsv\"\n",
    "train_df = pd.read_csv(train_path, sep=\"\\t\", header=None)\n",
    "\n",
    "# Column structure: [ID1+ID2, vector_string, label]\n",
    "# Split the vector string into list of floats\n",
    "train_df[1] = train_df[1].apply(lambda x: list(map(float, x.strip().split())))\n",
    "\n",
    "# Extract X and Y\n",
    "Train_X = np.vstack(train_df[1].values)  # Shape: (num_samples, 800)\n",
    "Train_Y = train_df[2].astype(int).values  # Shape: (num_samples, )\n",
    "\n",
    "# Reshape Train_Y to be a single row (1, num_samples)\n",
    "Train_Y = Train_Y.reshape(1, -1)  # Shape: (1, num_samples)\n",
    "\n",
    "print(\"Train_X shape:\", Train_X.shape)\n",
    "print(\"Train_Y shape:\", Train_Y.shape)\n",
    "\n",
    "# === Same for test file ===\n",
    "test_path = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\test_dataset11.tsv\"\n",
    "test_df = pd.read_csv(test_path, sep=\"\\t\", header=None)\n",
    "\n",
    "test_df[1] = test_df[1].apply(lambda x: list(map(float, x.strip().split())))\n",
    "Test_X = np.vstack(test_df[1].values)\n",
    "Test_Y = test_df[2].astype(int).values\n",
    "\n",
    "# Reshape Test_Y to be a single row (1, num_samples)\n",
    "Test_Y = Test_Y.reshape(1, -1)  # Shape: (1, num_samples)\n",
    "\n",
    "print(\"Test_X shape:\", Test_X.shape)\n",
    "print(\"Test_Y shape:\", Test_Y.shape)\n",
    "\n",
    "# Preview: First 5 rows of Train_X and Train_Y\n",
    "print(\"\\n🔍 Train_Y preview:\", Train_Y[:, :])  # Preview first 5 labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4b38e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Processing Fold: C1 - 11\n",
      " Train_X shape: (45380, 800)\n",
      " Train_Y shape: (1, 45380)\n",
      " Test_X shape: (5049, 800)\n",
      " Test_Y shape: (1, 5049)\n",
      "\n",
      " Processing Fold: C1 - 12\n",
      " Train_X shape: (45383, 800)\n",
      " Train_Y shape: (1, 45383)\n",
      " Test_X shape: (5046, 800)\n",
      " Test_Y shape: (1, 5046)\n",
      "\n",
      " Processing Fold: C1 - 13\n",
      " Train_X shape: (45380, 800)\n",
      " Train_Y shape: (1, 45380)\n",
      " Test_X shape: (5049, 800)\n",
      " Test_Y shape: (1, 5049)\n",
      "\n",
      " Processing Fold: C1 - 14\n",
      " Train_X shape: (45386, 800)\n",
      " Train_Y shape: (1, 45386)\n",
      " Test_X shape: (5043, 800)\n",
      " Test_Y shape: (1, 5043)\n",
      "\n",
      " Processing Fold: C1 - 15\n",
      " Train_X shape: (45384, 800)\n",
      " Train_Y shape: (1, 45384)\n",
      " Test_X shape: (5045, 800)\n",
      " Test_Y shape: (1, 5045)\n",
      "\n",
      " Processing Fold: C2 - 21\n",
      " Train_X shape: (18940, 800)\n",
      " Train_Y shape: (1, 18940)\n",
      " Test_X shape: (1496, 800)\n",
      " Test_Y shape: (1, 1496)\n",
      "\n",
      " Processing Fold: C2 - 22\n",
      " Train_X shape: (18356, 800)\n",
      " Train_Y shape: (1, 18356)\n",
      " Test_X shape: (1497, 800)\n",
      " Test_Y shape: (1, 1497)\n",
      "\n",
      " Processing Fold: C2 - 23\n",
      " Train_X shape: (18982, 800)\n",
      " Train_Y shape: (1, 18982)\n",
      " Test_X shape: (1488, 800)\n",
      " Test_Y shape: (1, 1488)\n",
      "\n",
      " Processing Fold: C2 - 24\n",
      " Train_X shape: (19237, 800)\n",
      " Train_Y shape: (1, 19237)\n",
      " Test_X shape: (1491, 800)\n",
      " Test_Y shape: (1, 1491)\n",
      "\n",
      " Processing Fold: C2 - 25\n",
      " Train_X shape: (19176, 800)\n",
      " Train_Y shape: (1, 19176)\n",
      " Test_X shape: (1499, 800)\n",
      " Test_Y shape: (1, 1499)\n",
      "\n",
      " Processing Fold: C3 - 31\n",
      " Train_X shape: (21512, 800)\n",
      " Train_Y shape: (1, 21512)\n",
      " Test_X shape: (5923, 800)\n",
      " Test_Y shape: (1, 5923)\n",
      "\n",
      " Processing Fold: C3 - 32\n",
      " Train_X shape: (21964, 800)\n",
      " Train_Y shape: (1, 21964)\n",
      " Test_X shape: (5777, 800)\n",
      " Test_Y shape: (1, 5777)\n",
      "\n",
      " Processing Fold: C3 - 33\n",
      " Train_X shape: (20898, 800)\n",
      " Train_Y shape: (1, 20898)\n",
      " Test_X shape: (6264, 800)\n",
      " Test_Y shape: (1, 6264)\n",
      "\n",
      " Processing Fold: C3 - 34\n",
      " Train_X shape: (20980, 800)\n",
      " Train_Y shape: (1, 20980)\n",
      " Test_X shape: (6363, 800)\n",
      " Test_Y shape: (1, 6363)\n",
      "\n",
      " Processing Fold: C3 - 35\n",
      " Train_X shape: (21472, 800)\n",
      " Train_Y shape: (1, 21472)\n",
      " Test_X shape: (6153, 800)\n",
      " Test_Y shape: (1, 6153)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# === Base path ===\n",
    "base_path = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Dataset_output\"\n",
    "\n",
    "# === Dataset fold configurations ===\n",
    "dataset_folds = [\n",
    "    # C1\n",
    "    (11, \"C1\"), (12, \"C1\"), (13, \"C1\"), (14, \"C1\"), (15, \"C1\"),\n",
    "    # C2\n",
    "    (21, \"C2\"), (22, \"C2\"), (23, \"C2\"), (24, \"C2\"), (25, \"C2\"),\n",
    "    # C3\n",
    "    (31, \"C3\"), (32, \"C3\"), (33, \"C3\"), (34, \"C3\"), (35, \"C3\"),\n",
    "]\n",
    "\n",
    "# === Iterate over each fold ===\n",
    "for fold_num, category in dataset_folds:\n",
    "    print(f\"\\n DataSet Folder:= {category}: set-{fold_num}\")\n",
    "\n",
    "    train_path = os.path.join(base_path, f\"For_{category}\", \"For_Train\", f\"train_dataset{fold_num}.tsv\")\n",
    "    test_path = os.path.join(base_path, f\"For_{category}\", \"For_Test\", f\"test_dataset{fold_num}.tsv\")\n",
    "\n",
    "    # === Load Train Set ===\n",
    "    train_df = pd.read_csv(train_path, sep=\"\\t\", header=None)\n",
    "    train_df[1] = train_df[1].apply(lambda x: list(map(float, x.strip().split())))\n",
    "    Train_X = np.vstack(train_df[1].values)\n",
    "    Train_Y = train_df[2].astype(int).values.reshape(1, -1)\n",
    "\n",
    "    print(f\" Train_X shape: {Train_X.shape}\")\n",
    "    print(f\" Train_Y shape: {Train_Y.shape}\")\n",
    "\n",
    "    # === Load Test Set ===\n",
    "    test_df = pd.read_csv(test_path, sep=\"\\t\", header=None)\n",
    "    test_df[1] = test_df[1].apply(lambda x: list(map(float, x.strip().split())))\n",
    "    Test_X = np.vstack(test_df[1].values)\n",
    "    Test_Y = test_df[2].astype(int).values.reshape(1, -1)\n",
    "\n",
    "    print(f\" Test_X shape: {Test_X.shape}\")\n",
    "    print(f\" Test_Y shape: {Test_Y.shape}\")\n",
    "\n",
    "    # # === Optional Preview ===\n",
    "    # print(f\" First 5 Train_Y: {Train_Y[:, :5]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e1b24fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " DataSet Folder:= C1: set-11\n",
      " Train_X shape: (45380, 800)\n",
      " Train_Y shape: (1, 45380)\n",
      " Test_X shape: (5049, 800)\n",
      " Test_Y shape: (1, 5049)\n",
      "\n",
      " DataSet Folder:= C1: set-12\n",
      " Train_X shape: (45383, 800)\n",
      " Train_Y shape: (1, 45383)\n",
      " Test_X shape: (5046, 800)\n",
      " Test_Y shape: (1, 5046)\n",
      "\n",
      " DataSet Folder:= C1: set-13\n",
      " Train_X shape: (45380, 800)\n",
      " Train_Y shape: (1, 45380)\n",
      " Test_X shape: (5049, 800)\n",
      " Test_Y shape: (1, 5049)\n",
      "\n",
      " DataSet Folder:= C1: set-14\n",
      " Train_X shape: (45386, 800)\n",
      " Train_Y shape: (1, 45386)\n",
      " Test_X shape: (5043, 800)\n",
      " Test_Y shape: (1, 5043)\n",
      "\n",
      " DataSet Folder:= C1: set-15\n",
      " Train_X shape: (45384, 800)\n",
      " Train_Y shape: (1, 45384)\n",
      " Test_X shape: (5045, 800)\n",
      " Test_Y shape: (1, 5045)\n",
      "\n",
      " DataSet Folder:= C2: set-21\n",
      " Train_X shape: (18940, 800)\n",
      " Train_Y shape: (1, 18940)\n",
      " Test_X shape: (1496, 800)\n",
      " Test_Y shape: (1, 1496)\n",
      "\n",
      " DataSet Folder:= C2: set-22\n",
      " Train_X shape: (18356, 800)\n",
      " Train_Y shape: (1, 18356)\n",
      " Test_X shape: (1497, 800)\n",
      " Test_Y shape: (1, 1497)\n",
      "\n",
      " DataSet Folder:= C2: set-23\n",
      " Train_X shape: (18982, 800)\n",
      " Train_Y shape: (1, 18982)\n",
      " Test_X shape: (1488, 800)\n",
      " Test_Y shape: (1, 1488)\n",
      "\n",
      " DataSet Folder:= C2: set-24\n",
      " Train_X shape: (19237, 800)\n",
      " Train_Y shape: (1, 19237)\n",
      " Test_X shape: (1491, 800)\n",
      " Test_Y shape: (1, 1491)\n",
      "\n",
      " DataSet Folder:= C2: set-25\n",
      " Train_X shape: (19176, 800)\n",
      " Train_Y shape: (1, 19176)\n",
      " Test_X shape: (1499, 800)\n",
      " Test_Y shape: (1, 1499)\n",
      "\n",
      " DataSet Folder:= C3: set-31\n",
      " Train_X shape: (21512, 800)\n",
      " Train_Y shape: (1, 21512)\n",
      " Test_X shape: (5923, 800)\n",
      " Test_Y shape: (1, 5923)\n",
      "\n",
      " DataSet Folder:= C3: set-32\n",
      " Train_X shape: (21964, 800)\n",
      " Train_Y shape: (1, 21964)\n",
      " Test_X shape: (5777, 800)\n",
      " Test_Y shape: (1, 5777)\n",
      "\n",
      " DataSet Folder:= C3: set-33\n",
      " Train_X shape: (20898, 800)\n",
      " Train_Y shape: (1, 20898)\n",
      " Test_X shape: (6264, 800)\n",
      " Test_Y shape: (1, 6264)\n",
      "\n",
      " DataSet Folder:= C3: set-34\n",
      " Train_X shape: (20980, 800)\n",
      " Train_Y shape: (1, 20980)\n",
      " Test_X shape: (6363, 800)\n",
      " Test_Y shape: (1, 6363)\n",
      "\n",
      " DataSet Folder:= C3: set-35\n",
      " Train_X shape: (21472, 800)\n",
      " Train_Y shape: (1, 21472)\n",
      " Test_X shape: (6153, 800)\n",
      " Test_Y shape: (1, 6153)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# === Base path ===\n",
    "base_path = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Dataset_output\"\n",
    "\n",
    "# === Dataset fold configurations ===\n",
    "dataset_folds = [\n",
    "    # C1\n",
    "    (11, \"C1\"), (12, \"C1\"), (13, \"C1\"), (14, \"C1\"), (15, \"C1\"),\n",
    "    # C2\n",
    "    (21, \"C2\"), (22, \"C2\"), (23, \"C2\"), (24, \"C2\"), (25, \"C2\"),\n",
    "    # C3\n",
    "    (31, \"C3\"), (32, \"C3\"), (33, \"C3\"), (34, \"C3\"), (35, \"C3\"),\n",
    "]\n",
    "\n",
    "# === Store all test data ===\n",
    "all_test_data = {}\n",
    "\n",
    "# === Iterate over each fold ===\n",
    "for fold_num, category in dataset_folds:\n",
    "    print(f\"\\n DataSet Folder:= {category}: set-{fold_num}\")\n",
    "\n",
    "    train_path = os.path.join(base_path, f\"For_{category}\", \"For_Train\", f\"train_dataset{fold_num}.tsv\")\n",
    "    test_path = os.path.join(base_path, f\"For_{category}\", \"For_Test\", f\"test_dataset{fold_num}.tsv\")\n",
    "\n",
    "    # === Load Train Set ===\n",
    "    train_df = pd.read_csv(train_path, sep=\"\\t\", header=None)\n",
    "    train_df[1] = train_df[1].apply(lambda x: list(map(float, x.strip().split())))\n",
    "    Train_X = np.vstack(train_df[1].values)\n",
    "    Train_Y = train_df[2].astype(int).values.reshape(1, -1)\n",
    "\n",
    "    print(f\" Train_X shape: {Train_X.shape}\")\n",
    "    print(f\" Train_Y shape: {Train_Y.shape}\")\n",
    "\n",
    "    # === Load Test Set ===\n",
    "    test_df = pd.read_csv(test_path, sep=\"\\t\", header=None)\n",
    "    test_df[1] = test_df[1].apply(lambda x: list(map(float, x.strip().split())))\n",
    "    Test_X = np.vstack(test_df[1].values)\n",
    "    Test_Y = test_df[2].astype(int).values.reshape(1, -1)\n",
    "\n",
    "    print(f\" Test_X shape: {Test_X.shape}\")\n",
    "    print(f\" Test_Y shape: {Test_Y.shape}\")\n",
    "\n",
    "    # === Store Test Data ===\n",
    "    if category not in all_test_data:\n",
    "        all_test_data[category] = {}\n",
    "\n",
    "    all_test_data[category][fold_num] = {\n",
    "        \"Test_X\": Test_X,\n",
    "        \"Test_Y\": Test_Y\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a05198a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\cs21b\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\cs21b\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-learn) (2.2.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\cs21b\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\cs21b\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\cs21b\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-learn) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: C:\\Users\\cs21b\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%pip install scikit-learn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ee79c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 C1 (45380, 800) (5049, 800)\n",
      "12 C1 (45383, 800) (5046, 800)\n",
      "13 C1 (45380, 800) (5049, 800)\n",
      "14 C1 (45386, 800) (5043, 800)\n",
      "15 C1 (45384, 800) (5045, 800)\n",
      "21 C2 (18940, 800) (1496, 800)\n",
      "22 C2 (18356, 800) (1497, 800)\n",
      "23 C2 (18982, 800) (1488, 800)\n",
      "24 C2 (19237, 800) (1491, 800)\n",
      "25 C2 (19176, 800) (1499, 800)\n",
      "31 C3 (21512, 800) (5923, 800)\n",
      "32 C3 (21964, 800) (5777, 800)\n",
      "33 C3 (20898, 800) (6264, 800)\n",
      "34 C3 (20980, 800) (6363, 800)\n",
      "35 C3 (21472, 800) (6153, 800)\n",
      "\n",
      " All evaluation completed and results saved to: C:\\Users\\cs21b\\Desktop\\Major_project_trail\\combined_results_summary_new_svm.csv\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import os\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# # === Base path ===\n",
    "# base_path = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Dataset_output\"\n",
    "\n",
    "# # === Dataset fold configurations ===\n",
    "# dataset_folds = [\n",
    "#     # C1\n",
    "#     (11, \"C1\"), (12, \"C1\"), (13, \"C1\"), (14, \"C1\"), (15, \"C1\"),\n",
    "#     # C2\n",
    "#     (21, \"C2\"), (22, \"C2\"), (23, \"C2\"), (24, \"C2\"), (25, \"C2\"),\n",
    "#     # C3\n",
    "#     (31, \"C3\"), (32, \"C3\"), (33, \"C3\"), (34, \"C3\"), (35, \"C3\"),\n",
    "# ]\n",
    "\n",
    "# # === Classifiers ===\n",
    "# classifiers = {\n",
    "#     \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "#     # \"SVM (RBF Kernel)\": SVC(kernel='rbf', random_state=42),\n",
    "#     \"K-Nearest Neighbors\": KNeighborsClassifier(n_neighbors=5)\n",
    "# }\n",
    "\n",
    "# # === To store all test data ===\n",
    "# all_test_data = {}\n",
    "# all_results = []\n",
    "\n",
    "# # === Iterate over each fold ===\n",
    "# for fold_num, category in dataset_folds:\n",
    "#     print(f\"\\n DataSet Folder: {category} | Set-{fold_num}\")\n",
    "\n",
    "#     train_path = os.path.join(base_path, f\"For_{category}\", \"For_Train\", f\"train_dataset{fold_num}.tsv\")\n",
    "#     test_path = os.path.join(base_path, f\"For_{category}\", \"For_Test\", f\"test_dataset{fold_num}.tsv\")\n",
    "\n",
    "#     # === Load Train Set ===\n",
    "#     train_df = pd.read_csv(train_path, sep=\"\\t\", header=None)\n",
    "#     train_df[1] = train_df[1].apply(lambda x: list(map(float, x.strip().split())))\n",
    "#     Train_X = np.vstack(train_df[1].values)\n",
    "#     Train_Y = train_df[2].astype(int).values.ravel()\n",
    "\n",
    "#     # === Load Test Set ===\n",
    "#     test_df = pd.read_csv(test_path, sep=\"\\t\", header=None)\n",
    "#     test_df[1] = test_df[1].apply(lambda x: list(map(float, x.strip().split())))\n",
    "#     Test_X = np.vstack(test_df[1].values)\n",
    "#     Test_Y = test_df[2].astype(int).values.ravel()\n",
    "\n",
    "#     # === Store test data for future use ===\n",
    "#     if category not in all_test_data:\n",
    "#         all_test_data[category] = {}\n",
    "#     all_test_data[category][fold_num] = {\n",
    "#         \"Test_X\": Test_X,\n",
    "#         \"Test_Y\": Test_Y\n",
    "#     }\n",
    "\n",
    "#     # === Train and evaluate classifiers ===\n",
    "#     for name, clf in classifiers.items():\n",
    "#         print(f\" Training: {name} on {category} set-{fold_num}\")\n",
    "#         clf.fit(Train_X, Train_Y)\n",
    "\n",
    "#         train_pred = clf.predict(Train_X)\n",
    "#         test_pred = clf.predict(Test_X)\n",
    "\n",
    "#         train_acc = accuracy_score(Train_Y, train_pred)\n",
    "#         test_acc = accuracy_score(Test_Y, test_pred)\n",
    "\n",
    "#         all_results.append({\n",
    "#             \"Category\": category,\n",
    "#             \"Fold\": fold_num,\n",
    "#             \"Classifier\": name,\n",
    "#             \"Train_Accuracy\": round(train_acc * 100, 2),\n",
    "#             \"Test_Accuracy\": round(test_acc * 100, 2)\n",
    "#         })\n",
    "\n",
    "# # === Print Summary ===\n",
    "# print(\"\\n Accuracy Summary\")\n",
    "# print(\"{:<10} {:<6} {:<25} {:<15} {:<15}\".format(\"Category\", \"Fold\", \"Classifier\", \"Train Accuracy\", \"Test Accuracy\"))\n",
    "# print(\"-\" * 75)\n",
    "# for res in all_results:\n",
    "#     print(\"{:<10} {:<6} {:<25} {:<15.2f} {:<15.2f}\".format(\n",
    "#         res[\"Category\"], res[\"Fold\"], res[\"Classifier\"], res[\"Train_Accuracy\"], res[\"Test_Accuracy\"]\n",
    "#     ))\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, roc_auc_score, average_precision_score,\n",
    "    matthews_corrcoef\n",
    ")\n",
    "\n",
    "# === Update base path accordingly ===\n",
    "base_path = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\Dataset_output\"\n",
    "\n",
    "# === Dataset fold configurations ===\n",
    "dataset_folds = [\n",
    "    (11, \"C1\"), (12, \"C1\"), (13, \"C1\"), (14, \"C1\"), (15, \"C1\"),\n",
    "    (21, \"C2\"), (22, \"C2\"), (23, \"C2\"), (24, \"C2\"), (25, \"C2\"),\n",
    "    (31, \"C3\"), (32, \"C3\"), (33, \"C3\"), (34, \"C3\"), (35, \"C3\"),\n",
    "]\n",
    "\n",
    "\n",
    "# === Classifiers ===\n",
    "classifiers = {\n",
    "    # \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"SVM (RBF Kernel)\": SVC(kernel='rbf', probability=True, random_state=42),\n",
    "    # \"SVM (RBF Kernel)\": SVC(kernel='rbf', random_state=42),\n",
    "    # \"K-Nearest Neighbors\": KNeighborsClassifier(n_neighbors=5)\n",
    "}\n",
    "\n",
    "# === Evaluation & Result Storage ===\n",
    "final_results = []\n",
    "\n",
    "for fold_num, category in dataset_folds:\n",
    "    \n",
    "    # === Load training set (static base training set) ===\n",
    "    train_path = os.path.join(base_path, f\"For_{category}\", \"For_Train\", f\"train_dataset{fold_num}.tsv\")\n",
    "    train_df = pd.read_csv(train_path, sep=\"\\t\", header=None)\n",
    "    train_df[1] = train_df[1].apply(lambda x: list(map(float, x.strip().split())))\n",
    "    Train_X = np.vstack(train_df[1].values)\n",
    "    Train_Y = train_df[2].astype(int).values.ravel()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    test_path = os.path.join(base_path, f\"For_{category}\", \"For_Test\", f\"test_dataset{fold_num}.tsv\")\n",
    "    test_df = pd.read_csv(test_path, sep=\"\\t\", header=None)\n",
    "    test_df[1] = test_df[1].apply(lambda x: list(map(float, x.strip().split())))\n",
    "    Test_X = np.vstack(test_df[1].values)\n",
    "    Test_Y = test_df[2].astype(int).values.ravel()\n",
    "    print(fold_num, category,Train_X.shape, Test_X.shape)\n",
    "    for name, clf in classifiers.items():\n",
    "        clf.fit(Train_X, Train_Y)\n",
    "        test_pred = clf.predict(Test_X)\n",
    "\n",
    "        try:\n",
    "            test_proba = clf.predict_proba(Test_X)[:, 1]\n",
    "        except:\n",
    "            test_proba = None\n",
    "\n",
    "        acc = accuracy_score(Test_Y, test_pred)\n",
    "        f1 = f1_score(Test_Y, test_pred)\n",
    "        mcc = matthews_corrcoef(Test_Y, test_pred)\n",
    "        auc = roc_auc_score(Test_Y, test_proba) if test_proba is not None else None\n",
    "        auprc = average_precision_score(Test_Y, test_proba) if test_proba is not None else None\n",
    "\n",
    "        final_results.append([\n",
    "            category, fold_num, name,\n",
    "            round(acc * 100, 2), round(f1, 4), round(mcc, 4),\n",
    "            round(auc, 4) if auc is not None else \"NA\",\n",
    "            round(auprc, 4) if auprc is not None else \"NA\"\n",
    "        ])\n",
    "\n",
    "\n",
    "df_final = pd.DataFrame(final_results, columns=[\n",
    "    \"Category\", \"Fold\", \"Classifier\",\n",
    "    \"Accuracy\", \"F1 Score\", \"MCC\", \"AUC\", \"AUPRC\"\n",
    "])\n",
    "\n",
    "output_csv_path = r\"C:\\Users\\cs21b\\Desktop\\Major_project_trail\\combined_results_summary_new_svm.csv\"\n",
    "df_final.to_csv(output_csv_path, index=False)\n",
    "print(f\"\\n All evaluation completed and results saved to: {output_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fa4da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Classifier  Train Accuracy  Test Accuracy\n",
      "0        Random Forest      100.000000      79.302977\n",
      "1  K-Nearest Neighbors       97.723667      84.913663\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# rf_clf.fit(Train_X, Train_Y.ravel()) \n",
    "\n",
    "# Train_Y_pred = rf_clf.predict(Train_X)\n",
    "\n",
    "# train_accuracy = accuracy_score(Train_Y, Train_Y_pred)\n",
    "# print(f\"Train Accuracy: {train_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Test_Y_pred = rf_clf.predict(Test_X)\n",
    "# test_accuracy = accuracy_score(Test_Y, Test_Y_pred)\n",
    "# print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e693fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e44d796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔷 Training: Random Forest\n",
      "\n",
      "🔷 Training: SVM (RBF Kernel)\n",
      "\n",
      "🔷 Training: K-Nearest Neighbors\n",
      "\n",
      " Accuracy Summary:\n",
      "Classifier                Train Accuracy  Test Accuracy  \n",
      "-------------------------------------------------------\n",
      "Random Forest             100.00          59.30          \n",
      "SVM (RBF Kernel)          98.66           83.98          \n",
      "K-Nearest Neighbors       97.72           86.79          \n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Flatten labels (in case they are shaped like (1, N))\n",
    "Train_Y = Train_Y.ravel()\n",
    "Test_Y = Test_Y.ravel()\n",
    "\n",
    "# Dictionary of classifiers\n",
    "classifiers = {\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"SVM (RBF Kernel)\": SVC(kernel='rbf', random_state=42),\n",
    "    \"K-Nearest Neighbors\": KNeighborsClassifier(n_neighbors=5)\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "# Loop through classifiers\n",
    "for name, clf in classifiers.items():\n",
    "    print(f\"\\n Training: {name}\")\n",
    "    clf.fit(Train_X, Train_Y)\n",
    "\n",
    "    # Predictions\n",
    "    train_pred = clf.predict(Train_X)\n",
    "    test_pred = clf.predict(Test_X)\n",
    "\n",
    "    # Accuracy\n",
    "    train_acc = accuracy_score(Train_Y, train_pred)\n",
    "    test_acc = accuracy_score(Test_Y, test_pred)\n",
    "\n",
    "    results.append((name, train_acc * 100, test_acc * 100))\n",
    "\n",
    "print(\"\\n Accuracy Summary:\")\n",
    "print(\"{:<25} {:<15} {:<15}\".format(\"Classifier\", \"Train Accuracy\", \"Test Accuracy\"))\n",
    "print(\"-\" * 55)\n",
    "for name, train_acc, test_acc in results:\n",
    "    print(\"{:<25} {:<15.2f} {:<15.2f}\".format(name, train_acc, test_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "06056d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45380, 800)\n",
      "(45380,)\n"
     ]
    }
   ],
   "source": [
    "print(Train_X.shape)\n",
    "\n",
    "print(Train_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5185db33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔷 Training: Random Forest\n",
      "\n",
      "📌 Evaluation Metrics for: Random Forest\n",
      "✅ Accuracy       : 59.30%\n",
      "🎯 F1 Score       : 0.7094\n",
      "📈 ROC-AUC        : 0.9736\n",
      "🔁 PR-AUC (AUPRC) : 0.9809\n",
      "🧠 MCC            : 0.3102\n",
      "\n",
      "📊 Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.19      0.32      2524\n",
      "           1       0.55      0.99      0.71      2525\n",
      "\n",
      "    accuracy                           0.59      5049\n",
      "   macro avg       0.76      0.59      0.52      5049\n",
      "weighted avg       0.76      0.59      0.52      5049\n",
      "\n",
      "📌 Confusion Matrix:\n",
      "[[ 486 2038]\n",
      " [  17 2508]]\n",
      "------------------------------------------------------------\n",
      "\n",
      "🔷 Training: SVM (RBF Kernel)\n",
      "\n",
      "📌 Evaluation Metrics for: SVM (RBF Kernel)\n",
      "✅ Accuracy       : 83.98%\n",
      "🎯 F1 Score       : 0.8580\n",
      "📈 ROC-AUC        : 0.9109\n",
      "🔁 PR-AUC (AUPRC) : 0.8660\n",
      "🧠 MCC            : 0.7030\n",
      "\n",
      "📊 Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.71      0.82      2524\n",
      "           1       0.77      0.97      0.86      2525\n",
      "\n",
      "    accuracy                           0.84      5049\n",
      "   macro avg       0.86      0.84      0.84      5049\n",
      "weighted avg       0.86      0.84      0.84      5049\n",
      "\n",
      "📌 Confusion Matrix:\n",
      "[[1796  728]\n",
      " [  81 2444]]\n",
      "------------------------------------------------------------\n",
      "\n",
      "🔷 Training: K-Nearest Neighbors\n",
      "\n",
      "📌 Evaluation Metrics for: K-Nearest Neighbors\n",
      "✅ Accuracy       : 86.79%\n",
      "🎯 F1 Score       : 0.8775\n",
      "📈 ROC-AUC        : 0.9493\n",
      "🔁 PR-AUC (AUPRC) : 0.9301\n",
      "🧠 MCC            : 0.7449\n",
      "\n",
      "📊 Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.79      0.86      2524\n",
      "           1       0.82      0.95      0.88      2525\n",
      "\n",
      "    accuracy                           0.87      5049\n",
      "   macro avg       0.88      0.87      0.87      5049\n",
      "weighted avg       0.88      0.87      0.87      5049\n",
      "\n",
      "📌 Confusion Matrix:\n",
      "[[1994  530]\n",
      " [ 137 2388]]\n",
      "------------------------------------------------------------\n",
      "\n",
      "📊 Accuracy Summary Table:\n",
      "Classifier                Train Accuracy  Test Accuracy  \n",
      "-------------------------------------------------------\n",
      "Random Forest             100.00          59.30          \n",
      "SVM (RBF Kernel)          98.66           83.98          \n",
      "K-Nearest Neighbors       97.72           86.79          \n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, roc_auc_score, average_precision_score,\n",
    "    matthews_corrcoef, classification_report, confusion_matrix\n",
    ")\n",
    "\n",
    "# === Evaluation Function ===\n",
    "def evaluate_model(name, y_true, y_pred, y_proba=None):\n",
    "    print(f\"\\n Evaluation Metrics for: {name}\")\n",
    "    \n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    mcc = matthews_corrcoef(y_true, y_pred)\n",
    "\n",
    "    if y_proba is not None:\n",
    "        auc = roc_auc_score(y_true, y_proba)\n",
    "        auprc = average_precision_score(y_true, y_proba)\n",
    "    else:\n",
    "        auc = auprc = None\n",
    "\n",
    "    print(f\" Accuracy       : {acc * 100:.2f}%\")\n",
    "    print(f\" F1 Score       : {f1:.4f}\")\n",
    "    print(f\" ROC-AUC        : {auc:.4f}\" if auc is not None else \"📈 ROC-AUC        : Not Available\")\n",
    "    print(f\" PR-AUC (AUPRC) : {auprc:.4f}\" if auprc is not None else \"🔁 PR-AUC (AUPRC) : Not Available\")\n",
    "    print(f\" MCC            : {mcc:.4f}\")\n",
    "    \n",
    "    print(\"\\n Classification Report:\")\n",
    "    print(classification_report(y_true, y_pred))\n",
    "\n",
    "    print(\" Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "# === Flatten labels ===\n",
    "Train_Y = Train_Y.ravel()\n",
    "Test_Y = Test_Y.ravel()\n",
    "\n",
    "# === Classifier Dictionary ===\n",
    "classifiers = {\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"SVM (RBF Kernel)\": SVC(kernel='rbf', probability=True, random_state=42),\n",
    "    \"K-Nearest Neighbors\": KNeighborsClassifier(n_neighbors=5)\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "# === Loop & Evaluate ===\n",
    "for name, clf in classifiers.items():\n",
    "    print(f\"\\n Training: {name}\")\n",
    "    clf.fit(Train_X, Train_Y)\n",
    "\n",
    "    # train_pred = clf.predict(Train_X)\n",
    "    test_pred = clf.predict(Test_X)\n",
    "\n",
    "    # For AUC, AUPRC → need probability of class 1\n",
    "    try:\n",
    "        test_proba = clf.predict_proba(Test_X)[:, 1]\n",
    "    except:\n",
    "        test_proba = None  # Not all classifiers support predict_proba\n",
    "\n",
    "    # Store accuracy for summary\n",
    "    # train_acc = accuracy_score(Train_Y, train_pred)\n",
    "    test_acc = accuracy_score(Test_Y, test_pred)\n",
    "    results.append((name, test_acc * 100))\n",
    "\n",
    "    # Evaluate and print full metrics\n",
    "    evaluate_model(name, Test_Y, test_pred, test_proba)\n",
    "\n",
    "# === Accuracy Summary ===\n",
    "print(\"\\n Accuracy Summary Table:\")\n",
    "print(\"{:<25} {:<15} {:<15}\".format(\"Classifier\", \"Train Accuracy\", \"Test Accuracy\"))\n",
    "print(\"-\" * 55)\n",
    "for name, train_acc, test_acc in results:\n",
    "    print(\"{:<25} {:<15.2f} {:<15.2f}\".format(name, train_acc, test_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83c16602",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RandomForestClassifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m help(\u001b[43mRandomForestClassifier\u001b[49m)\n\u001b[0;32m      2\u001b[0m help(SVC)\n\u001b[0;32m      3\u001b[0m help(KNeighborsClassifier)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'RandomForestClassifier' is not defined"
     ]
    }
   ],
   "source": [
    "help(RandomForestClassifier)\n",
    "help(SVC)\n",
    "help(KNeighborsClassifier)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9796ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f77082",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
